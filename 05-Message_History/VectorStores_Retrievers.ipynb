{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25cbf319",
   "metadata": {},
   "source": [
    "# Vector Stores and Retrievers\n",
    "- LangChain's vector store and retriever abstraction.\n",
    "\n",
    "- Integrates data retrieval (from vector database and other sources) to LLM Workflows.\n",
    "\n",
    "- Important for RAGs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "836c4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text = \"\"\n",
    "with open(\"speech.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 400, chunk_overlap = 100)\n",
    "docs = splitter.create_documents([text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e66231d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000251143C61B0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002511386F080>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up environment Variables and llm\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model = \"Llama3-8b-8192\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43a0f2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HCL Tech\\Udemy\\Gen AI\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embedding = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6ad62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chroma is a vector store \n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(docs, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca333e20",
   "metadata": {},
   "source": [
    "### Retrievers\n",
    "- LangChain VectorStore objects do not subclass Runnables. Thus cannot be integrated directly into LangChain Expressioin Language (LCEL) chains.\n",
    "\n",
    "- LangChain Retrievers are Runnables. They are designed to be integrated with LCEL.\n",
    "\n",
    "- `synchronous invoke`, `asynchronous invoke` and  `batch operations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6a4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='321c943b-515f-426d-91b0-5f8a0f86807e', metadata={}, page_content='In a world where digital currents flow like rivers through silicon valleys, the convergence of ideas gives birth to innovations beyond imagination. The humming of servers replaces the songs of birds, and every keystroke echoes with the possibility of change. Amidst this backdrop, a quiet revolution brews—fueled not by steel and fire, but by code and thought. Language models whisper insights, data')],\n",
       " [Document(id='59e0f09d-653d-4758-b4a9-a744934b842c', metadata={}, page_content='brews—fueled not by steel and fire, but by code and thought. Language models whisper insights, data dances in patterns unseen, and human curiosity dares to ask questions machines strive to answer.')]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorstore.as_retriever()\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# we will try to create Runnable by 2 ways\n",
    "# 1. Runnable Lambda\n",
    "\n",
    "retriever = RunnableLambda(vectorstore.similarity_search).bind(k=1)\n",
    "retriever.batch([\"world\", \"coffee\"])\n",
    "\n",
    "# the RunnableLambda is basically using the paramenters we give in batch([]) to run the similarity_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12a546ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='321c943b-515f-426d-91b0-5f8a0f86807e', metadata={}, page_content='In a world where digital currents flow like rivers through silicon valleys, the convergence of ideas gives birth to innovations beyond imagination. The humming of servers replaces the songs of birds, and every keystroke echoes with the possibility of change. Amidst this backdrop, a quiet revolution brews—fueled not by steel and fire, but by code and thought. Language models whisper insights, data')],\n",
       " [Document(id='8ff345e2-0478-40c0-93a5-32100c5b6ce7', metadata={}, page_content='In a coffee shop, a startup founder sketches out an architecture on a napkin, unsure whether it will disrupt an industry or crash on launch. Down the street, a graphic designer experiments with generative art, watching in awe as a diffusion model paints what she only dreamed. New languages emerge—not of grammar, but of instructions, tokens, and layers. New literacies are demanded not by')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a better technique is as_retriever\n",
    "# creates a VectorStoreRetriever\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\":1}\n",
    ")\n",
    "retriever.batch([\"world\", \"paper\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e51eb81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"Answer the questions based on provided context only: \\n{question} \\n\\nContext: \\n{context}\")\n",
    "])\n",
    "\n",
    "# runnablepassthrough means we'll be getting thast specific value in the invoke method\n",
    "rag_chain = {\"context\":retriever, \"question\":RunnablePassthrough()} | prompt | llm \n",
    "\n",
    "response = rag_chain.invoke(\"Tell me about the world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "620cc252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, here are my answers:\\n\\nWhat is the world like?\\n\\nThe world is a digital one, where digital currents flow like rivers through silicon valleys, replacing the natural sounds of birdsong.\\n\\nWhat is happening in this world?\\n\\nA quiet revolution is brewing, fueled by code and thought, rather than traditional means of revolution like steel and fire.\\n\\nWhat are the key features of this world?\\n\\nThe key features of this world are the convergence of ideas, innovations beyond imagination, and the role of language models and data in driving change.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9e9bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
