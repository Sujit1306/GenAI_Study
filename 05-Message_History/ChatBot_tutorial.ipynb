{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adec23c8",
   "metadata": {},
   "source": [
    "# LLM powered Chatbot\n",
    "- Design and Implement a LLM powered ChatBot.\n",
    "\n",
    "- It will be able to have conversation and remember previous interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f69aa",
   "metadata": {},
   "source": [
    "### Related Topics: \n",
    "1. **Conversational RAG**: ChatBot experience over **external source of data**.\n",
    "\n",
    "2. **Agents**: ChatBot that can take **Action**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48d19b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHINA_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROKECT\"] = \"LLM_POWERED_CHATBOT\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009320d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model = \"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ac54b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You are Sujit, and you are studying Gen AI. üòä \\n\\nIs there anything else you'd like to know or discuss about Gen AI? I'm happy to help in any way I can.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 88, 'total_tokens': 134, 'completion_time': 0.083636364, 'prompt_time': 0.0054058, 'queue_time': 0.26442536, 'total_time': 0.089042164}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--65bad498-8f84-47f9-be5c-5c06017e84e9-0', usage_metadata={'input_tokens': 88, 'output_tokens': 46, 'total_tokens': 134})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content = \"Hi, my name is Sujit and I am studying Gen AI\"),\n",
    "    AIMessage(content = \"Hello Sujit! It's nice to meet you. \\n\\nWhat field in Gen AI are you studying these days? \\n\\nIm always eager to aid you in your studies, let me know if you need something.\"),\n",
    "    HumanMessage(content = \"Hey what is my name and what do I study?\")\n",
    "]\n",
    "\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055fe3be",
   "metadata": {},
   "source": [
    "### Message History\n",
    "\n",
    "- We can use a `Message history class` to wrap our model and make it `stateful`. <br>\n",
    "\n",
    "- This will keep track of `inputs and outputs` of our model and store them in a `datastore`.<br>\n",
    "\n",
    "- Future interactions will then `load those messages` and pass them into chain as `part of the input`. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "067baee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant class, set up our chain which wraps the model and add in the message history\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ab8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb233a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\" : {\"session_id\" : \"Chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2b7b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Sujit,\\n\\nIt's great to meet you!\\n\\nGen AI is a fascinating field. What aspects of it are you most interested in? \\n\\nPerhaps I can help you with some information or answer any questions you have. üòä\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(llm, get_session_history)\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"Hi, my name is Sujit and I am studying Gen AI\")],\n",
    "    config = config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b42a1965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"You told me your name is Sujit and you are studying Gen AI!  \\n\\nIs there anything else you'd like to know or discuss about it? üòä  \\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 93, 'total_tokens': 130, 'completion_time': 0.067272727, 'prompt_time': 0.00512947, 'queue_time': 0.265239668, 'total_time': 0.072402197}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--b44da66f-230a-494e-a590-03b3fb6c8857-0', usage_metadata={'input_tokens': 93, 'output_tokens': 37, 'total_tokens': 130})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [HumanMessage(content = \"What's my name and what do i study?\")],\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54a1673d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I have no memory of past conversations and no way of knowing your name. If you'd like to tell me your name, I'd be happy to use it!\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 15, 'total_tokens': 57, 'completion_time': 0.076363636, 'prompt_time': 0.001848747, 'queue_time': 0.258536541, 'total_time': 0.078212383}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--80685112-4175-41ab-b1aa-c448c06f6e78-0', usage_metadata={'input_tokens': 15, 'output_tokens': 42, 'total_tokens': 57})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing the session id\n",
    "config1 = {\"configurable\" : {\"session_id\" : \"Chat2\"}}\n",
    "\n",
    "with_message_history.invoke(\n",
    "    [HumanMessage(content = \"What's my name?\")],\n",
    "    config = config1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d622255",
   "metadata": {},
   "source": [
    "By changing `session_id` we can switch the context of conversation we are having."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3fbb42",
   "metadata": {},
   "source": [
    "### Prompt Template:\n",
    "Integrating prompt templates in ChatBots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6e331a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer the questions to the best of your capability.\"),\n",
    "        MessagesPlaceholder(variable_name = \"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59207a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Sujit! It's nice to meet you. \\n\\nHow can I help you today? üòÑ  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 32, 'total_tokens': 59, 'completion_time': 0.049090909, 'prompt_time': 0.002281435, 'queue_time': 0.257851255, 'total_time': 0.051372344}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--f34c17dd-bc8c-4372-8a5e-c1752b95f7f3-0', usage_metadata={'input_tokens': 32, 'output_tokens': 27, 'total_tokens': 59})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content = \"Hi, my name is Sujit\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c48bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2133db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Sujit! It's nice to meet you.  \\n\\nHow can I help you today? üòä  \\n\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config2 = {\"configurable\" : {\"session_id\" : \"Chat3\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = \"Hi, my name is Sujit\")],\n",
    "    config = config2\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d28d8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding more complexity\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant. Answer the questions to the best of your capability in {language}.\"),\n",
    "        MessagesPlaceholder(variable_name = \"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52cf45a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§∏‡•Å‡§ú‡§ø‡§§! ‡§Æ‡•à‡§Ç ‡§Ü‡§™‡§ï‡•Ä ‡§Æ‡§¶‡§¶ ‡§ï‡§∞‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•Ç‡§Å‡•§ ‡§Ü‡§™ ‡§Æ‡•Å‡§ù‡§∏‡•á ‡§ï‡•ç‡§Ø‡§æ ‡§™‡•Ç‡§õ‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç? \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content = \"Hi, my name is sujit.\")],\n",
    "        \"language\": \"Hindi\"\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e7aff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key= \"messages\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8af2936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞ ‡§∏‡•Å‡§ú‡§ø‡§§! ‡§Æ‡§≤‡§æ ‡§Ü‡§®‡§Ç‡§¶ ‡§Ü‡§π‡•á ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§Æ‡§¶‡§§ ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä. ‡§§‡•Å‡§Æ‡•ç‡§π‡§æ‡§≤‡§æ ‡§ï‡§æ‡§Ø ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§Ü‡§π‡•á? \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3 = {\"configurable\": {\"session_id\": \"Chat4\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content= \"Hi, I am Sujit\")], \"language\": \"Marathi\"},\n",
    "    config = config3\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d3e1a",
   "metadata": {},
   "source": [
    "### Managing Conversation History\n",
    "1. LLMs have a fixed **context size**, whereas **Chat Histories** may not\n",
    "\n",
    "2. If left unmanaged, list of messages will **grow unbounded** \n",
    "\n",
    "3. Potentially **overflow** the **Context window** of LLM\n",
    "\n",
    "It is Important to add a step that `limits the size of messages` you are passing in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a5e7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import trim_messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34fbd1",
   "metadata": {},
   "source": [
    "`trim_messages` helps reduce how many messages we're sending to the model. <br>\n",
    "\n",
    "It allows us to specify how many tokens we want to keep, along with other parameters. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17ca38ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"You're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi! I am Rob', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like Chocolate Ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"what's 2+2\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Thanks!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens = 70,\n",
    "    strategy = \"last\", # focuses on the last conversation it had\n",
    "    token_counter = llm, \n",
    "    include_system = True, # include the system message\n",
    "    allow_partial = False, # dont allow partial message\n",
    "    start_on = \"human\" # it should start from human conversation\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content= \"You're a good assistant\"),\n",
    "    HumanMessage(content= \"Hi! I am Rob\"),\n",
    "    AIMessage(content= \"Hi!\"),\n",
    "    HumanMessage(content= \"I like Chocolate Ice cream\"),\n",
    "    AIMessage(content= \"nice\"),\n",
    "    HumanMessage(content= \"what's 2+2\"),\n",
    "    AIMessage(content= \"4\"),\n",
    "    HumanMessage(content= \"Thanks!\"),\n",
    "    AIMessage(content= \"no problem\"),\n",
    "    HumanMessage(content= \"Having fun?\"),\n",
    "    AIMessage(content= \"yes\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5899139a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You said you like chocolate ice cream!  üç¶üòä  Anything else I can help you with? \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages = itemgetter(\"messages\")|trimmer)\n",
    "    | prompt \n",
    "    | llm\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\":messages + [HumanMessage(content= \"What ice-cream do i like?\")],\n",
    "        \"language\":\"English\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "601c2d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets wrap this in message history\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key= \"messages\",\n",
    ")\n",
    "config = {\"configurable\":{\"session_id\":\"FinalChat\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052167e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
