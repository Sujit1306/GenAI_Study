{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4892b920",
   "metadata": {},
   "source": [
    "# General Prompting Styles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968741d9",
   "metadata": {},
   "source": [
    "### 1. Message based Prompting: `from_messages(...)`\n",
    "\n",
    "This is used for **chat-style models** (like ChatGPT, Groq, Claude) which recognize role-based messaging.\n",
    "\n",
    "- You're interacting with chat models\n",
    "\n",
    "- You want to set context/purpose using roles\n",
    "\n",
    "- You want clarity between user/system/assistant roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ede070",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"Explain LangChain.\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13935d29",
   "metadata": {},
   "source": [
    "### 2. Template based Prompting: `from_template(...)`\n",
    "This creates a **formatted text prompt** with variables (like `{context}`, `{input}`, etc.).\n",
    "\n",
    "- You're doing **RAG** (Retrieval-Augmented Generation)\n",
    "\n",
    "- You’re passing in dynamic variables\n",
    "\n",
    "- You want a cleaner, more controlled format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4bd4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Given this context:\n",
    "    {context}\n",
    "    Answer the question:\n",
    "    {input}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68a2b79",
   "metadata": {},
   "source": [
    "### 3. Few-Shot Prompting `FewShotPromptTemplate`\n",
    "This involves giving the model a few **examples of inputs and outputs**, so it learns how to respond before answering a new query.\n",
    "\n",
    "- You want the model to learn from **examples**\n",
    "\n",
    "- You need **more reliable, few-shot results**\n",
    "\n",
    "- You're doing classification, extraction, summarization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5145f6b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"question\": \"What is LangChain?\", \"answer\": \"LangChain is a framework for LLM apps.\"},\n",
    "    {\"question\": \"What is Groq?\", \"answer\": \"Groq provides fast inference for LLMs.\"}\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=\"Q: {question}\\nA: {answer}\"\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=\"Answer the questions below:\\n\",\n",
    "    suffix=\"Q: {input}\\nA:\",\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe7ac69",
   "metadata": {},
   "source": [
    "### 4. System-Only Prompting `Single Block`\n",
    "Some APIs just accept a plain string with no role separation (e.g., non-chat models like text-davinci-003 or LLaMA-based text models).\n",
    "\n",
    "- You’re not using role-based chat\n",
    "\n",
    "- Your model expects plain instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af73467",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"You are a data analyst. Extract key points from the following:\\n{text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad5040",
   "metadata": {},
   "source": [
    "### 5. `HumanMessage`, `SystemMessage` and `AIMessage`\n",
    "They are structured classes used to explicitly define message roles for chat models (instead of using plain tuples like `(\"system\",\"...\")`). <br>\n",
    "<br>\n",
    "This does exactly what `from_messages()` would do — but gives you **clearer object-level control**, especially useful in more complex applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3cf16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"What is LangChain?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
