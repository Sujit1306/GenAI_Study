{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b553d7",
   "metadata": {},
   "source": [
    "# Chains using LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6656fef",
   "metadata": {},
   "source": [
    "### 1. Chat Messages\n",
    "Whenever we try to create a complex workflow, we as humans need to be able to provide an input and get an output from AI LLM. This Chat Messages is labelled with Human Message and AI Message. With help of chains, we will be able to manage these messages within the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec48e4e",
   "metadata": {},
   "source": [
    "### 2. Chat Models\n",
    "In every or any node, we can integrate various LLM models based on our requirements. We will be looking at how to use Chat Models in graph nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04351ddb",
   "metadata": {},
   "source": [
    "### 3. Binding tools\n",
    "Lets say we ask a specific input example: \"Whats the latest AI news?\". <br>\n",
    "Usually LLM models are trained on previous data, what we can do is, Integrate this LLM with a third party API and Vector Databases or some external source which are integrated as tools. We'll look at how can we bind tools with our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58169d7f",
   "metadata": {},
   "source": [
    "### 4. Execute Toolcalls \n",
    "How does the LLM understand that it needs to make a tool call from graph node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec1852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e1d81",
   "metadata": {},
   "source": [
    "### How to use chat messages as our Chat State \n",
    "Messages can be used to capture different roles within a conversation. <br>\n",
    "\n",
    "Various message types:\n",
    "* HumanMessage\n",
    "* AIMessage\n",
    "* SystemMessage\n",
    "* ToolMessage\n",
    "\n",
    "Components of message:\n",
    "1. content - content of the message\n",
    "2. name - specifies the name of author\n",
    "3. response_metadata (optional) - a dictionary of metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "621a146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: LLMModel\n",
      "\n",
      "Hi how can i help you today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Sujit\n",
      "\n",
      "I want to study Generative AI\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: LLMModel\n",
      "\n",
      "Okay what do you want to study in Generative AI\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: Sujit\n",
      "\n",
      "I want to know about Langchain, Langsmith and Langserve\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage \n",
    "from pprint import pprint\n",
    "\n",
    "messages = [AIMessage(content= f\"Hi how can i help you today?\", name= \"LLMModel\")]\n",
    "messages.append(HumanMessage(content= f\"I want to study Generative AI\", name= \"Sujit\"))\n",
    "messages.append(AIMessage(content= f\"Okay what do you want to study in Generative AI\", name= \"LLMModel\"))\n",
    "messages.append(HumanMessage(content= f\"I want to know about Langchain, Langsmith and Langserve\", name= \"Sujit\"))\n",
    "\n",
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "531fe9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You\\'ve picked three exciting tools in the world of generative AI! \\n\\nLet\\'s break down LangChain, LangSmith, and LangServe:\\n\\n**LangChain:**\\n\\n* **What it is:**  LangChain is a framework designed to simplify the development of applications using large language models (LLMs). Think of it as a toolbox filled with components that help you build sophisticated AI applications.\\n* **Key Features:**\\n    * **Chains:**  LangChain\\'s core concept is \"chains,\" which are sequences of steps that an LLM executes. You can combine different components like prompting, memory, and tool usage to create complex workflows.\\n    * **Agents:**  LangChain allows you to build AI agents that can interact with the world. These agents can access external tools, retrieve information, and make decisions based on their interactions.\\n    * **Memory:**  LangChain provides mechanisms for LLMs to remember past interactions, enabling them to have more context-aware and engaging conversations.\\n* **Use Cases:**\\n    * Chatbots that can access real-time information\\n    * Summarizers that condense large documents\\n    * Question answering systems that retrieve answers from specific sources\\n\\n**LangSmith:**\\n\\n* **What it is:** LangSmith is an open-source tool specifically focused on streamlining the process of fine-tuning LLMs. It makes it easier to experiment with different model architectures and training parameters.\\n* **Key Features:**\\n    * **User-Friendly Interface:** LangSmith provides a visual interface that simplifies the fine-tuning process.\\n    * **Experiment Tracking:** It helps you track different fine-tuning runs and compare their results.\\n    * **Model Sharing:** You can easily share your fine-tuned models with others.\\n* **Use Cases:**\\n\\n    * Adapting a pre-trained LLM to a specific domain (e.g., healthcare, legal)\\n    * Improving the performance of an LLM on a particular task (e.g., translation, code generation)\\n\\n**LangServe:**\\n\\n* **What it is:** LangServe is a tool for deploying and serving LLMs efficiently.  It focuses on making it fast and scalable to run LLMs in production environments.\\n* **Key Features:**\\n    * **Fast Inference:**  LangServe is designed to optimize the speed at which LLMs generate responses.\\n    * **Scalability:** It can handle a high volume of requests, making it suitable for applications with many users.\\n    * **Integration:** LangServe integrates well with other tools and platforms, such as LangChain.\\n* **Use Cases:**\\n\\n    * Powering chatbots that handle large numbers of conversations\\n    * Serving LLMs as APIs for other applications\\n\\n**In Summary:**\\n\\n* **LangChain:** The framework for building applications with LLMs.\\n* **LangSmith:** The tool for fine-tuning LLMs.\\n* **LangServe:** The platform for deploying and serving LLMs.\\n\\nTogether, these tools provide a powerful and versatile ecosystem for working with generative AI.\\n\\n\\nLet me know if you have any more questions about these tools or any other aspect of generative AI!\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model= \"gemma2-9b-it\")\n",
    "\n",
    "llm.invoke(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9280bde0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
