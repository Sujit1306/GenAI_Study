{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a60ebeda",
   "metadata": {},
   "source": [
    "# Agentic RAG\n",
    "Enhances traditional RAG (Retrieval Augmented Generation) with intelligent agents to handle complex tasks and make decisions dynamically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866157d8",
   "metadata": {},
   "source": [
    "### Flow of a Traditional RAG:\n",
    "1. User asks a query\n",
    "2. Query is passed to DB which provides some kind of context\n",
    "3. Context is combined with prompt and passed to LLM\n",
    "4. LLM provides the final output\n",
    "\n",
    "The LLM is specifically used only one time in a traditional RAG, only to generate an output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8854fe",
   "metadata": {},
   "source": [
    "### How is Agentic RAG different than Traditional RAG:\n",
    "Traditional RAGs fail in case of having to retrieve information from multiple databases.<br>\n",
    "So, the solution Agentic RAG provides is to incorporate an Agent to read the user's query.<br> \n",
    "The agent will read user input and determine which DB to query the result from thus .<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e8d06",
   "metadata": {},
   "source": [
    "### Flow of an Agentic RAG:\n",
    "1. User asks a query\n",
    "2. The Query Agent wrt query determines which DB to pass the query to \n",
    "3. Query is passed to that specific DB which provides some context\n",
    "4. Context is then passed to LLM to determine if it is accurate wrt the query\n",
    "5. If yes, then LLM provides a relevant summary.\n",
    "6. If no, then we rewrite the entire query i.e. repeat from step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e68eb65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"MISTRAL_API_KEY\"]=os.getenv(\"MISTRAL_API_KEY\")\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model= \"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3c11ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter as RCTS \n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_mistralai import MistralAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c20990f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/why-langgraph/', 'title': 'Overview', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Overview\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n            Build a custom workflow\\n          \\n\\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n    1. Build a basic chatbot\\n    \\n  \\n\\n\\n\\n\\n\\n    2. Add tools\\n    \\n  \\n\\n\\n\\n\\n\\n    3. Add memory\\n    \\n  \\n\\n\\n\\n\\n\\n    4. Add human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n    5. Customize state\\n    \\n  \\n\\n\\n\\n\\n\\n    6. Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview¶\\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\\n\\nReliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\\nLow-level and extensible. Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\\nFirst-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.\\n\\nLearn LangGraph basics¶\\nTo get acquainted with LangGraph's key concepts and features, complete the following LangGraph basics tutorials series:\\n\\nBuild a basic chatbot\\nAdd tools\\nAdd memory\\nAdd human-in-the-loop controls\\nCustomize state\\nTime travel\\n\\nIn completing this series of tutorials, you will build a support chatbot in LangGraph that can:\\n\\n✅ Answer common questions by searching the web\\n✅ Maintain conversation state across calls  \\n✅ Route complex queries to a human for review  \\n✅ Use custom state to control its behavior  \\n✅ Rewind and explore alternative conversation paths  \\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Start with a prebuilt agent\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                1. Build a basic chatbot\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Workflows & agents', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows & agents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Workflows & agents\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows and Agents¶\\nThis guide reviews common patterns for agentic systems. In describing these systems, it can be useful to make a distinction between \"workflows\" and \"agents\". One way to think about this difference is nicely explained in Anthropic\\'s Building Effective Agents blog post:\\n\\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths.\\nAgents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\\n\\nHere is a simple way to visualize these differences:\\n\\nWhen building agents and workflows, LangGraph offers a number of benefits including persistence, streaming, and support for debugging as well as deployment.\\nSet up¶\\nYou can use any chat model that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.\\n\\nInstall dependencies\\npip install langchain_core langchain-anthropic langgraph \\n\\n\\nInitialize an LLM\\nAPI Reference: ChatAnthropic\\nimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n\\nBuilding Blocks: The Augmented LLM¶\\nLLM have augmentations that support building workflows and agents. These include structured outputs and tool calling, as shown in this image from the Anthropic blog on Building Effective Agents:\\n\\n# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\nPrompt chaining¶\\nIn prompt chaining, each LLM call processes the output of the previous one. \\nAs noted in the Anthropic blog on Building Effective Agents: \\n\\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.\\nWhen to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/a0281fca-3a71-46de-beee-791468607b75/r\\nResources:\\nLangChain Academy\\nSee our lesson on Prompt Chaining here.\\n\\n\\nfrom langgraph.func import entrypoint, task\\n\\n\\n# Tasks\\n@task\\ndef generate_joke(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\\n    return msg.content\\n\\n\\ndef check_punchline(joke: str):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in joke or \"!\" in joke:\\n        return \"Fail\"\\n\\n    return \"Pass\"\\n\\n\\n@task\\ndef improve_joke(joke: str):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\\n    return msg.content\\n\\n\\n@task\\ndef polish_joke(joke: str):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\\n    return msg.content\\n\\n\\n@entrypoint()\\ndef prompt_chaining_workflow(topic: str):\\n    original_joke = generate_joke(topic).result()\\n    if check_punchline(original_joke) == \"Pass\":\\n        return original_joke\\n\\n    improved_joke = improve_joke(original_joke).result()\\n    return polish_joke(improved_joke).result()\\n\\n# Invoke\\nfor step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/332fa4fc-b6ca-416e-baa3-161625e69163/r\\n\\n\\n\\nParallelization¶\\nWith parallelization, LLMs work simultaneously on a task:\\n\\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs.\\nWhen to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/3be2e53c-ca94-40dd-934f-82ff87fac277/r\\nResources:\\nDocumentation\\nSee our documentation on parallelization here.\\nLangChain Academy\\nSee our lesson on parallelization here.\\n\\n\\n@task\\ndef call_llm_1(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_2(topic: str):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n    msg = llm.invoke(f\"Write a story about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_3(topic):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n    msg = llm.invoke(f\"Write a poem about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef aggregator(topic, joke, story, poem):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {topic}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{story}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{joke}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{poem}\"\\n    return combined\\n\\n\\n# Build workflow\\n@entrypoint()\\ndef parallel_workflow(topic: str):\\n    joke_fut = call_llm_1(topic)\\n    story_fut = call_llm_2(topic)\\n    poem_fut = call_llm_3(topic)\\n    return aggregator(\\n        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\\n    ).result()\\n\\n# Invoke\\nfor step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/623d033f-e814-41e9-80b1-75e6abb67801/r\\n\\n\\n\\nRouting¶\\nRouting classifies an input and directs it to a followup task. As noted in the Anthropic blog on Building Effective Agents: \\n\\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\\nWhen to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import Literal\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/c4580b74-fe91-47e4-96fe-7fac598d509c/r\\nResources:\\nLangChain Academy\\nSee our lesson on routing here.\\nExamples\\nHere is RAG workflow that routes questions. See our video here.\\n\\n\\nfrom typing_extensions import Literal\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n@task\\ndef llm_call_1(input_: str):\\n    \"\"\"Write a story\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_2(input_: str):\\n    \"\"\"Write a joke\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_3(input_: str):\\n    \"\"\"Write a poem\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\ndef llm_call_router(input_: str):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=input_),\\n        ]\\n    )\\n    return decision.step\\n\\n\\n# Create workflow\\n@entrypoint()\\ndef router_workflow(input_: str):\\n    next_step = llm_call_router(input_)\\n    if next_step == \"story\":\\n        llm_call = llm_call_1\\n    elif next_step == \"joke\":\\n        llm_call = llm_call_2\\n    elif next_step == \"poem\":\\n        llm_call = llm_call_3\\n\\n    return llm_call(input_).result()\\n\\n# Invoke\\nfor step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/5e2eb979-82dd-402c-b1a0-a8cceaf2a28a/r\\n\\n\\n\\nOrchestrator-Worker¶\\nWith orchestrator-worker, an orchestrator breaks down a task and delegates each sub-task to workers. As noted in the Anthropic blog on Building Effective Agents: \\n\\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\\nWhen to use this workflow: This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren\\'t pre-defined, but determined by the orchestrator based on the specific input.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\nCreating Workers in LangGraph\\nBecause orchestrator-worker workflows are common, LangGraph has the Send API to support this. It lets you dynamically create worker nodes and send each one a specific input. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. As you can see below, we iterate over a list of sections and Send each to a worker node. See further documentation here and here.\\nfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/78cbcfc3-38bf-471d-b62a-b299b144237d/r\\nResources:\\nLangChain Academy\\nSee our lesson on orchestrator-worker here.\\nExamples\\nHere is a project that uses orchestrator-worker for report planning and writing. See our video here.\\n\\n\\nfrom typing import List\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\n@task\\ndef orchestrator(topic: str):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {topic}\"),\\n        ]\\n    )\\n\\n    return report_sections.sections\\n\\n\\n@task\\ndef llm_call(section: Section):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    result = llm.invoke(\\n        [\\n            SystemMessage(content=\"Write a report section.\"),\\n            HumanMessage(\\n                content=f\"Here is the section name: {section.name} and description: {section.description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return result.content\\n\\n\\n@task\\ndef synthesizer(completed_sections: list[str]):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n    final_report = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n    return final_report\\n\\n\\n@entrypoint()\\ndef orchestrator_worker(topic: str):\\n    sections = orchestrator(topic).result()\\n    section_futures = [llm_call(section) for section in sections]\\n    final_report = synthesizer(\\n        [section_fut.result() for section_fut in section_futures]\\n    ).result()\\n    return final_report\\n\\n# Invoke\\nreport = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\\nfrom IPython.display import Markdown\\nMarkdown(report)\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/75a636d0-6179-4a12-9836-e0aa571e87c5/r\\n\\n\\n\\nEvaluator-optimizer¶\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop:\\n\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\\nWhen to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/86ab3e60-2000-4bff-b988-9b89a3269789/r\\nResources:\\nExamples\\nHere is an assistant that uses evaluator-optimizer to improve a report. See our video here.\\nHere is a RAG workflow that grades answers for hallucinations or errors. See our video here.\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\n@task\\ndef llm_call_generator(topic: str, feedback: Feedback):\\n    \"\"\"LLM generates a joke\"\"\"\\n    if feedback:\\n        msg = llm.invoke(\\n            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef llm_call_evaluator(joke: str):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\\n    return feedback\\n\\n\\n@entrypoint()\\ndef optimizer_workflow(topic: str):\\n    feedback = None\\n    while True:\\n        joke = llm_call_generator(topic, feedback).result()\\n        feedback = llm_call_evaluator(joke).result()\\n        if feedback.grade == \"funny\":\\n            break\\n\\n    return joke\\n\\n# Invoke\\nfor step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/f66830be-4339-4a6b-8a93-389ce5ae27b4/r\\n\\n\\n\\nAgent¶\\nAgents are typically implemented as an LLM performing actions (via tool-calling) based on environmental feedback in a loop. As noted in the Anthropic blog on Building Effective Agents:\\n\\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully.\\nWhen to use agents: Agents can be used for open-ended problems where it’s difficult or impossible to predict the required number of steps, and where you can’t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents\\' autonomy makes them ideal for scaling tasks in trusted environments.\\n\\n\\nAPI Reference: tool\\nfrom langchain_core.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional API\\n\\n\\nfrom langgraph.graph import MessagesState\\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"environment\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"Action\"\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"environment\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    {\\n        # Name returned by should_continue : Name of next node to visit\\n        \"Action\": \"environment\",\\n        END: END,\\n    },\\n)\\nagent_builder.add_edge(\"environment\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/051f0391-6761-4f8c-a53b-22231b016690/r\\nResources:\\nLangChain Academy\\nSee our lesson on agents here.\\nExamples\\nHere is a project that uses a tool calling agent to create / store long-term memories.\\n\\n\\nfrom langgraph.graph import add_messages\\nfrom langchain_core.messages import (\\n    SystemMessage,\\n    HumanMessage,\\n    BaseMessage,\\n    ToolCall,\\n)\\n\\n\\n@task\\ndef call_llm(messages: list[BaseMessage]):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n    return llm_with_tools.invoke(\\n        [\\n            SystemMessage(\\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n            )\\n        ]\\n        + messages\\n    )\\n\\n\\n@task\\ndef call_tool(tool_call: ToolCall):\\n    \"\"\"Performs the tool call\"\"\"\\n    tool = tools_by_name[tool_call[\"name\"]]\\n    return tool.invoke(tool_call)\\n\\n\\n@entrypoint()\\ndef agent(messages: list[BaseMessage]):\\n    llm_response = call_llm(messages).result()\\n\\n    while True:\\n        if not llm_response.tool_calls:\\n            break\\n\\n        # Execute tools\\n        tool_result_futures = [\\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\\n        ]\\n        tool_results = [fut.result() for fut in tool_result_futures]\\n        messages = add_messages(messages, [llm_response, *tool_results])\\n        llm_response = call_llm(messages).result()\\n\\n    messages = add_messages(messages, llm_response)\\n    return messages\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\\n    print(chunk)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/42ae8bf9-3935-4504-a081-8ddbcbfc8b2e/r\\n\\n\\n\\nPre-built¶\\nLangGraph also provides a pre-built method for creating an agent as defined above (using the create_react_agent function):\\nhttps://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\\nAPI Reference: create_react_agent\\nfrom langgraph.prebuilt import create_react_agent\\n\\n# Pass in:\\n# (1) the augmented LLM with tools\\n# (2) the tools list (which is used to create the tool node)\\npre_built_agent = create_react_agent(llm, tools=tools)\\n\\n# Show the agent\\ndisplay(Image(pre_built_agent.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = pre_built_agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/graph-api/#map-reduce-and-the-send-api', 'title': 'Use the Graph API', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse the Graph API\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Use the Graph API\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Guides\\n          \\n\\n\\n\\n\\n\\n    Agent development\\n    \\n  \\n\\n\\n\\n\\n\\n            Agent development\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n    Run an agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph APIs\\n    \\n  \\n\\n\\n\\n\\n\\n            LangGraph APIs\\n          \\n\\n\\n\\n\\n\\n    Graph API\\n    \\n  \\n\\n\\n\\n\\n\\n            Graph API\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Use the Graph API\\n    \\n  \\n\\n\\n\\n\\n    Use the Graph API\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Setup\\n    \\n\\n\\n\\n\\n\\n      Define and update state\\n    \\n\\n\\n\\n\\n\\n\\n      Define state\\n    \\n\\n\\n\\n\\n\\n      Update state\\n    \\n\\n\\n\\n\\n\\n      Process state updates with reducers\\n    \\n\\n\\n\\n\\n\\n\\n      MessagesState\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Define input and output schemas\\n    \\n\\n\\n\\n\\n\\n      Pass private state between nodes\\n    \\n\\n\\n\\n\\n\\n      Use Pydantic models for graph state\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Add runtime configuration\\n    \\n\\n\\n\\n\\n\\n      Add retry policies\\n    \\n\\n\\n\\n\\n\\n      Add node caching\\n    \\n\\n\\n\\n\\n\\n      Create a sequence of steps\\n    \\n\\n\\n\\n\\n\\n      Create branches\\n    \\n\\n\\n\\n\\n\\n\\n      Run graph nodes in parallel\\n    \\n\\n\\n\\n\\n\\n      Defer node execution\\n    \\n\\n\\n\\n\\n\\n      Conditional branching\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Map-Reduce and the Send API\\n    \\n\\n\\n\\n\\n\\n      Create and control loops\\n    \\n\\n\\n\\n\\n\\n\\n      Impose a recursion limit\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Async\\n    \\n\\n\\n\\n\\n\\n      Combine control flow and state updates with Command\\n    \\n\\n\\n\\n\\n\\n\\n      Navigate to a node in a parent graph\\n    \\n\\n\\n\\n\\n\\n      Use inside tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Visualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Functional API\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Runtime\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Core capabilities\\n    \\n  \\n\\n\\n\\n\\n\\n            Core capabilities\\n          \\n\\n\\n\\n\\n    Streaming\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Persistence\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Durable execution\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Memory\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Context\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Models\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tools\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Subgraphs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Multi-agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    MCP\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tracing\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Platform-only capabilities\\n    \\n  \\n\\n\\n\\n\\n\\n            Platform-only capabilities\\n          \\n\\n\\n\\n\\n    LangGraph Platform\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Authentication & access control\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Assistants\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Double-texting\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Webhooks\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Cron jobs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Server customization\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Data management\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Deployment\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Setup\\n    \\n\\n\\n\\n\\n\\n      Define and update state\\n    \\n\\n\\n\\n\\n\\n\\n      Define state\\n    \\n\\n\\n\\n\\n\\n      Update state\\n    \\n\\n\\n\\n\\n\\n      Process state updates with reducers\\n    \\n\\n\\n\\n\\n\\n\\n      MessagesState\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Define input and output schemas\\n    \\n\\n\\n\\n\\n\\n      Pass private state between nodes\\n    \\n\\n\\n\\n\\n\\n      Use Pydantic models for graph state\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Add runtime configuration\\n    \\n\\n\\n\\n\\n\\n      Add retry policies\\n    \\n\\n\\n\\n\\n\\n      Add node caching\\n    \\n\\n\\n\\n\\n\\n      Create a sequence of steps\\n    \\n\\n\\n\\n\\n\\n      Create branches\\n    \\n\\n\\n\\n\\n\\n\\n      Run graph nodes in parallel\\n    \\n\\n\\n\\n\\n\\n      Defer node execution\\n    \\n\\n\\n\\n\\n\\n      Conditional branching\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Map-Reduce and the Send API\\n    \\n\\n\\n\\n\\n\\n      Create and control loops\\n    \\n\\n\\n\\n\\n\\n\\n      Impose a recursion limit\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Async\\n    \\n\\n\\n\\n\\n\\n      Combine control flow and state updates with Command\\n    \\n\\n\\n\\n\\n\\n\\n      Navigate to a node in a parent graph\\n    \\n\\n\\n\\n\\n\\n      Use inside tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Visualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow to use the graph API¶\\nThis guide demonstrates the basics of LangGraph\\'s Graph API. It walks through state, as well as composing common graph structures such as sequences, branches, and loops. It also covers LangGraph\\'s control features, including the Send API for map-reduce workflows and the Command API for combining state updates with \"hops\" across nodes.\\nSetup¶\\nInstall langgraph:\\npip install -U langgraph\\n\\n\\nSet up LangSmith for better debugging\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the docs.\\n\\nDefine and update state¶\\nHere we show how to define and update state in LangGraph. We will demonstrate:\\n\\nHow to use state to define a graph\\'s schema\\nHow to use reducers to control how state updates are processed.\\n\\nDefine state¶\\nState in LangGraph can be a TypedDict, Pydantic model, or dataclass. Below we will use TypedDict. See this section for detail on using Pydantic.\\nBy default, graphs will have the same input and output schema, and the state determines that schema. See this section for how to define distinct input and output schemas.\\nLet\\'s consider a simple example using messages. This represents a versatile formulation of state for many LLM applications. See our concepts page for more detail.\\nAPI Reference: AnyMessage\\nfrom langchain_core.messages import AnyMessage\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    messages: list[AnyMessage]\\n    extra_field: int\\n\\nThis state tracks a list of message objects, as well as an extra integer field.\\nUpdate state¶\\nLet\\'s build an example graph with a single node. Our node is just a Python function that reads our graph\\'s state and makes updates to it. The first argument to this function will always be the state:\\nAPI Reference: AIMessage\\nfrom langchain_core.messages import AIMessage\\n\\ndef node(state: State):\\n    messages = state[\"messages\"]\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": messages + [new_message], \"extra_field\": 10}\\n\\nThis node simply appends a message to our message list, and populates an extra field.\\n\\nImportant\\nNodes should return updates to the state directly, instead of mutating the state.\\n\\nLet\\'s next define a simple graph containing this node. We use StateGraph to define a graph that operates on this state. We then use add_node populate our graph.\\nAPI Reference: StateGraph\\nfrom langgraph.graph import StateGraph\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(node)\\nbuilder.set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our graph. See this section for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIn this case, our graph just executes a single node. Let\\'s proceed with a simple invocation:\\nAPI Reference: HumanMessage\\nfrom langchain_core.messages import HumanMessage\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\nresult\\n\\n{\\'messages\\': [HumanMessage(content=\\'Hi\\'), AIMessage(content=\\'Hello!\\')], \\'extra_field\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by updating a single key of the state.\\nWe receive the entire state in the invocation result.\\n\\nFor convenience, we frequently inspect the content of message objects via pretty-print:\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nProcess state updates with reducers¶\\nEach key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.\\nFor TypedDict state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.\\nIn the earlier example, our node updated the \"messages\" key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:\\nfrom typing_extensions import Annotated\\n\\ndef add(left, right):\\n    \"\"\"Can also import `add` from the `operator` built-in.\"\"\"\\n    return left + right\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add]\\n    extra_field: int\\n\\nNow our node can be simplified:\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\nAPI Reference: START\\nfrom langgraph.graph import START\\n\\ngraph = StateGraph(State).add_node(node).add_edge(START, \"node\").compile()\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nMessagesState¶\\nIn practice, there are additional considerations for updating lists of messages:\\n\\nWe may wish to update an existing message in the state.\\nWe may want to accept short-hands for message formats, such as OpenAI format.\\n\\nLangGraph includes a built-in reducer add_messages that handles these considerations:\\nAPI Reference: add_messages\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n    extra_field: int\\n\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\ngraph = StateGraph(State).add_node(node).set_entry_point(\"node\").compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hi\"}\\n\\nresult = graph.invoke({\"messages\": [input_message]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nThis is a versatile representation of state for applications involving chat models. LangGraph includes a pre-built MessagesState for convenience, so that we can have:\\nfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    extra_field: int\\n\\nDefine input and output schemas¶\\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it\\'s also possible to define distinct input and output schemas for a graph.\\nWhen distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.\\nBelow, we\\'ll see how to define distinct input and output schema.\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# Define the schema for the input\\nclass InputState(TypedDict):\\n    question: str\\n\\n# Define the schema for the output\\nclass OutputState(TypedDict):\\n    answer: str\\n\\n# Define the overall schema, combining both input and output\\nclass OverallState(InputState, OutputState):\\n    pass\\n\\n# Define the node that processes the input and generates an answer\\ndef answer_node(state: InputState):\\n    # Example answer and an extra key\\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\\n\\n# Build the graph with input and output schemas specified\\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\\nbuilder.add_node(answer_node)  # Add the answer node\\nbuilder.add_edge(START, \"answer_node\")  # Define the starting edge\\nbuilder.add_edge(\"answer_node\", END)  # Define the ending edge\\ngraph = builder.compile()  # Compile the graph\\n\\n# Invoke the graph with an input and print the result\\nprint(graph.invoke({\"question\": \"hi\"}))\\n\\n{\\'answer\\': \\'bye\\'}\\n\\nNotice that the output of invoke only includes the output schema.\\nPass private state between nodes¶\\nIn some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn\\'t need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.\\nBelow, we\\'ll create an example sequential graph consisting of three nodes (node_1, node_2 and node_3), where private data is passed between the first two steps (node_1 and node_2), while the third step (node_3) only has access to the public overall state.\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(TypedDict):\\n    a: str\\n\\n# Output from node_1 contains private data that is not part of the overall state\\nclass Node1Output(TypedDict):\\n    private_data: str\\n\\n# The private data is only shared between node_1 and node_2\\ndef node_1(state: OverallState) -> Node1Output:\\n    output = {\"private_data\": \"set by node_1\"}\\n    print(f\"Entered node `node_1`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 2 input only requests the private data available after node_1\\nclass Node2Input(TypedDict):\\n    private_data: str\\n\\ndef node_2(state: Node2Input) -> OverallState:\\n    output = {\"a\": \"set by node_2\"}\\n    print(f\"Entered node `node_2`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 3 only has access to the overall state (no access to private data from node_1)\\ndef node_3(state: OverallState) -> OverallState:\\n    output = {\"a\": \"set by node_3\"}\\n    print(f\"Entered node `node_3`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Connect nodes in a sequence\\n# node_2 accepts private data from node_1, whereas\\n# node_3 does not see the private data.\\nbuilder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])\\nbuilder.add_edge(START, \"node_1\")\\ngraph = builder.compile()\\n\\n# Invoke the graph with the initial state\\nresponse = graph.invoke(\\n    {\\n        \"a\": \"set at start\",\\n    }\\n)\\n\\nprint()\\nprint(f\"Output of graph invocation: {response}\")\\n\\nEntered node `node_1`:\\n    Input: {\\'a\\': \\'set at start\\'}.\\n    Returned: {\\'private_data\\': \\'set by node_1\\'}\\nEntered node `node_2`:\\n    Input: {\\'private_data\\': \\'set by node_1\\'}.\\n    Returned: {\\'a\\': \\'set by node_2\\'}\\nEntered node `node_3`:\\n    Input: {\\'a\\': \\'set by node_2\\'}.\\n    Returned: {\\'a\\': \\'set by node_3\\'}\\n\\nOutput of graph invocation: {\\'a\\': \\'set by node_3\\'}\\n\\nUse Pydantic models for graph state¶\\nA StateGraph accepts a state_schema argument on initialization that specifies the \"shape\" of the state that the nodes in the graph can access and update.\\nIn our examples, we typically use a python-native TypedDict or dataclass for state_schema, but state_schema can be any type.\\nHere, we\\'ll see how a Pydantic BaseModel can be used for state_schema to add run-time validation on inputs.\\n\\nKnown Limitations\\n\\nCurrently, the output of the graph will NOT be an instance of a pydantic model.\\nRun-time validation only occurs on inputs into nodes, not on the outputs.\\nThe validation error trace from pydantic does not show which node the error arises in.\\nPydantic\\'s recursive validation can be slow. For performance-sensitive applications, you may want to consider using a dataclass instead.\\n\\n\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\nfrom pydantic import BaseModel\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(BaseModel):\\n    a: str\\n\\ndef node(state: OverallState):\\n    return {\"a\": \"goodbye\"}\\n\\n# Build the state graph\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(node)  # node_1 is the first node\\nbuilder.add_edge(START, \"node\")  # Start the graph with node_1\\nbuilder.add_edge(\"node\", END)  # End the graph after node_1\\ngraph = builder.compile()\\n\\n# Test the graph with a valid input\\ngraph.invoke({\"a\": \"hello\"})\\n\\nInvoke the graph with an invalid input\\ntry:\\n    graph.invoke({\"a\": 123})  # Should be a string\\nexcept Exception as e:\\n    print(\"An exception was raised because `a` is an integer rather than a string.\")\\n    print(e)\\n\\nAn exception was raised because `a` is an integer rather than a string.\\n1 validation error for OverallState\\na\\n  Input should be a valid string [type=string_type, input_value=123, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type\\n\\nSee below for additional features of Pydantic model state:\\n\\nSerialization Behavior\\nWhen using Pydantic models as state schemas, it\\'s important to understand how serialization works, especially when:\\n- Passing Pydantic objects as inputs\\n- Receiving outputs from the graph\\n- Working with nested Pydantic models\\nLet\\'s see these behaviors in action.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass NestedModel(BaseModel):\\n    value: str\\n\\nclass ComplexState(BaseModel):\\n    text: str\\n    count: int\\n    nested: NestedModel\\n\\ndef process_node(state: ComplexState):\\n    # Node receives a validated Pydantic object\\n    print(f\"Input state type: {type(state)}\")\\n    print(f\"Nested type: {type(state.nested)}\")\\n    # Return a dictionary update\\n    return {\"text\": state.text + \" processed\", \"count\": state.count + 1}\\n\\n# Build the graph\\nbuilder = StateGraph(ComplexState)\\nbuilder.add_node(\"process\", process_node)\\nbuilder.add_edge(START, \"process\")\\nbuilder.add_edge(\"process\", END)\\ngraph = builder.compile()\\n\\n# Create a Pydantic instance for input\\ninput_state = ComplexState(text=\"hello\", count=0, nested=NestedModel(value=\"test\"))\\nprint(f\"Input object type: {type(input_state)}\")\\n\\n# Invoke graph with a Pydantic instance\\nresult = graph.invoke(input_state)\\nprint(f\"Output type: {type(result)}\")\\nprint(f\"Output content: {result}\")\\n\\n# Convert back to Pydantic model if needed\\noutput_model = ComplexState(**result)\\nprint(f\"Converted back to Pydantic: {type(output_model)}\")\\n\\n\\n\\nRuntime Type Coercion\\nPydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you\\'re not aware of it.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass CoercionExample(BaseModel):\\n    # Pydantic will coerce string numbers to integers\\n    number: int\\n    # Pydantic will parse string booleans to bool\\n    flag: bool\\n\\ndef inspect_node(state: CoercionExample):\\n    print(f\"number: {state.number} (type: {type(state.number)})\")\\n    print(f\"flag: {state.flag} (type: {type(state.flag)})\")\\n    return {}\\n\\nbuilder = StateGraph(CoercionExample)\\nbuilder.add_node(\"inspect\", inspect_node)\\nbuilder.add_edge(START, \"inspect\")\\nbuilder.add_edge(\"inspect\", END)\\ngraph = builder.compile()\\n\\n# Demonstrate coercion with string inputs that will be converted\\nresult = graph.invoke({\"number\": \"42\", \"flag\": \"true\"})\\n\\n# This would fail with a validation error\\ntry:\\n    graph.invoke({\"number\": \"not-a-number\", \"flag\": \"true\"})\\nexcept Exception as e:\\n    print(f\"\\\\nExpected validation error: {e}\")\\n\\n\\n\\nWorking with Message Models\\nWhen working with LangChain message types in your state schema, there are important considerations for serialization. You should use AnyMessage (rather than BaseMessage) for proper serialization/deserialization when using message objects over the wire.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, AIMessage, AnyMessage\\nfrom typing import List\\n\\nclass ChatState(BaseModel):\\n    messages: List[AnyMessage]\\n    context: str\\n\\ndef add_message(state: ChatState):\\n    return {\"messages\": state.messages + [AIMessage(content=\"Hello there!\")]}\\n\\nbuilder = StateGraph(ChatState)\\nbuilder.add_node(\"add_message\", add_message)\\nbuilder.add_edge(START, \"add_message\")\\nbuilder.add_edge(\"add_message\", END)\\ngraph = builder.compile()\\n\\n# Create input with a message\\ninitial_state = ChatState(\\n    messages=[HumanMessage(content=\"Hi\")], context=\"Customer support chat\"\\n)\\n\\nresult = graph.invoke(initial_state)\\nprint(f\"Output: {result}\")\\n\\n# Convert back to Pydantic model to see message types\\noutput_model = ChatState(**result)\\nfor i, msg in enumerate(output_model.messages):\\n    print(f\"Message {i}: {type(msg).__name__} - {msg.content}\")\\n\\n\\nAdd runtime configuration¶\\nSometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, without polluting the graph state with these parameters.\\nTo add runtime configuration:\\n\\nSpecify a schema for your configuration\\nAdd the configuration to the function signature for nodes or conditional edges\\nPass the configuration into the graph.\\n\\nSee below for a simple example:\\nAPI Reference: END | StateGraph | START\\nfrom langgraph.graph import END, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n# 1. Specify config schema\\nclass ContextSchema(TypedDict):\\n    my_runtime_value: str\\n\\n# 2. Define a graph that accesses the config in a node\\nclass State(TypedDict):\\n    my_state_value: str\\n\\ndef node(state: State, runtime: Runtime[ContextSchema]):\\n    if runtime.context[\"my_runtime_value\"] == \"a\":\\n        return {\"my_state_value\": 1}\\n    elif runtime.context[\"my_runtime_value\"] == \"b\":\\n        return {\"my_state_value\": 2}\\n    else:\\n        raise ValueError(\"Unknown values.\")\\n\\nbuilder = StateGraph(State, context_schema=ContextSchema)\\nbuilder.add_node(node)\\nbuilder.add_edge(START, \"node\")\\nbuilder.add_edge(\"node\", END)\\n\\ngraph = builder.compile()\\n\\n# 3. Pass in configuration at runtime:\\nprint(graph.invoke({}, context={\"my_runtime_value\": \"a\"}))\\nprint(graph.invoke({}, context={\"my_runtime_value\": \"b\"}))\\n\\n{\\'my_state_value\\': 1}\\n{\\'my_state_value\\': 2}\\n\\n\\nExtended example: specifying LLM at runtime\\nBelow we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.\\nfrom dataclasses import dataclass\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, END, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n@dataclass\\nclass ContextSchema:\\n    model_provider: str = \"anthropic\"\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\\n    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\\n    model = MODELS[runtime.context.model_provider]\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, context_schema=ContextSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\n# With no configuration, uses default (Anthropic)\\nresponse_1 = graph.invoke({\"messages\": [input_message]})[\"messages\"][-1]\\n# Or, can set OpenAI\\nresponse_2 = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\"})[\"messages\"][-1]\\n\\nprint(response_1.response_metadata[\"model_name\"])\\nprint(response_2.response_metadata[\"model_name\"])\\n\\nclaude-3-5-haiku-20241022\\ngpt-4.1-mini-2025-04-14\\n\\n\\n\\nExtended example: specifying model and system message at runtime\\nBelow we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.\\nfrom dataclasses import dataclass\\nfrom typing import Optional\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.messages import SystemMessage\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n@dataclass\\nclass ContextSchema:\\n    model_provider: str = \"anthropic\"\\n    system_message: str | None = None\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\\n    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\\n    model = MODELS[runtime.context.model_provider]\\n    messages = state[\"messages\"]\\n    if (system_message := runtime.context.system_message):\\n        messages = [SystemMessage(system_message)] + messages\\n    response = model.invoke(messages)\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, context_schema=ContextSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\nresponse = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\", \"system_message\": \"Respond in Italian.\"})\\nfor message in response[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nhi\\n================================== Ai Message ==================================\\n\\nCiao! Come posso aiutarti oggi?\\n\\n\\nAdd retry policies¶\\nThere are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes.\\nTo configure a retry policy, pass the retry_policy parameter to the add_node. The retry_policy parameter takes in a RetryPolicy named tuple object. Below we instantiate a RetryPolicy object with the default parameters and associate it with a node:\\nfrom langgraph.pregel import RetryPolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    retry_policy=RetryPolicy(),\\n)\\n\\nBy default, the retry_on parameter uses the default_retry_on function, which retries on any exception except for the following:\\n\\nValueError\\nTypeError\\nArithmeticError\\nImportError\\nLookupError\\nNameError\\nSyntaxError\\nRuntimeError\\nReferenceError\\nStopIteration\\nStopAsyncIteration\\nOSError\\n\\nIn addition, for exceptions from popular http request libraries such as requests and httpx it only retries on 5xx status codes.\\n\\nExtended example: customizing retry policies\\nConsider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes:\\nimport sqlite3\\nfrom typing_extensions import TypedDict\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.pregel import RetryPolicy\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_core.messages import AIMessage\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///:memory:\")\\nmodel = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\\n\\ndef query_database(state: MessagesState):\\n    query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\\n    return {\"messages\": [AIMessage(content=query_result)]}\\n\\ndef call_model(state: MessagesState):\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\n# Define a new graph\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(\\n    \"query_database\",\\n    query_database,\\n    retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),\\n)\\nbuilder.add_node(\"model\", call_model, retry_policy=RetryPolicy(max_attempts=5))\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", \"query_database\")\\nbuilder.add_edge(\"query_database\", END)\\ngraph = builder.compile()\\n\\n\\nAdd node caching¶\\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph.\\nTo configure a cache policy, pass the cache_policy parameter to the add_node function. In the following example, a CachePolicy object is instantiated with a time to live of 120 seconds and the default key_func generator. Then it is associated with a node:\\nfrom langgraph.types import CachePolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    cache_policy=CachePolicy(ttl=120),\\n)\\n\\nThen, to enable node-level caching for a graph, set the cache argument when compiling the graph. The example below uses InMemoryCache to set up a graph with in-memory cache, but SqliteCache is also available.\\nfrom langgraph.cache.memory import InMemoryCache\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nCreate a sequence of steps¶\\n\\nPrerequisites\\nThis guide assumes familiarity with the above section on state.\\n\\nHere we demonstrate how to construct a simple sequence of steps. We will show:\\n\\nHow to build a sequential graph\\nBuilt-in short-hand for constructing similar graphs.\\n\\nTo add a sequence of nodes, we use the .add_node and .add_edge methods of our graph:\\nAPI Reference: START | StateGraph\\nfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\nWe can also use the built-in shorthand .add_sequence:\\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\n\\nWhy split application steps into a sequence with LangGraph?\\nLangGraph makes it easy to add an underlying persistence layer to your application.\\nThis allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:\\n\\nHow state updates are checkpointed\\nHow interruptions are resumed in human-in-the-loop workflows\\nHow we can \"rewind\" and branch-off executions using LangGraph\\'s time travel features\\n\\nThey also determine how execution steps are streamed, and how your application is visualized\\nand debugged using LangGraph Studio.\\n\\nLet\\'s demonstrate an end-to-end example. We will create a sequence of three steps:\\n\\nPopulate a value in a key of the state\\nUpdate the same value\\nPopulate a different value\\n\\nLet\\'s first define our state. This governs the schema of the graph, and can also specify how to apply updates. See this section for more detail.\\nIn our case, we will just keep track of two values:\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    value_1: str\\n    value_2: int\\n\\nOur nodes are just Python functions that read our graph\\'s state and make updates to it. The first argument to this function will always be the state:\\ndef step_1(state: State):\\n    return {\"value_1\": \"a\"}\\n\\ndef step_2(state: State):\\n    current_value_1 = state[\"value_1\"]\\n    return {\"value_1\": f\"{current_value_1} b\"}\\n\\ndef step_3(state: State):\\n    return {\"value_2\": 10}\\n\\n\\nNote\\nNote that when issuing updates to the state, each node can just specify the value of the key it wishes to update.\\nBy default, this will overwrite the value of the corresponding key. You can also use reducers to control how updates are processed— for example, you can append successive updates to a key instead. See this section for more detail.\\n\\nFinally, we define the graph. We use StateGraph to define a graph that operates on this state.\\nWe will then use add_node and add_edge to populate our graph and define its control flow.\\nAPI Reference: START | StateGraph\\nfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\n\\nSpecifying custom names\\nYou can specify custom names for nodes using .add_node:\\nbuilder.add_node(\"my_node\", step_1)\\n\\n\\nNote that:\\n\\n.add_edge takes the names of nodes, which for functions defaults to node.__name__.\\nWe must specify the entry point of the graph. For this we add an edge with the START node.\\nThe graph halts when there are no more nodes to execute.\\n\\nWe next compile our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a checkpointer, it would also be passed in here.\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our sequence. See this guide for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nLet\\'s proceed with a simple invocation:\\ngraph.invoke({\"value_1\": \"c\"})\\n\\n{\\'value_1\\': \\'a b\\', \\'value_2\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.\\nThe value we passed in was overwritten by the first node.\\nThe second node updated the value.\\nThe third node populated a different value.\\n\\n\\nBuilt-in shorthand\\nlanggraph>=0.2.46 includes a built-in short-hand add_sequence for adding node sequences. You can compile the same graph as follows:\\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"value_1\": \"c\"})    \\n\\n\\nCreate branches¶\\nParallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.\\nRun graph nodes in parallel¶\\nIn this example, we fan out from Node A to B and C and then fan in to D. With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on state reducers for more detail on updating state with reducers.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nWith the reducer, you can see that the values added in each node are accumulated.\\ngraph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\n\\nAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\']\\n\\n\\nNote\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. Because they are in the same step, node \"d\" executes after both \"b\" and \"c\" are finished.\\nImportantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.\\n\\n\\nException handling?\\nLangGraph executes nodes within supersteps, meaning that while parallel branches are executed in parallel, the entire superstep is transactional. If any of these branches raises an exception, none of the updates are applied to the state (the entire superstep errors).\\nImportantly, when using a checkpointer, results from successful nodes within a superstep are saved, and don\\'t repeat when resumed.\\nIf you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this:\\n\\nYou can write regular python code within your node to catch and handle exceptions.\\nYou can set a retry_policy to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn\\'t worry about performing redundant work.\\n\\nTogether, these let you perform parallel execution and fully control exception handling.\\n\\nDefer node execution¶\\nDeferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows.\\nThe above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let\\'s add a node \"b_2\" in the \"b\" branch:\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef b_2(state: State):\\n    print(f\\'Adding \"B_2\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B_2\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(b_2)\\nbuilder.add_node(c)\\nbuilder.add_node(d, defer=True)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"b_2\")\\nbuilder.add_edge(\"b_2\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\ngraph.invoke({\"aggregate\": []})\\n\\nAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"B_2\" to [\\'A\\', \\'B\\', \\'C\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\', \\'B_2\\']\\n\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. We set defer=True on node d so it will not execute until all pending tasks are finished. In this case, this means that \"d\" waits to execute until the entire \"b\" branch is finished.\\nConditional branching¶\\nIf your fan-out should vary at runtime based on the state, you can use add_conditional_edges to select one or more paths using the graph state. See example below, where node a generates a state update that determines the following node.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Literal, Sequence\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    # Add a key to the state. We will set this key to determine\\n    # how we branch.\\n    which: str\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"], \"which\": \"c\"}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"b\", END)\\nbuilder.add_edge(\"c\", END)\\n\\ndef conditional_edge(state: State) -> Literal[\"b\", \"c\"]:\\n    # Fill in arbitrary logic here that uses the state\\n    # to determine the next node\\n    return state[\"which\"]\\n\\nbuilder.add_conditional_edges(\"a\", conditional_edge)\\n\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nresult = graph.invoke({\"aggregate\": []})\\nprint(result)\\n\\nAdding \"A\" to []\\nAdding \"C\" to [\\'A\\']\\n{\\'aggregate\\': [\\'A\\', \\'C\\'], \\'which\\': \\'c\\'}\\n\\n\\nTip\\nYour conditional edges can route to multiple destination nodes. For example:\\ndef route_bc_or_cd(state: State) -> Sequence[str]:\\n    if state[\"which\"] == \"cd\":\\n        return [\"c\", \"d\"]\\n    return [\"b\", \"c\"]\\n\\n\\nMap-Reduce and the Send API¶\\nLangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:\\nAPI Reference: StateGraph | START | END | Send\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Send\\nfrom typing_extensions import TypedDict, Annotated\\nimport operator\\n\\nclass OverallState(TypedDict):\\n    topic: str\\n    subjects: list[str]\\n    jokes: Annotated[list[str], operator.add]\\n    best_selected_joke: str\\n\\ndef generate_topics(state: OverallState):\\n    return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]}\\n\\ndef generate_joke(state: OverallState):\\n    joke_map = {\\n        \"lions\": \"Why don\\'t lions like fast food? Because they can\\'t catch it!\",\\n        \"elephants\": \"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\",\\n        \"penguins\": \"Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\"\\n    }\\n    return {\"jokes\": [joke_map[state[\"subject\"]]]}\\n\\ndef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\\n\\ndef best_joke(state: OverallState):\\n    return {\"best_selected_joke\": \"penguins\"}\\n\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(\"generate_topics\", generate_topics)\\nbuilder.add_node(\"generate_joke\", generate_joke)\\nbuilder.add_node(\"best_joke\", best_joke)\\nbuilder.add_edge(START, \"generate_topics\")\\nbuilder.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\\nbuilder.add_edge(\"generate_joke\", \"best_joke\")\\nbuilder.add_edge(\"best_joke\", END)\\nbuilder.add_edge(\"generate_topics\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\n# Call the graph: here we call it to generate a list of jokes\\nfor step in graph.stream({\"topic\": \"animals\"}):\\n    print(step)\\n\\n{\\'generate_topics\\': {\\'subjects\\': [\\'lions\\', \\'elephants\\', \\'penguins\\']}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t lions like fast food? Because they can\\'t catch it!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\\'Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\\']}}\\n{\\'best_joke\\': {\\'best_selected_joke\\': \\'penguins\\'}}\\n\\nCreate and control loops¶\\nWhen creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition.\\nYou can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here.\\nLet\\'s consider a simple graph with a loop to better understand how these mechanisms work.\\n\\nTip\\nTo return the last value of your state instead of receiving a recursion limit error, see the next section.\\n\\nWhen creating a loop, you can include a conditional edge that specifies a termination condition:\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if termination_condition(state):\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nTo control the recursion limit, specify \"recursion_limit\" in the config. This will raise a GraphRecursionError, which you can catch and handle:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke(inputs, {\"recursion_limit\": 3})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nLet\\'s define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis architecture is similar to a ReAct agent in which node \"a\" is a tool-calling model, and node \"b\" represents the tools.\\nIn our route conditional edge, we specify that we should end after the \"aggregate\" list in the state passes a threshold length.\\nInvoking the graph, we see that we alternate between nodes \"a\" and \"b\" before terminating once we reach the termination condition.\\ngraph.invoke({\"aggregate\": []})\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\', \\'B\\']\\n\\nImpose a recursion limit¶\\nIn some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph\\'s recursion limit. This will raise a GraphRecursionError after a given number of supersteps. We can then catch and handle this exception:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\nExtended example: return state on hitting recursion limit\\nInstead of raising GraphRecursionError, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.\\nLangGraph implements a special RemainingSteps annotation. Under the hood, it creates a ManagedValue channel -- a state channel that will exist for the duration of our graph run and no longer.\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.managed.is_last_step import RemainingSteps\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    remaining_steps: RemainingSteps\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if state[\"remaining_steps\"] <= 2:\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\n# Test it out\\nresult = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nprint(result)\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\n{\\'aggregate\\': [\\'A\\', \\'B\\', \\'A\\']}\\n\\n\\n\\nExtended example: loops with branches\\nTo better understand how the recursion limit works, let\\'s consider a more complex example. Below we implement a loop, but one step fans out into two nodes:\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Node C sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Node D sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge([\"c\", \"d\"], \"a\")\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis graph looks complex, but can be conceptualized as loop of supersteps:\\n\\nNode A\\nNode B\\nNodes C and D\\nNode A\\n...\\n\\nWe have a loop of four supersteps, where nodes C and D are executed concurrently.\\nInvoking the graph as before, we see that we complete two full \"laps\" before hitting the termination condition:\\nresult = graph.invoke({\"aggregate\": []})\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nNode B sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\']\\nNode D sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\', \\'C\\', \\'D\\']\\n\\nHowever, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\nAsync¶\\nUsing the async programming paradigm can produce significant performance improvements when running IO-bound code concurrently (e.g., making concurrent API requests to a chat model provider).\\nTo convert a sync implementation of the graph to an async implementation, you will need to:\\n\\nUpdate nodes use async def instead of def.\\nUpdate the code inside to use await appropriately.\\nInvoke the graph with .ainvoke or .astream as desired.\\n\\nBecause many LangChain objects implement the Runnable Protocol which has async variants of all the sync methods it\\'s typically fairly quick to upgrade a sync graph to an async graph.\\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\\nOpenAIAnthropicAzureGoogle GeminiAWS Bedrock\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"openai:gpt-4.1\")\\n\\n👉 Read the OpenAI integration docs\\n\\n\\npip install -U \"langchain[anthropic]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\\n\\n👉 Read the Anthropic integration docs\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nllm = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\n👉 Read the Azure integration docs\\n\\n\\npip install -U \"langchain[google-genai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\\n\\n👉 Read the Google GenAI integration docs\\n\\n\\npip install -U \"langchain[aws]\"\\n\\nfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nllm = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\n👉 Read the AWS Bedrock integration docs\\n\\n\\n\\nAPI Reference: init_chat_model | StateGraph\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, StateGraph\\n\\nasync def node(state: MessagesState): # (1)!\\n    new_message = await llm.ainvoke(state[\"messages\"]) # (2)!\\n    return {\"messages\": [new_message]}\\n\\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\\nresult = await graph.ainvoke({\"messages\": [input_message]}) # (3)!\\n\\n\\nDeclare nodes to be async functions.\\nUse async invocations when available within the node.\\nUse async invocations on the graph object itself.\\n\\n\\nAsync streaming\\nSee the streaming guide for examples of streaming with async.\\n\\nCombine control flow and state updates with Command¶\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWe show an end-to-end example below. Let\\'s create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.\\nAPI Reference: StateGraph | START | Command\\nimport random\\nfrom typing_extensions import TypedDict, Literal\\nfrom langgraph.graph import StateGraph, START\\nfrom langgraph.types import Command\\n\\n# Define graph state\\nclass State(TypedDict):\\n    foo: str\\n\\n# Define the nodes\\n\\ndef node_a(state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\\n    print(\"Called A\")\\n    value = random.choice([\"b\", \"c\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"b\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        # this is the state update\\n        update={\"foo\": value},\\n        # this is a replacement for an edge\\n        goto=goto,\\n    )\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    return {\"foo\": state[\"foo\"] + \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": state[\"foo\"] + \"c\"}\\n\\nWe can now create the StateGraph with the above nodes. Notice that the graph doesn\\'t have conditional edges for routing! This is because control flow is defined with Command inside node_a.\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"node_a\")\\nbuilder.add_node(node_a)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n# NOTE: there are no edges between nodes A, B and C!\\n\\ngraph = builder.compile()\\n\\n\\nImportant\\nYou might have noticed that we used Command as a return type annotation, e.g. Command[Literal[\"node_b\", \"node_c\"]]. This is necessary for the graph rendering and tells LangGraph that node_a can navigate to node_b and node_c.\\n\\nfrom IPython.display import display, Image\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIf we run the graph multiple times, we\\'d see it take different paths (A -> B or A -> C) based on the random choice in node A.\\ngraph.invoke({\"foo\": \"\"})\\n\\nCalled A\\nCalled C\\n\\nNavigate to a node in a parent graph¶\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n\\nLet\\'s demonstrate this using the above example. We\\'ll do so by changing node_a in the above example into a single-node graph that we\\'ll add as a subgraph to our parent graph.\\n\\nState updates with Command.PARENT\\nWhen you send updates from a subgraph node to a parent graph node for a key that\\'s shared by both parent and subgraph state schemas, you must define a reducer for the key you\\'re updating in the parent graph state. See the example below.\\n\\nimport operator\\nfrom typing_extensions import Annotated\\n\\nclass State(TypedDict):\\n    # NOTE: we define a reducer here\\n    foo: Annotated[str, operator.add]\\n\\ndef node_a(state: State):\\n    print(\"Called A\")\\n    value = random.choice([\"a\", \"b\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"a\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        update={\"foo\": value},\\n        goto=goto,\\n        # this tells LangGraph to navigate to node_b or node_c in the parent graph\\n        # NOTE: this will navigate to the closest parent graph relative to the subgraph\\n        graph=Command.PARENT,\\n    )\\n\\nsubgraph = StateGraph(State).add_node(node_a).add_edge(START, \"node_a\").compile()\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    # NOTE: since we\\'ve defined a reducer, we don\\'t need to manually append\\n    # new characters to existing \\'foo\\' value. instead, reducer will append these\\n    # automatically (via operator.add)\\n    return {\"foo\": \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": \"c\"}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"subgraph\")\\nbuilder.add_node(\"subgraph\", subgraph)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"foo\": \"\"})\\n\\nCalled A\\nCalled C\\n\\nUse inside tools¶\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]}) from the tool:\\n@tool\\ndef lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\\n    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\\n    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\\n    return Command(\\n        update={\\n            # update the state keys\\n            \"user_info\": user_info,\\n            # update the message history\\n            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\\n        }\\n    )\\n\\n\\nImportant\\nYou MUST include messages (or any state key used for the message history) in Command.update when returning Command from a tool and the list of messages in messages MUST contain a ToolMessage. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).\\n\\nIf you are using tools that update state via Command, we recommend using prebuilt ToolNode which automatically handles tools returning Command objects and propagates them to the graph state. If you\\'re writing a custom node that calls tools, you would need to manually propagate Command objects returned by the tools as the update from the node.\\nVisualize your graph¶\\nHere we demonstrate how to visualize the graphs you create.\\nYou can visualize any arbitrary Graph, including StateGraph. Let\\'s have some fun by drawing fractals :).\\nAPI Reference: StateGraph | START | END | add_messages\\nimport random\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\nclass MyNode:\\n    def __init__(self, name: str):\\n        self.name = name\\n    def __call__(self, state: State):\\n        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\\n\\ndef route(state) -> Literal[\"entry_node\", \"__end__\"]:\\n    if len(state[\"messages\"]) > 10:\\n        return \"__end__\"\\n    return \"entry_node\"\\n\\ndef add_fractal_nodes(builder, current_node, level, max_level):\\n    if level > max_level:\\n        return\\n    # Number of nodes to create at this level\\n    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\\n    for i in range(num_nodes):\\n        nm = [\"A\", \"B\", \"C\"][i]\\n        node_name = f\"node_{current_node}_{nm}\"\\n        builder.add_node(node_name, MyNode(node_name))\\n        builder.add_edge(current_node, node_name)\\n        # Recursively add more nodes\\n        r = random.random()\\n        if r > 0.2 and level + 1 < max_level:\\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\\n        elif r > 0.05:\\n            builder.add_conditional_edges(node_name, route, node_name)\\n        else:\\n            # End\\n            builder.add_edge(node_name, \"__end__\")\\n\\ndef build_fractal_graph(max_level: int):\\n    builder = StateGraph(State)\\n    entry_point = \"entry_node\"\\n    builder.add_node(entry_point, MyNode(entry_point))\\n    builder.add_edge(START, entry_point)\\n    add_fractal_nodes(builder, entry_point, 1, max_level)\\n    # Optional: set a finish point if required\\n    builder.add_edge(entry_point, END)  # or any specific node\\n    return builder.compile()\\n\\napp = build_fractal_graph(3)\\n\\nMermaid¶\\nWe can also convert a graph class into Mermaid syntax.\\nprint(app.get_graph().draw_mermaid())\\n\\n%%{init: {\\'flowchart\\': {\\'curve\\': \\'linear\\'}}}%%\\ngraph TD;\\n    __start__([<p>__start__</p>]):::first\\n    entry_node(entry_node)\\n    node_entry_node_A(node_entry_node_A)\\n    node_entry_node_B(node_entry_node_B)\\n    node_node_entry_node_B_A(node_node_entry_node_B_A)\\n    node_node_entry_node_B_B(node_node_entry_node_B_B)\\n    node_node_entry_node_B_C(node_node_entry_node_B_C)\\n    __end__([<p>__end__</p>]):::last\\n    __start__ --> entry_node;\\n    entry_node --> __end__;\\n    entry_node --> node_entry_node_A;\\n    entry_node --> node_entry_node_B;\\n    node_entry_node_B --> node_node_entry_node_B_A;\\n    node_entry_node_B --> node_node_entry_node_B_B;\\n    node_entry_node_B --> node_node_entry_node_B_C;\\n    node_entry_node_A -.-> entry_node;\\n    node_entry_node_A -.-> __end__;\\n    node_node_entry_node_B_A -.-> entry_node;\\n    node_node_entry_node_B_A -.-> __end__;\\n    node_node_entry_node_B_B -.-> entry_node;\\n    node_node_entry_node_B_B -.-> __end__;\\n    node_node_entry_node_B_C -.-> entry_node;\\n    node_node_entry_node_B_C -.-> __end__;\\n    classDef default fill:#f2f0ff,line-height:1.2\\n    classDef first fill-opacity:0\\n    classDef last fill:#bfb6fc\\n\\nPNG¶\\nIf preferred, we could render the Graph into a  .png. Here we could use three options:\\n\\nUsing Mermaid.ink API (does not require additional packages)\\nUsing Mermaid + Pyppeteer (requires pip install pyppeteer)\\nUsing graphviz (which requires pip install graphviz)\\n\\nUsing Mermaid.Ink\\nBy default, draw_mermaid_png() uses Mermaid.Ink\\'s API to generate the diagram.\\nAPI Reference: CurveStyle | MermaidDrawMethod | NodeStyles\\nfrom IPython.display import Image, display\\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\\n\\ndisplay(Image(app.get_graph().draw_mermaid_png()))\\n\\n\\nUsing Mermaid + Pyppeteer\\nimport nest_asyncio\\n\\nnest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\\n\\ndisplay(\\n    Image(\\n        app.get_graph().draw_mermaid_png(\\n            curve_style=CurveStyle.LINEAR,\\n            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\\n            wrap_label_n_words=9,\\n            output_file_path=None,\\n            draw_method=MermaidDrawMethod.PYPPETEER,\\n            background_color=\"white\",\\n            padding=10,\\n        )\\n    )\\n)\\n\\nUsing Graphviz\\ntry:\\n    display(Image(app.get_graph().draw_png()))\\nexcept ImportError:\\n    print(\\n        \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\"\\n    )\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Overview\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Overview\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls=[\n",
    "    \"https://langchain-ai.github.io/langgraph/concepts/why-langgraph/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/tutorials/workflows/\",\n",
    "    \"https://langchain-ai.github.io/langgraph/how-tos/graph-api/#map-reduce-and-the-send-api\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86ca35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/why-langgraph/', 'title': 'Overview', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Overview\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n            Build a custom workflow\\n          \\n\\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n    1. Build a basic chatbot\\n    \\n  \\n\\n\\n\\n\\n\\n    2. Add tools\\n    \\n  \\n\\n\\n\\n\\n\\n    3. Add memory\\n    \\n  \\n\\n\\n\\n\\n\\n    4. Add human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n    5. Customize state\\n    \\n  \\n\\n\\n\\n\\n\\n    6. Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview¶\\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\\n\\nReliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\\nLow-level and extensible. Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\\nFirst-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.\\n\\nLearn LangGraph basics¶\\nTo get acquainted with LangGraph's key concepts and features, complete the following LangGraph basics tutorials series:\\n\\nBuild a basic chatbot\\nAdd tools\\nAdd memory\\nAdd human-in-the-loop controls\\nCustomize state\\nTime travel\\n\\nIn completing this series of tutorials, you will build a support chatbot in LangGraph that can:\\n\\n✅ Answer common questions by searching the web\\n✅ Maintain conversation state across calls  \\n✅ Route complex queries to a human for review  \\n✅ Use custom state to control its behavior  \\n✅ Rewind and explore alternative conversation paths  \\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Start with a prebuilt agent\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                1. Build a basic chatbot\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Workflows & agents', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows & agents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Workflows & agents\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows and Agents¶\\nThis guide reviews common patterns for agentic systems. In describing these systems, it can be useful to make a distinction between \"workflows\" and \"agents\". One way to think about this difference is nicely explained in Anthropic\\'s Building Effective Agents blog post:\\n\\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths.\\nAgents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\\n\\nHere is a simple way to visualize these differences:\\n\\nWhen building agents and workflows, LangGraph offers a number of benefits including persistence, streaming, and support for debugging as well as deployment.\\nSet up¶\\nYou can use any chat model that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.\\n\\nInstall dependencies\\npip install langchain_core langchain-anthropic langgraph \\n\\n\\nInitialize an LLM\\nAPI Reference: ChatAnthropic\\nimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n\\nBuilding Blocks: The Augmented LLM¶\\nLLM have augmentations that support building workflows and agents. These include structured outputs and tool calling, as shown in this image from the Anthropic blog on Building Effective Agents:\\n\\n# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\nPrompt chaining¶\\nIn prompt chaining, each LLM call processes the output of the previous one. \\nAs noted in the Anthropic blog on Building Effective Agents: \\n\\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.\\nWhen to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/a0281fca-3a71-46de-beee-791468607b75/r\\nResources:\\nLangChain Academy\\nSee our lesson on Prompt Chaining here.\\n\\n\\nfrom langgraph.func import entrypoint, task\\n\\n\\n# Tasks\\n@task\\ndef generate_joke(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\\n    return msg.content\\n\\n\\ndef check_punchline(joke: str):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in joke or \"!\" in joke:\\n        return \"Fail\"\\n\\n    return \"Pass\"\\n\\n\\n@task\\ndef improve_joke(joke: str):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\\n    return msg.content\\n\\n\\n@task\\ndef polish_joke(joke: str):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\\n    return msg.content\\n\\n\\n@entrypoint()\\ndef prompt_chaining_workflow(topic: str):\\n    original_joke = generate_joke(topic).result()\\n    if check_punchline(original_joke) == \"Pass\":\\n        return original_joke\\n\\n    improved_joke = improve_joke(original_joke).result()\\n    return polish_joke(improved_joke).result()\\n\\n# Invoke\\nfor step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/332fa4fc-b6ca-416e-baa3-161625e69163/r\\n\\n\\n\\nParallelization¶\\nWith parallelization, LLMs work simultaneously on a task:\\n\\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs.\\nWhen to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/3be2e53c-ca94-40dd-934f-82ff87fac277/r\\nResources:\\nDocumentation\\nSee our documentation on parallelization here.\\nLangChain Academy\\nSee our lesson on parallelization here.\\n\\n\\n@task\\ndef call_llm_1(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_2(topic: str):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n    msg = llm.invoke(f\"Write a story about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_3(topic):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n    msg = llm.invoke(f\"Write a poem about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef aggregator(topic, joke, story, poem):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {topic}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{story}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{joke}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{poem}\"\\n    return combined\\n\\n\\n# Build workflow\\n@entrypoint()\\ndef parallel_workflow(topic: str):\\n    joke_fut = call_llm_1(topic)\\n    story_fut = call_llm_2(topic)\\n    poem_fut = call_llm_3(topic)\\n    return aggregator(\\n        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\\n    ).result()\\n\\n# Invoke\\nfor step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/623d033f-e814-41e9-80b1-75e6abb67801/r\\n\\n\\n\\nRouting¶\\nRouting classifies an input and directs it to a followup task. As noted in the Anthropic blog on Building Effective Agents: \\n\\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\\nWhen to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import Literal\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/c4580b74-fe91-47e4-96fe-7fac598d509c/r\\nResources:\\nLangChain Academy\\nSee our lesson on routing here.\\nExamples\\nHere is RAG workflow that routes questions. See our video here.\\n\\n\\nfrom typing_extensions import Literal\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n@task\\ndef llm_call_1(input_: str):\\n    \"\"\"Write a story\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_2(input_: str):\\n    \"\"\"Write a joke\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_3(input_: str):\\n    \"\"\"Write a poem\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\ndef llm_call_router(input_: str):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=input_),\\n        ]\\n    )\\n    return decision.step\\n\\n\\n# Create workflow\\n@entrypoint()\\ndef router_workflow(input_: str):\\n    next_step = llm_call_router(input_)\\n    if next_step == \"story\":\\n        llm_call = llm_call_1\\n    elif next_step == \"joke\":\\n        llm_call = llm_call_2\\n    elif next_step == \"poem\":\\n        llm_call = llm_call_3\\n\\n    return llm_call(input_).result()\\n\\n# Invoke\\nfor step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/5e2eb979-82dd-402c-b1a0-a8cceaf2a28a/r\\n\\n\\n\\nOrchestrator-Worker¶\\nWith orchestrator-worker, an orchestrator breaks down a task and delegates each sub-task to workers. As noted in the Anthropic blog on Building Effective Agents: \\n\\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\\nWhen to use this workflow: This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren\\'t pre-defined, but determined by the orchestrator based on the specific input.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\nCreating Workers in LangGraph\\nBecause orchestrator-worker workflows are common, LangGraph has the Send API to support this. It lets you dynamically create worker nodes and send each one a specific input. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. As you can see below, we iterate over a list of sections and Send each to a worker node. See further documentation here and here.\\nfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/78cbcfc3-38bf-471d-b62a-b299b144237d/r\\nResources:\\nLangChain Academy\\nSee our lesson on orchestrator-worker here.\\nExamples\\nHere is a project that uses orchestrator-worker for report planning and writing. See our video here.\\n\\n\\nfrom typing import List\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\n@task\\ndef orchestrator(topic: str):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {topic}\"),\\n        ]\\n    )\\n\\n    return report_sections.sections\\n\\n\\n@task\\ndef llm_call(section: Section):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    result = llm.invoke(\\n        [\\n            SystemMessage(content=\"Write a report section.\"),\\n            HumanMessage(\\n                content=f\"Here is the section name: {section.name} and description: {section.description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return result.content\\n\\n\\n@task\\ndef synthesizer(completed_sections: list[str]):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n    final_report = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n    return final_report\\n\\n\\n@entrypoint()\\ndef orchestrator_worker(topic: str):\\n    sections = orchestrator(topic).result()\\n    section_futures = [llm_call(section) for section in sections]\\n    final_report = synthesizer(\\n        [section_fut.result() for section_fut in section_futures]\\n    ).result()\\n    return final_report\\n\\n# Invoke\\nreport = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\\nfrom IPython.display import Markdown\\nMarkdown(report)\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/75a636d0-6179-4a12-9836-e0aa571e87c5/r\\n\\n\\n\\nEvaluator-optimizer¶\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop:\\n\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\\nWhen to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/86ab3e60-2000-4bff-b988-9b89a3269789/r\\nResources:\\nExamples\\nHere is an assistant that uses evaluator-optimizer to improve a report. See our video here.\\nHere is a RAG workflow that grades answers for hallucinations or errors. See our video here.\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\n@task\\ndef llm_call_generator(topic: str, feedback: Feedback):\\n    \"\"\"LLM generates a joke\"\"\"\\n    if feedback:\\n        msg = llm.invoke(\\n            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef llm_call_evaluator(joke: str):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\\n    return feedback\\n\\n\\n@entrypoint()\\ndef optimizer_workflow(topic: str):\\n    feedback = None\\n    while True:\\n        joke = llm_call_generator(topic, feedback).result()\\n        feedback = llm_call_evaluator(joke).result()\\n        if feedback.grade == \"funny\":\\n            break\\n\\n    return joke\\n\\n# Invoke\\nfor step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/f66830be-4339-4a6b-8a93-389ce5ae27b4/r\\n\\n\\n\\nAgent¶\\nAgents are typically implemented as an LLM performing actions (via tool-calling) based on environmental feedback in a loop. As noted in the Anthropic blog on Building Effective Agents:\\n\\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully.\\nWhen to use agents: Agents can be used for open-ended problems where it’s difficult or impossible to predict the required number of steps, and where you can’t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents\\' autonomy makes them ideal for scaling tasks in trusted environments.\\n\\n\\nAPI Reference: tool\\nfrom langchain_core.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional API\\n\\n\\nfrom langgraph.graph import MessagesState\\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"environment\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"Action\"\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"environment\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    {\\n        # Name returned by should_continue : Name of next node to visit\\n        \"Action\": \"environment\",\\n        END: END,\\n    },\\n)\\nagent_builder.add_edge(\"environment\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/051f0391-6761-4f8c-a53b-22231b016690/r\\nResources:\\nLangChain Academy\\nSee our lesson on agents here.\\nExamples\\nHere is a project that uses a tool calling agent to create / store long-term memories.\\n\\n\\nfrom langgraph.graph import add_messages\\nfrom langchain_core.messages import (\\n    SystemMessage,\\n    HumanMessage,\\n    BaseMessage,\\n    ToolCall,\\n)\\n\\n\\n@task\\ndef call_llm(messages: list[BaseMessage]):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n    return llm_with_tools.invoke(\\n        [\\n            SystemMessage(\\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n            )\\n        ]\\n        + messages\\n    )\\n\\n\\n@task\\ndef call_tool(tool_call: ToolCall):\\n    \"\"\"Performs the tool call\"\"\"\\n    tool = tools_by_name[tool_call[\"name\"]]\\n    return tool.invoke(tool_call)\\n\\n\\n@entrypoint()\\ndef agent(messages: list[BaseMessage]):\\n    llm_response = call_llm(messages).result()\\n\\n    while True:\\n        if not llm_response.tool_calls:\\n            break\\n\\n        # Execute tools\\n        tool_result_futures = [\\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\\n        ]\\n        tool_results = [fut.result() for fut in tool_result_futures]\\n        messages = add_messages(messages, [llm_response, *tool_results])\\n        llm_response = call_llm(messages).result()\\n\\n    messages = add_messages(messages, llm_response)\\n    return messages\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\\n    print(chunk)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/42ae8bf9-3935-4504-a081-8ddbcbfc8b2e/r\\n\\n\\n\\nPre-built¶\\nLangGraph also provides a pre-built method for creating an agent as defined above (using the create_react_agent function):\\nhttps://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\\nAPI Reference: create_react_agent\\nfrom langgraph.prebuilt import create_react_agent\\n\\n# Pass in:\\n# (1) the augmented LLM with tools\\n# (2) the tools list (which is used to create the tool node)\\npre_built_agent = create_react_agent(llm, tools=tools)\\n\\n# Show the agent\\ndisplay(Image(pre_built_agent.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = pre_built_agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/graph-api/#map-reduce-and-the-send-api', 'title': 'Use the Graph API', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse the Graph API\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Use the Graph API\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Guides\\n          \\n\\n\\n\\n\\n\\n    Agent development\\n    \\n  \\n\\n\\n\\n\\n\\n            Agent development\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n    Run an agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph APIs\\n    \\n  \\n\\n\\n\\n\\n\\n            LangGraph APIs\\n          \\n\\n\\n\\n\\n\\n    Graph API\\n    \\n  \\n\\n\\n\\n\\n\\n            Graph API\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Use the Graph API\\n    \\n  \\n\\n\\n\\n\\n    Use the Graph API\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Setup\\n    \\n\\n\\n\\n\\n\\n      Define and update state\\n    \\n\\n\\n\\n\\n\\n\\n      Define state\\n    \\n\\n\\n\\n\\n\\n      Update state\\n    \\n\\n\\n\\n\\n\\n      Process state updates with reducers\\n    \\n\\n\\n\\n\\n\\n\\n      MessagesState\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Define input and output schemas\\n    \\n\\n\\n\\n\\n\\n      Pass private state between nodes\\n    \\n\\n\\n\\n\\n\\n      Use Pydantic models for graph state\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Add runtime configuration\\n    \\n\\n\\n\\n\\n\\n      Add retry policies\\n    \\n\\n\\n\\n\\n\\n      Add node caching\\n    \\n\\n\\n\\n\\n\\n      Create a sequence of steps\\n    \\n\\n\\n\\n\\n\\n      Create branches\\n    \\n\\n\\n\\n\\n\\n\\n      Run graph nodes in parallel\\n    \\n\\n\\n\\n\\n\\n      Defer node execution\\n    \\n\\n\\n\\n\\n\\n      Conditional branching\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Map-Reduce and the Send API\\n    \\n\\n\\n\\n\\n\\n      Create and control loops\\n    \\n\\n\\n\\n\\n\\n\\n      Impose a recursion limit\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Async\\n    \\n\\n\\n\\n\\n\\n      Combine control flow and state updates with Command\\n    \\n\\n\\n\\n\\n\\n\\n      Navigate to a node in a parent graph\\n    \\n\\n\\n\\n\\n\\n      Use inside tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Visualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Functional API\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Runtime\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Core capabilities\\n    \\n  \\n\\n\\n\\n\\n\\n            Core capabilities\\n          \\n\\n\\n\\n\\n    Streaming\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Persistence\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Durable execution\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Memory\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Context\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Models\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tools\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Subgraphs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Multi-agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    MCP\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tracing\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Platform-only capabilities\\n    \\n  \\n\\n\\n\\n\\n\\n            Platform-only capabilities\\n          \\n\\n\\n\\n\\n    LangGraph Platform\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Authentication & access control\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Assistants\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Double-texting\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Webhooks\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Cron jobs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Server customization\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Data management\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Deployment\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Setup\\n    \\n\\n\\n\\n\\n\\n      Define and update state\\n    \\n\\n\\n\\n\\n\\n\\n      Define state\\n    \\n\\n\\n\\n\\n\\n      Update state\\n    \\n\\n\\n\\n\\n\\n      Process state updates with reducers\\n    \\n\\n\\n\\n\\n\\n\\n      MessagesState\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Define input and output schemas\\n    \\n\\n\\n\\n\\n\\n      Pass private state between nodes\\n    \\n\\n\\n\\n\\n\\n      Use Pydantic models for graph state\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Add runtime configuration\\n    \\n\\n\\n\\n\\n\\n      Add retry policies\\n    \\n\\n\\n\\n\\n\\n      Add node caching\\n    \\n\\n\\n\\n\\n\\n      Create a sequence of steps\\n    \\n\\n\\n\\n\\n\\n      Create branches\\n    \\n\\n\\n\\n\\n\\n\\n      Run graph nodes in parallel\\n    \\n\\n\\n\\n\\n\\n      Defer node execution\\n    \\n\\n\\n\\n\\n\\n      Conditional branching\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Map-Reduce and the Send API\\n    \\n\\n\\n\\n\\n\\n      Create and control loops\\n    \\n\\n\\n\\n\\n\\n\\n      Impose a recursion limit\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Async\\n    \\n\\n\\n\\n\\n\\n      Combine control flow and state updates with Command\\n    \\n\\n\\n\\n\\n\\n\\n      Navigate to a node in a parent graph\\n    \\n\\n\\n\\n\\n\\n      Use inside tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Visualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow to use the graph API¶\\nThis guide demonstrates the basics of LangGraph\\'s Graph API. It walks through state, as well as composing common graph structures such as sequences, branches, and loops. It also covers LangGraph\\'s control features, including the Send API for map-reduce workflows and the Command API for combining state updates with \"hops\" across nodes.\\nSetup¶\\nInstall langgraph:\\npip install -U langgraph\\n\\n\\nSet up LangSmith for better debugging\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the docs.\\n\\nDefine and update state¶\\nHere we show how to define and update state in LangGraph. We will demonstrate:\\n\\nHow to use state to define a graph\\'s schema\\nHow to use reducers to control how state updates are processed.\\n\\nDefine state¶\\nState in LangGraph can be a TypedDict, Pydantic model, or dataclass. Below we will use TypedDict. See this section for detail on using Pydantic.\\nBy default, graphs will have the same input and output schema, and the state determines that schema. See this section for how to define distinct input and output schemas.\\nLet\\'s consider a simple example using messages. This represents a versatile formulation of state for many LLM applications. See our concepts page for more detail.\\nAPI Reference: AnyMessage\\nfrom langchain_core.messages import AnyMessage\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    messages: list[AnyMessage]\\n    extra_field: int\\n\\nThis state tracks a list of message objects, as well as an extra integer field.\\nUpdate state¶\\nLet\\'s build an example graph with a single node. Our node is just a Python function that reads our graph\\'s state and makes updates to it. The first argument to this function will always be the state:\\nAPI Reference: AIMessage\\nfrom langchain_core.messages import AIMessage\\n\\ndef node(state: State):\\n    messages = state[\"messages\"]\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": messages + [new_message], \"extra_field\": 10}\\n\\nThis node simply appends a message to our message list, and populates an extra field.\\n\\nImportant\\nNodes should return updates to the state directly, instead of mutating the state.\\n\\nLet\\'s next define a simple graph containing this node. We use StateGraph to define a graph that operates on this state. We then use add_node populate our graph.\\nAPI Reference: StateGraph\\nfrom langgraph.graph import StateGraph\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(node)\\nbuilder.set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our graph. See this section for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIn this case, our graph just executes a single node. Let\\'s proceed with a simple invocation:\\nAPI Reference: HumanMessage\\nfrom langchain_core.messages import HumanMessage\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\nresult\\n\\n{\\'messages\\': [HumanMessage(content=\\'Hi\\'), AIMessage(content=\\'Hello!\\')], \\'extra_field\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by updating a single key of the state.\\nWe receive the entire state in the invocation result.\\n\\nFor convenience, we frequently inspect the content of message objects via pretty-print:\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nProcess state updates with reducers¶\\nEach key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.\\nFor TypedDict state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.\\nIn the earlier example, our node updated the \"messages\" key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:\\nfrom typing_extensions import Annotated\\n\\ndef add(left, right):\\n    \"\"\"Can also import `add` from the `operator` built-in.\"\"\"\\n    return left + right\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add]\\n    extra_field: int\\n\\nNow our node can be simplified:\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\nAPI Reference: START\\nfrom langgraph.graph import START\\n\\ngraph = StateGraph(State).add_node(node).add_edge(START, \"node\").compile()\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nMessagesState¶\\nIn practice, there are additional considerations for updating lists of messages:\\n\\nWe may wish to update an existing message in the state.\\nWe may want to accept short-hands for message formats, such as OpenAI format.\\n\\nLangGraph includes a built-in reducer add_messages that handles these considerations:\\nAPI Reference: add_messages\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n    extra_field: int\\n\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\ngraph = StateGraph(State).add_node(node).set_entry_point(\"node\").compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hi\"}\\n\\nresult = graph.invoke({\"messages\": [input_message]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nThis is a versatile representation of state for applications involving chat models. LangGraph includes a pre-built MessagesState for convenience, so that we can have:\\nfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    extra_field: int\\n\\nDefine input and output schemas¶\\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it\\'s also possible to define distinct input and output schemas for a graph.\\nWhen distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.\\nBelow, we\\'ll see how to define distinct input and output schema.\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# Define the schema for the input\\nclass InputState(TypedDict):\\n    question: str\\n\\n# Define the schema for the output\\nclass OutputState(TypedDict):\\n    answer: str\\n\\n# Define the overall schema, combining both input and output\\nclass OverallState(InputState, OutputState):\\n    pass\\n\\n# Define the node that processes the input and generates an answer\\ndef answer_node(state: InputState):\\n    # Example answer and an extra key\\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\\n\\n# Build the graph with input and output schemas specified\\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\\nbuilder.add_node(answer_node)  # Add the answer node\\nbuilder.add_edge(START, \"answer_node\")  # Define the starting edge\\nbuilder.add_edge(\"answer_node\", END)  # Define the ending edge\\ngraph = builder.compile()  # Compile the graph\\n\\n# Invoke the graph with an input and print the result\\nprint(graph.invoke({\"question\": \"hi\"}))\\n\\n{\\'answer\\': \\'bye\\'}\\n\\nNotice that the output of invoke only includes the output schema.\\nPass private state between nodes¶\\nIn some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn\\'t need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.\\nBelow, we\\'ll create an example sequential graph consisting of three nodes (node_1, node_2 and node_3), where private data is passed between the first two steps (node_1 and node_2), while the third step (node_3) only has access to the public overall state.\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(TypedDict):\\n    a: str\\n\\n# Output from node_1 contains private data that is not part of the overall state\\nclass Node1Output(TypedDict):\\n    private_data: str\\n\\n# The private data is only shared between node_1 and node_2\\ndef node_1(state: OverallState) -> Node1Output:\\n    output = {\"private_data\": \"set by node_1\"}\\n    print(f\"Entered node `node_1`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 2 input only requests the private data available after node_1\\nclass Node2Input(TypedDict):\\n    private_data: str\\n\\ndef node_2(state: Node2Input) -> OverallState:\\n    output = {\"a\": \"set by node_2\"}\\n    print(f\"Entered node `node_2`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 3 only has access to the overall state (no access to private data from node_1)\\ndef node_3(state: OverallState) -> OverallState:\\n    output = {\"a\": \"set by node_3\"}\\n    print(f\"Entered node `node_3`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Connect nodes in a sequence\\n# node_2 accepts private data from node_1, whereas\\n# node_3 does not see the private data.\\nbuilder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])\\nbuilder.add_edge(START, \"node_1\")\\ngraph = builder.compile()\\n\\n# Invoke the graph with the initial state\\nresponse = graph.invoke(\\n    {\\n        \"a\": \"set at start\",\\n    }\\n)\\n\\nprint()\\nprint(f\"Output of graph invocation: {response}\")\\n\\nEntered node `node_1`:\\n    Input: {\\'a\\': \\'set at start\\'}.\\n    Returned: {\\'private_data\\': \\'set by node_1\\'}\\nEntered node `node_2`:\\n    Input: {\\'private_data\\': \\'set by node_1\\'}.\\n    Returned: {\\'a\\': \\'set by node_2\\'}\\nEntered node `node_3`:\\n    Input: {\\'a\\': \\'set by node_2\\'}.\\n    Returned: {\\'a\\': \\'set by node_3\\'}\\n\\nOutput of graph invocation: {\\'a\\': \\'set by node_3\\'}\\n\\nUse Pydantic models for graph state¶\\nA StateGraph accepts a state_schema argument on initialization that specifies the \"shape\" of the state that the nodes in the graph can access and update.\\nIn our examples, we typically use a python-native TypedDict or dataclass for state_schema, but state_schema can be any type.\\nHere, we\\'ll see how a Pydantic BaseModel can be used for state_schema to add run-time validation on inputs.\\n\\nKnown Limitations\\n\\nCurrently, the output of the graph will NOT be an instance of a pydantic model.\\nRun-time validation only occurs on inputs into nodes, not on the outputs.\\nThe validation error trace from pydantic does not show which node the error arises in.\\nPydantic\\'s recursive validation can be slow. For performance-sensitive applications, you may want to consider using a dataclass instead.\\n\\n\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\nfrom pydantic import BaseModel\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(BaseModel):\\n    a: str\\n\\ndef node(state: OverallState):\\n    return {\"a\": \"goodbye\"}\\n\\n# Build the state graph\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(node)  # node_1 is the first node\\nbuilder.add_edge(START, \"node\")  # Start the graph with node_1\\nbuilder.add_edge(\"node\", END)  # End the graph after node_1\\ngraph = builder.compile()\\n\\n# Test the graph with a valid input\\ngraph.invoke({\"a\": \"hello\"})\\n\\nInvoke the graph with an invalid input\\ntry:\\n    graph.invoke({\"a\": 123})  # Should be a string\\nexcept Exception as e:\\n    print(\"An exception was raised because `a` is an integer rather than a string.\")\\n    print(e)\\n\\nAn exception was raised because `a` is an integer rather than a string.\\n1 validation error for OverallState\\na\\n  Input should be a valid string [type=string_type, input_value=123, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type\\n\\nSee below for additional features of Pydantic model state:\\n\\nSerialization Behavior\\nWhen using Pydantic models as state schemas, it\\'s important to understand how serialization works, especially when:\\n- Passing Pydantic objects as inputs\\n- Receiving outputs from the graph\\n- Working with nested Pydantic models\\nLet\\'s see these behaviors in action.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass NestedModel(BaseModel):\\n    value: str\\n\\nclass ComplexState(BaseModel):\\n    text: str\\n    count: int\\n    nested: NestedModel\\n\\ndef process_node(state: ComplexState):\\n    # Node receives a validated Pydantic object\\n    print(f\"Input state type: {type(state)}\")\\n    print(f\"Nested type: {type(state.nested)}\")\\n    # Return a dictionary update\\n    return {\"text\": state.text + \" processed\", \"count\": state.count + 1}\\n\\n# Build the graph\\nbuilder = StateGraph(ComplexState)\\nbuilder.add_node(\"process\", process_node)\\nbuilder.add_edge(START, \"process\")\\nbuilder.add_edge(\"process\", END)\\ngraph = builder.compile()\\n\\n# Create a Pydantic instance for input\\ninput_state = ComplexState(text=\"hello\", count=0, nested=NestedModel(value=\"test\"))\\nprint(f\"Input object type: {type(input_state)}\")\\n\\n# Invoke graph with a Pydantic instance\\nresult = graph.invoke(input_state)\\nprint(f\"Output type: {type(result)}\")\\nprint(f\"Output content: {result}\")\\n\\n# Convert back to Pydantic model if needed\\noutput_model = ComplexState(**result)\\nprint(f\"Converted back to Pydantic: {type(output_model)}\")\\n\\n\\n\\nRuntime Type Coercion\\nPydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you\\'re not aware of it.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass CoercionExample(BaseModel):\\n    # Pydantic will coerce string numbers to integers\\n    number: int\\n    # Pydantic will parse string booleans to bool\\n    flag: bool\\n\\ndef inspect_node(state: CoercionExample):\\n    print(f\"number: {state.number} (type: {type(state.number)})\")\\n    print(f\"flag: {state.flag} (type: {type(state.flag)})\")\\n    return {}\\n\\nbuilder = StateGraph(CoercionExample)\\nbuilder.add_node(\"inspect\", inspect_node)\\nbuilder.add_edge(START, \"inspect\")\\nbuilder.add_edge(\"inspect\", END)\\ngraph = builder.compile()\\n\\n# Demonstrate coercion with string inputs that will be converted\\nresult = graph.invoke({\"number\": \"42\", \"flag\": \"true\"})\\n\\n# This would fail with a validation error\\ntry:\\n    graph.invoke({\"number\": \"not-a-number\", \"flag\": \"true\"})\\nexcept Exception as e:\\n    print(f\"\\\\nExpected validation error: {e}\")\\n\\n\\n\\nWorking with Message Models\\nWhen working with LangChain message types in your state schema, there are important considerations for serialization. You should use AnyMessage (rather than BaseMessage) for proper serialization/deserialization when using message objects over the wire.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, AIMessage, AnyMessage\\nfrom typing import List\\n\\nclass ChatState(BaseModel):\\n    messages: List[AnyMessage]\\n    context: str\\n\\ndef add_message(state: ChatState):\\n    return {\"messages\": state.messages + [AIMessage(content=\"Hello there!\")]}\\n\\nbuilder = StateGraph(ChatState)\\nbuilder.add_node(\"add_message\", add_message)\\nbuilder.add_edge(START, \"add_message\")\\nbuilder.add_edge(\"add_message\", END)\\ngraph = builder.compile()\\n\\n# Create input with a message\\ninitial_state = ChatState(\\n    messages=[HumanMessage(content=\"Hi\")], context=\"Customer support chat\"\\n)\\n\\nresult = graph.invoke(initial_state)\\nprint(f\"Output: {result}\")\\n\\n# Convert back to Pydantic model to see message types\\noutput_model = ChatState(**result)\\nfor i, msg in enumerate(output_model.messages):\\n    print(f\"Message {i}: {type(msg).__name__} - {msg.content}\")\\n\\n\\nAdd runtime configuration¶\\nSometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, without polluting the graph state with these parameters.\\nTo add runtime configuration:\\n\\nSpecify a schema for your configuration\\nAdd the configuration to the function signature for nodes or conditional edges\\nPass the configuration into the graph.\\n\\nSee below for a simple example:\\nAPI Reference: END | StateGraph | START\\nfrom langgraph.graph import END, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n# 1. Specify config schema\\nclass ContextSchema(TypedDict):\\n    my_runtime_value: str\\n\\n# 2. Define a graph that accesses the config in a node\\nclass State(TypedDict):\\n    my_state_value: str\\n\\ndef node(state: State, runtime: Runtime[ContextSchema]):\\n    if runtime.context[\"my_runtime_value\"] == \"a\":\\n        return {\"my_state_value\": 1}\\n    elif runtime.context[\"my_runtime_value\"] == \"b\":\\n        return {\"my_state_value\": 2}\\n    else:\\n        raise ValueError(\"Unknown values.\")\\n\\nbuilder = StateGraph(State, context_schema=ContextSchema)\\nbuilder.add_node(node)\\nbuilder.add_edge(START, \"node\")\\nbuilder.add_edge(\"node\", END)\\n\\ngraph = builder.compile()\\n\\n# 3. Pass in configuration at runtime:\\nprint(graph.invoke({}, context={\"my_runtime_value\": \"a\"}))\\nprint(graph.invoke({}, context={\"my_runtime_value\": \"b\"}))\\n\\n{\\'my_state_value\\': 1}\\n{\\'my_state_value\\': 2}\\n\\n\\nExtended example: specifying LLM at runtime\\nBelow we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.\\nfrom dataclasses import dataclass\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, END, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n@dataclass\\nclass ContextSchema:\\n    model_provider: str = \"anthropic\"\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\\n    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\\n    model = MODELS[runtime.context.model_provider]\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, context_schema=ContextSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\n# With no configuration, uses default (Anthropic)\\nresponse_1 = graph.invoke({\"messages\": [input_message]})[\"messages\"][-1]\\n# Or, can set OpenAI\\nresponse_2 = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\"})[\"messages\"][-1]\\n\\nprint(response_1.response_metadata[\"model_name\"])\\nprint(response_2.response_metadata[\"model_name\"])\\n\\nclaude-3-5-haiku-20241022\\ngpt-4.1-mini-2025-04-14\\n\\n\\n\\nExtended example: specifying model and system message at runtime\\nBelow we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.\\nfrom dataclasses import dataclass\\nfrom typing import Optional\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.messages import SystemMessage\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n@dataclass\\nclass ContextSchema:\\n    model_provider: str = \"anthropic\"\\n    system_message: str | None = None\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\\n    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\\n    model = MODELS[runtime.context.model_provider]\\n    messages = state[\"messages\"]\\n    if (system_message := runtime.context.system_message):\\n        messages = [SystemMessage(system_message)] + messages\\n    response = model.invoke(messages)\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, context_schema=ContextSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\nresponse = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\", \"system_message\": \"Respond in Italian.\"})\\nfor message in response[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nhi\\n================================== Ai Message ==================================\\n\\nCiao! Come posso aiutarti oggi?\\n\\n\\nAdd retry policies¶\\nThere are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes.\\nTo configure a retry policy, pass the retry_policy parameter to the add_node. The retry_policy parameter takes in a RetryPolicy named tuple object. Below we instantiate a RetryPolicy object with the default parameters and associate it with a node:\\nfrom langgraph.pregel import RetryPolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    retry_policy=RetryPolicy(),\\n)\\n\\nBy default, the retry_on parameter uses the default_retry_on function, which retries on any exception except for the following:\\n\\nValueError\\nTypeError\\nArithmeticError\\nImportError\\nLookupError\\nNameError\\nSyntaxError\\nRuntimeError\\nReferenceError\\nStopIteration\\nStopAsyncIteration\\nOSError\\n\\nIn addition, for exceptions from popular http request libraries such as requests and httpx it only retries on 5xx status codes.\\n\\nExtended example: customizing retry policies\\nConsider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes:\\nimport sqlite3\\nfrom typing_extensions import TypedDict\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.pregel import RetryPolicy\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_core.messages import AIMessage\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///:memory:\")\\nmodel = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\\n\\ndef query_database(state: MessagesState):\\n    query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\\n    return {\"messages\": [AIMessage(content=query_result)]}\\n\\ndef call_model(state: MessagesState):\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\n# Define a new graph\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(\\n    \"query_database\",\\n    query_database,\\n    retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),\\n)\\nbuilder.add_node(\"model\", call_model, retry_policy=RetryPolicy(max_attempts=5))\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", \"query_database\")\\nbuilder.add_edge(\"query_database\", END)\\ngraph = builder.compile()\\n\\n\\nAdd node caching¶\\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph.\\nTo configure a cache policy, pass the cache_policy parameter to the add_node function. In the following example, a CachePolicy object is instantiated with a time to live of 120 seconds and the default key_func generator. Then it is associated with a node:\\nfrom langgraph.types import CachePolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    cache_policy=CachePolicy(ttl=120),\\n)\\n\\nThen, to enable node-level caching for a graph, set the cache argument when compiling the graph. The example below uses InMemoryCache to set up a graph with in-memory cache, but SqliteCache is also available.\\nfrom langgraph.cache.memory import InMemoryCache\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nCreate a sequence of steps¶\\n\\nPrerequisites\\nThis guide assumes familiarity with the above section on state.\\n\\nHere we demonstrate how to construct a simple sequence of steps. We will show:\\n\\nHow to build a sequential graph\\nBuilt-in short-hand for constructing similar graphs.\\n\\nTo add a sequence of nodes, we use the .add_node and .add_edge methods of our graph:\\nAPI Reference: START | StateGraph\\nfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\nWe can also use the built-in shorthand .add_sequence:\\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\n\\nWhy split application steps into a sequence with LangGraph?\\nLangGraph makes it easy to add an underlying persistence layer to your application.\\nThis allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:\\n\\nHow state updates are checkpointed\\nHow interruptions are resumed in human-in-the-loop workflows\\nHow we can \"rewind\" and branch-off executions using LangGraph\\'s time travel features\\n\\nThey also determine how execution steps are streamed, and how your application is visualized\\nand debugged using LangGraph Studio.\\n\\nLet\\'s demonstrate an end-to-end example. We will create a sequence of three steps:\\n\\nPopulate a value in a key of the state\\nUpdate the same value\\nPopulate a different value\\n\\nLet\\'s first define our state. This governs the schema of the graph, and can also specify how to apply updates. See this section for more detail.\\nIn our case, we will just keep track of two values:\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    value_1: str\\n    value_2: int\\n\\nOur nodes are just Python functions that read our graph\\'s state and make updates to it. The first argument to this function will always be the state:\\ndef step_1(state: State):\\n    return {\"value_1\": \"a\"}\\n\\ndef step_2(state: State):\\n    current_value_1 = state[\"value_1\"]\\n    return {\"value_1\": f\"{current_value_1} b\"}\\n\\ndef step_3(state: State):\\n    return {\"value_2\": 10}\\n\\n\\nNote\\nNote that when issuing updates to the state, each node can just specify the value of the key it wishes to update.\\nBy default, this will overwrite the value of the corresponding key. You can also use reducers to control how updates are processed— for example, you can append successive updates to a key instead. See this section for more detail.\\n\\nFinally, we define the graph. We use StateGraph to define a graph that operates on this state.\\nWe will then use add_node and add_edge to populate our graph and define its control flow.\\nAPI Reference: START | StateGraph\\nfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\n\\nSpecifying custom names\\nYou can specify custom names for nodes using .add_node:\\nbuilder.add_node(\"my_node\", step_1)\\n\\n\\nNote that:\\n\\n.add_edge takes the names of nodes, which for functions defaults to node.__name__.\\nWe must specify the entry point of the graph. For this we add an edge with the START node.\\nThe graph halts when there are no more nodes to execute.\\n\\nWe next compile our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a checkpointer, it would also be passed in here.\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our sequence. See this guide for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nLet\\'s proceed with a simple invocation:\\ngraph.invoke({\"value_1\": \"c\"})\\n\\n{\\'value_1\\': \\'a b\\', \\'value_2\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.\\nThe value we passed in was overwritten by the first node.\\nThe second node updated the value.\\nThe third node populated a different value.\\n\\n\\nBuilt-in shorthand\\nlanggraph>=0.2.46 includes a built-in short-hand add_sequence for adding node sequences. You can compile the same graph as follows:\\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"value_1\": \"c\"})    \\n\\n\\nCreate branches¶\\nParallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.\\nRun graph nodes in parallel¶\\nIn this example, we fan out from Node A to B and C and then fan in to D. With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on state reducers for more detail on updating state with reducers.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nWith the reducer, you can see that the values added in each node are accumulated.\\ngraph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\n\\nAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\']\\n\\n\\nNote\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. Because they are in the same step, node \"d\" executes after both \"b\" and \"c\" are finished.\\nImportantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.\\n\\n\\nException handling?\\nLangGraph executes nodes within supersteps, meaning that while parallel branches are executed in parallel, the entire superstep is transactional. If any of these branches raises an exception, none of the updates are applied to the state (the entire superstep errors).\\nImportantly, when using a checkpointer, results from successful nodes within a superstep are saved, and don\\'t repeat when resumed.\\nIf you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this:\\n\\nYou can write regular python code within your node to catch and handle exceptions.\\nYou can set a retry_policy to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn\\'t worry about performing redundant work.\\n\\nTogether, these let you perform parallel execution and fully control exception handling.\\n\\nDefer node execution¶\\nDeferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows.\\nThe above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let\\'s add a node \"b_2\" in the \"b\" branch:\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef b_2(state: State):\\n    print(f\\'Adding \"B_2\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B_2\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(b_2)\\nbuilder.add_node(c)\\nbuilder.add_node(d, defer=True)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"b_2\")\\nbuilder.add_edge(\"b_2\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\ngraph.invoke({\"aggregate\": []})\\n\\nAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"B_2\" to [\\'A\\', \\'B\\', \\'C\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\', \\'B_2\\']\\n\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. We set defer=True on node d so it will not execute until all pending tasks are finished. In this case, this means that \"d\" waits to execute until the entire \"b\" branch is finished.\\nConditional branching¶\\nIf your fan-out should vary at runtime based on the state, you can use add_conditional_edges to select one or more paths using the graph state. See example below, where node a generates a state update that determines the following node.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Literal, Sequence\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    # Add a key to the state. We will set this key to determine\\n    # how we branch.\\n    which: str\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"], \"which\": \"c\"}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"b\", END)\\nbuilder.add_edge(\"c\", END)\\n\\ndef conditional_edge(state: State) -> Literal[\"b\", \"c\"]:\\n    # Fill in arbitrary logic here that uses the state\\n    # to determine the next node\\n    return state[\"which\"]\\n\\nbuilder.add_conditional_edges(\"a\", conditional_edge)\\n\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nresult = graph.invoke({\"aggregate\": []})\\nprint(result)\\n\\nAdding \"A\" to []\\nAdding \"C\" to [\\'A\\']\\n{\\'aggregate\\': [\\'A\\', \\'C\\'], \\'which\\': \\'c\\'}\\n\\n\\nTip\\nYour conditional edges can route to multiple destination nodes. For example:\\ndef route_bc_or_cd(state: State) -> Sequence[str]:\\n    if state[\"which\"] == \"cd\":\\n        return [\"c\", \"d\"]\\n    return [\"b\", \"c\"]\\n\\n\\nMap-Reduce and the Send API¶\\nLangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:\\nAPI Reference: StateGraph | START | END | Send\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Send\\nfrom typing_extensions import TypedDict, Annotated\\nimport operator\\n\\nclass OverallState(TypedDict):\\n    topic: str\\n    subjects: list[str]\\n    jokes: Annotated[list[str], operator.add]\\n    best_selected_joke: str\\n\\ndef generate_topics(state: OverallState):\\n    return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]}\\n\\ndef generate_joke(state: OverallState):\\n    joke_map = {\\n        \"lions\": \"Why don\\'t lions like fast food? Because they can\\'t catch it!\",\\n        \"elephants\": \"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\",\\n        \"penguins\": \"Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\"\\n    }\\n    return {\"jokes\": [joke_map[state[\"subject\"]]]}\\n\\ndef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\\n\\ndef best_joke(state: OverallState):\\n    return {\"best_selected_joke\": \"penguins\"}\\n\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(\"generate_topics\", generate_topics)\\nbuilder.add_node(\"generate_joke\", generate_joke)\\nbuilder.add_node(\"best_joke\", best_joke)\\nbuilder.add_edge(START, \"generate_topics\")\\nbuilder.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\\nbuilder.add_edge(\"generate_joke\", \"best_joke\")\\nbuilder.add_edge(\"best_joke\", END)\\nbuilder.add_edge(\"generate_topics\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\n# Call the graph: here we call it to generate a list of jokes\\nfor step in graph.stream({\"topic\": \"animals\"}):\\n    print(step)\\n\\n{\\'generate_topics\\': {\\'subjects\\': [\\'lions\\', \\'elephants\\', \\'penguins\\']}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t lions like fast food? Because they can\\'t catch it!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\\'Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\\']}}\\n{\\'best_joke\\': {\\'best_selected_joke\\': \\'penguins\\'}}\\n\\nCreate and control loops¶\\nWhen creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition.\\nYou can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here.\\nLet\\'s consider a simple graph with a loop to better understand how these mechanisms work.\\n\\nTip\\nTo return the last value of your state instead of receiving a recursion limit error, see the next section.\\n\\nWhen creating a loop, you can include a conditional edge that specifies a termination condition:\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if termination_condition(state):\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nTo control the recursion limit, specify \"recursion_limit\" in the config. This will raise a GraphRecursionError, which you can catch and handle:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke(inputs, {\"recursion_limit\": 3})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nLet\\'s define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis architecture is similar to a ReAct agent in which node \"a\" is a tool-calling model, and node \"b\" represents the tools.\\nIn our route conditional edge, we specify that we should end after the \"aggregate\" list in the state passes a threshold length.\\nInvoking the graph, we see that we alternate between nodes \"a\" and \"b\" before terminating once we reach the termination condition.\\ngraph.invoke({\"aggregate\": []})\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\', \\'B\\']\\n\\nImpose a recursion limit¶\\nIn some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph\\'s recursion limit. This will raise a GraphRecursionError after a given number of supersteps. We can then catch and handle this exception:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\nExtended example: return state on hitting recursion limit\\nInstead of raising GraphRecursionError, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.\\nLangGraph implements a special RemainingSteps annotation. Under the hood, it creates a ManagedValue channel -- a state channel that will exist for the duration of our graph run and no longer.\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.managed.is_last_step import RemainingSteps\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    remaining_steps: RemainingSteps\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if state[\"remaining_steps\"] <= 2:\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\n# Test it out\\nresult = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nprint(result)\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\n{\\'aggregate\\': [\\'A\\', \\'B\\', \\'A\\']}\\n\\n\\n\\nExtended example: loops with branches\\nTo better understand how the recursion limit works, let\\'s consider a more complex example. Below we implement a loop, but one step fans out into two nodes:\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Node C sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Node D sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge([\"c\", \"d\"], \"a\")\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis graph looks complex, but can be conceptualized as loop of supersteps:\\n\\nNode A\\nNode B\\nNodes C and D\\nNode A\\n...\\n\\nWe have a loop of four supersteps, where nodes C and D are executed concurrently.\\nInvoking the graph as before, we see that we complete two full \"laps\" before hitting the termination condition:\\nresult = graph.invoke({\"aggregate\": []})\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nNode B sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\']\\nNode D sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\', \\'C\\', \\'D\\']\\n\\nHowever, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\nAsync¶\\nUsing the async programming paradigm can produce significant performance improvements when running IO-bound code concurrently (e.g., making concurrent API requests to a chat model provider).\\nTo convert a sync implementation of the graph to an async implementation, you will need to:\\n\\nUpdate nodes use async def instead of def.\\nUpdate the code inside to use await appropriately.\\nInvoke the graph with .ainvoke or .astream as desired.\\n\\nBecause many LangChain objects implement the Runnable Protocol which has async variants of all the sync methods it\\'s typically fairly quick to upgrade a sync graph to an async graph.\\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\\nOpenAIAnthropicAzureGoogle GeminiAWS Bedrock\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"openai:gpt-4.1\")\\n\\n👉 Read the OpenAI integration docs\\n\\n\\npip install -U \"langchain[anthropic]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\\n\\n👉 Read the Anthropic integration docs\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nllm = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\n👉 Read the Azure integration docs\\n\\n\\npip install -U \"langchain[google-genai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\\n\\n👉 Read the Google GenAI integration docs\\n\\n\\npip install -U \"langchain[aws]\"\\n\\nfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nllm = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\n👉 Read the AWS Bedrock integration docs\\n\\n\\n\\nAPI Reference: init_chat_model | StateGraph\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, StateGraph\\n\\nasync def node(state: MessagesState): # (1)!\\n    new_message = await llm.ainvoke(state[\"messages\"]) # (2)!\\n    return {\"messages\": [new_message]}\\n\\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\\nresult = await graph.ainvoke({\"messages\": [input_message]}) # (3)!\\n\\n\\nDeclare nodes to be async functions.\\nUse async invocations when available within the node.\\nUse async invocations on the graph object itself.\\n\\n\\nAsync streaming\\nSee the streaming guide for examples of streaming with async.\\n\\nCombine control flow and state updates with Command¶\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWe show an end-to-end example below. Let\\'s create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.\\nAPI Reference: StateGraph | START | Command\\nimport random\\nfrom typing_extensions import TypedDict, Literal\\nfrom langgraph.graph import StateGraph, START\\nfrom langgraph.types import Command\\n\\n# Define graph state\\nclass State(TypedDict):\\n    foo: str\\n\\n# Define the nodes\\n\\ndef node_a(state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\\n    print(\"Called A\")\\n    value = random.choice([\"b\", \"c\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"b\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        # this is the state update\\n        update={\"foo\": value},\\n        # this is a replacement for an edge\\n        goto=goto,\\n    )\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    return {\"foo\": state[\"foo\"] + \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": state[\"foo\"] + \"c\"}\\n\\nWe can now create the StateGraph with the above nodes. Notice that the graph doesn\\'t have conditional edges for routing! This is because control flow is defined with Command inside node_a.\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"node_a\")\\nbuilder.add_node(node_a)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n# NOTE: there are no edges between nodes A, B and C!\\n\\ngraph = builder.compile()\\n\\n\\nImportant\\nYou might have noticed that we used Command as a return type annotation, e.g. Command[Literal[\"node_b\", \"node_c\"]]. This is necessary for the graph rendering and tells LangGraph that node_a can navigate to node_b and node_c.\\n\\nfrom IPython.display import display, Image\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIf we run the graph multiple times, we\\'d see it take different paths (A -> B or A -> C) based on the random choice in node A.\\ngraph.invoke({\"foo\": \"\"})\\n\\nCalled A\\nCalled C\\n\\nNavigate to a node in a parent graph¶\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n\\nLet\\'s demonstrate this using the above example. We\\'ll do so by changing node_a in the above example into a single-node graph that we\\'ll add as a subgraph to our parent graph.\\n\\nState updates with Command.PARENT\\nWhen you send updates from a subgraph node to a parent graph node for a key that\\'s shared by both parent and subgraph state schemas, you must define a reducer for the key you\\'re updating in the parent graph state. See the example below.\\n\\nimport operator\\nfrom typing_extensions import Annotated\\n\\nclass State(TypedDict):\\n    # NOTE: we define a reducer here\\n    foo: Annotated[str, operator.add]\\n\\ndef node_a(state: State):\\n    print(\"Called A\")\\n    value = random.choice([\"a\", \"b\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"a\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        update={\"foo\": value},\\n        goto=goto,\\n        # this tells LangGraph to navigate to node_b or node_c in the parent graph\\n        # NOTE: this will navigate to the closest parent graph relative to the subgraph\\n        graph=Command.PARENT,\\n    )\\n\\nsubgraph = StateGraph(State).add_node(node_a).add_edge(START, \"node_a\").compile()\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    # NOTE: since we\\'ve defined a reducer, we don\\'t need to manually append\\n    # new characters to existing \\'foo\\' value. instead, reducer will append these\\n    # automatically (via operator.add)\\n    return {\"foo\": \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": \"c\"}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"subgraph\")\\nbuilder.add_node(\"subgraph\", subgraph)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"foo\": \"\"})\\n\\nCalled A\\nCalled C\\n\\nUse inside tools¶\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]}) from the tool:\\n@tool\\ndef lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\\n    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\\n    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\\n    return Command(\\n        update={\\n            # update the state keys\\n            \"user_info\": user_info,\\n            # update the message history\\n            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\\n        }\\n    )\\n\\n\\nImportant\\nYou MUST include messages (or any state key used for the message history) in Command.update when returning Command from a tool and the list of messages in messages MUST contain a ToolMessage. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).\\n\\nIf you are using tools that update state via Command, we recommend using prebuilt ToolNode which automatically handles tools returning Command objects and propagates them to the graph state. If you\\'re writing a custom node that calls tools, you would need to manually propagate Command objects returned by the tools as the update from the node.\\nVisualize your graph¶\\nHere we demonstrate how to visualize the graphs you create.\\nYou can visualize any arbitrary Graph, including StateGraph. Let\\'s have some fun by drawing fractals :).\\nAPI Reference: StateGraph | START | END | add_messages\\nimport random\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\nclass MyNode:\\n    def __init__(self, name: str):\\n        self.name = name\\n    def __call__(self, state: State):\\n        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\\n\\ndef route(state) -> Literal[\"entry_node\", \"__end__\"]:\\n    if len(state[\"messages\"]) > 10:\\n        return \"__end__\"\\n    return \"entry_node\"\\n\\ndef add_fractal_nodes(builder, current_node, level, max_level):\\n    if level > max_level:\\n        return\\n    # Number of nodes to create at this level\\n    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\\n    for i in range(num_nodes):\\n        nm = [\"A\", \"B\", \"C\"][i]\\n        node_name = f\"node_{current_node}_{nm}\"\\n        builder.add_node(node_name, MyNode(node_name))\\n        builder.add_edge(current_node, node_name)\\n        # Recursively add more nodes\\n        r = random.random()\\n        if r > 0.2 and level + 1 < max_level:\\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\\n        elif r > 0.05:\\n            builder.add_conditional_edges(node_name, route, node_name)\\n        else:\\n            # End\\n            builder.add_edge(node_name, \"__end__\")\\n\\ndef build_fractal_graph(max_level: int):\\n    builder = StateGraph(State)\\n    entry_point = \"entry_node\"\\n    builder.add_node(entry_point, MyNode(entry_point))\\n    builder.add_edge(START, entry_point)\\n    add_fractal_nodes(builder, entry_point, 1, max_level)\\n    # Optional: set a finish point if required\\n    builder.add_edge(entry_point, END)  # or any specific node\\n    return builder.compile()\\n\\napp = build_fractal_graph(3)\\n\\nMermaid¶\\nWe can also convert a graph class into Mermaid syntax.\\nprint(app.get_graph().draw_mermaid())\\n\\n%%{init: {\\'flowchart\\': {\\'curve\\': \\'linear\\'}}}%%\\ngraph TD;\\n    __start__([<p>__start__</p>]):::first\\n    entry_node(entry_node)\\n    node_entry_node_A(node_entry_node_A)\\n    node_entry_node_B(node_entry_node_B)\\n    node_node_entry_node_B_A(node_node_entry_node_B_A)\\n    node_node_entry_node_B_B(node_node_entry_node_B_B)\\n    node_node_entry_node_B_C(node_node_entry_node_B_C)\\n    __end__([<p>__end__</p>]):::last\\n    __start__ --> entry_node;\\n    entry_node --> __end__;\\n    entry_node --> node_entry_node_A;\\n    entry_node --> node_entry_node_B;\\n    node_entry_node_B --> node_node_entry_node_B_A;\\n    node_entry_node_B --> node_node_entry_node_B_B;\\n    node_entry_node_B --> node_node_entry_node_B_C;\\n    node_entry_node_A -.-> entry_node;\\n    node_entry_node_A -.-> __end__;\\n    node_node_entry_node_B_A -.-> entry_node;\\n    node_node_entry_node_B_A -.-> __end__;\\n    node_node_entry_node_B_B -.-> entry_node;\\n    node_node_entry_node_B_B -.-> __end__;\\n    node_node_entry_node_B_C -.-> entry_node;\\n    node_node_entry_node_B_C -.-> __end__;\\n    classDef default fill:#f2f0ff,line-height:1.2\\n    classDef first fill-opacity:0\\n    classDef last fill:#bfb6fc\\n\\nPNG¶\\nIf preferred, we could render the Graph into a  .png. Here we could use three options:\\n\\nUsing Mermaid.ink API (does not require additional packages)\\nUsing Mermaid + Pyppeteer (requires pip install pyppeteer)\\nUsing graphviz (which requires pip install graphviz)\\n\\nUsing Mermaid.Ink\\nBy default, draw_mermaid_png() uses Mermaid.Ink\\'s API to generate the diagram.\\nAPI Reference: CurveStyle | MermaidDrawMethod | NodeStyles\\nfrom IPython.display import Image, display\\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\\n\\ndisplay(Image(app.get_graph().draw_mermaid_png()))\\n\\n\\nUsing Mermaid + Pyppeteer\\nimport nest_asyncio\\n\\nnest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\\n\\ndisplay(\\n    Image(\\n        app.get_graph().draw_mermaid_png(\\n            curve_style=CurveStyle.LINEAR,\\n            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\\n            wrap_label_n_words=9,\\n            output_file_path=None,\\n            draw_method=MermaidDrawMethod.PYPPETEER,\\n            background_color=\"white\",\\n            padding=10,\\n        )\\n    )\\n)\\n\\nUsing Graphviz\\ntry:\\n    display(Image(app.get_graph().draw_png()))\\nexcept ImportError:\\n    print(\\n        \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\"\\n    )\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Overview\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Overview\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list = [item for sublist in docs for item in sublist]\n",
    "doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45eaa9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RCTS(chunk_size = 1000, chunk_overlap= 100)\n",
    "docs_split = splitter.split_documents(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b97bd584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HCL Tech\\Udemy\\Gen AI\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\HCL Tech\\Udemy\\Gen AI\\venv\\Lib\\site-packages\\langchain_mistralai\\embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vectorstore = FAISS.from_documents(docs_split, MistralAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77a9b265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever_vector_db_blog', description='Search and run information about LangGraph', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000251FF47FF60>, retriever=VectorStoreRetriever(tags=['FAISS', 'MistralAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000251C725BE30>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000251C6F91080>, retriever=VectorStoreRetriever(tags=['FAISS', 'MistralAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000251C725BE30>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retriever to retriever tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "retriever_tool = create_retriever_tool(retriever, name = \"retriever_vector_db_blog\", description = \"Search and run information about LangGraph\")\n",
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6071c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/why-langgraph/', 'title': 'Overview', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Overview\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n            Build a custom workflow\\n          \\n\\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n    1. Build a basic chatbot\\n    \\n  \\n\\n\\n\\n\\n\\n    2. Add tools\\n    \\n  \\n\\n\\n\\n\\n\\n    3. Add memory\\n    \\n  \\n\\n\\n\\n\\n\\n    4. Add human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n    5. Customize state\\n    \\n  \\n\\n\\n\\n\\n\\n    6. Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview¶\\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\\n\\nReliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\\nLow-level and extensible. Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\\nFirst-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.\\n\\nLearn LangGraph basics¶\\nTo get acquainted with LangGraph's key concepts and features, complete the following LangGraph basics tutorials series:\\n\\nBuild a basic chatbot\\nAdd tools\\nAdd memory\\nAdd human-in-the-loop controls\\nCustomize state\\nTime travel\\n\\nIn completing this series of tutorials, you will build a support chatbot in LangGraph that can:\\n\\n✅ Answer common questions by searching the web\\n✅ Maintain conversation state across calls  \\n✅ Route complex queries to a human for review  \\n✅ Use custom state to control its behavior  \\n✅ Rewind and explore alternative conversation paths  \\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Start with a prebuilt agent\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                1. Build a basic chatbot\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Workflows & agents', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows & agents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Workflows & agents\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows and Agents¶\\nThis guide reviews common patterns for agentic systems. In describing these systems, it can be useful to make a distinction between \"workflows\" and \"agents\". One way to think about this difference is nicely explained in Anthropic\\'s Building Effective Agents blog post:\\n\\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths.\\nAgents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\\n\\nHere is a simple way to visualize these differences:\\n\\nWhen building agents and workflows, LangGraph offers a number of benefits including persistence, streaming, and support for debugging as well as deployment.\\nSet up¶\\nYou can use any chat model that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.\\n\\nInstall dependencies\\npip install langchain_core langchain-anthropic langgraph \\n\\n\\nInitialize an LLM\\nAPI Reference: ChatAnthropic\\nimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n\\nBuilding Blocks: The Augmented LLM¶\\nLLM have augmentations that support building workflows and agents. These include structured outputs and tool calling, as shown in this image from the Anthropic blog on Building Effective Agents:\\n\\n# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\nPrompt chaining¶\\nIn prompt chaining, each LLM call processes the output of the previous one. \\nAs noted in the Anthropic blog on Building Effective Agents: \\n\\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.\\nWhen to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/a0281fca-3a71-46de-beee-791468607b75/r\\nResources:\\nLangChain Academy\\nSee our lesson on Prompt Chaining here.\\n\\n\\nfrom langgraph.func import entrypoint, task\\n\\n\\n# Tasks\\n@task\\ndef generate_joke(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\\n    return msg.content\\n\\n\\ndef check_punchline(joke: str):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in joke or \"!\" in joke:\\n        return \"Fail\"\\n\\n    return \"Pass\"\\n\\n\\n@task\\ndef improve_joke(joke: str):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\\n    return msg.content\\n\\n\\n@task\\ndef polish_joke(joke: str):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\\n    return msg.content\\n\\n\\n@entrypoint()\\ndef prompt_chaining_workflow(topic: str):\\n    original_joke = generate_joke(topic).result()\\n    if check_punchline(original_joke) == \"Pass\":\\n        return original_joke\\n\\n    improved_joke = improve_joke(original_joke).result()\\n    return polish_joke(improved_joke).result()\\n\\n# Invoke\\nfor step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/332fa4fc-b6ca-416e-baa3-161625e69163/r\\n\\n\\n\\nParallelization¶\\nWith parallelization, LLMs work simultaneously on a task:\\n\\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs.\\nWhen to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/3be2e53c-ca94-40dd-934f-82ff87fac277/r\\nResources:\\nDocumentation\\nSee our documentation on parallelization here.\\nLangChain Academy\\nSee our lesson on parallelization here.\\n\\n\\n@task\\ndef call_llm_1(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_2(topic: str):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n    msg = llm.invoke(f\"Write a story about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_3(topic):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n    msg = llm.invoke(f\"Write a poem about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef aggregator(topic, joke, story, poem):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {topic}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{story}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{joke}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{poem}\"\\n    return combined\\n\\n\\n# Build workflow\\n@entrypoint()\\ndef parallel_workflow(topic: str):\\n    joke_fut = call_llm_1(topic)\\n    story_fut = call_llm_2(topic)\\n    poem_fut = call_llm_3(topic)\\n    return aggregator(\\n        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\\n    ).result()\\n\\n# Invoke\\nfor step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/623d033f-e814-41e9-80b1-75e6abb67801/r\\n\\n\\n\\nRouting¶\\nRouting classifies an input and directs it to a followup task. As noted in the Anthropic blog on Building Effective Agents: \\n\\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\\nWhen to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import Literal\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/c4580b74-fe91-47e4-96fe-7fac598d509c/r\\nResources:\\nLangChain Academy\\nSee our lesson on routing here.\\nExamples\\nHere is RAG workflow that routes questions. See our video here.\\n\\n\\nfrom typing_extensions import Literal\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n@task\\ndef llm_call_1(input_: str):\\n    \"\"\"Write a story\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_2(input_: str):\\n    \"\"\"Write a joke\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_3(input_: str):\\n    \"\"\"Write a poem\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\ndef llm_call_router(input_: str):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=input_),\\n        ]\\n    )\\n    return decision.step\\n\\n\\n# Create workflow\\n@entrypoint()\\ndef router_workflow(input_: str):\\n    next_step = llm_call_router(input_)\\n    if next_step == \"story\":\\n        llm_call = llm_call_1\\n    elif next_step == \"joke\":\\n        llm_call = llm_call_2\\n    elif next_step == \"poem\":\\n        llm_call = llm_call_3\\n\\n    return llm_call(input_).result()\\n\\n# Invoke\\nfor step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/5e2eb979-82dd-402c-b1a0-a8cceaf2a28a/r\\n\\n\\n\\nOrchestrator-Worker¶\\nWith orchestrator-worker, an orchestrator breaks down a task and delegates each sub-task to workers. As noted in the Anthropic blog on Building Effective Agents: \\n\\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\\nWhen to use this workflow: This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren\\'t pre-defined, but determined by the orchestrator based on the specific input.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\nCreating Workers in LangGraph\\nBecause orchestrator-worker workflows are common, LangGraph has the Send API to support this. It lets you dynamically create worker nodes and send each one a specific input. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. As you can see below, we iterate over a list of sections and Send each to a worker node. See further documentation here and here.\\nfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/78cbcfc3-38bf-471d-b62a-b299b144237d/r\\nResources:\\nLangChain Academy\\nSee our lesson on orchestrator-worker here.\\nExamples\\nHere is a project that uses orchestrator-worker for report planning and writing. See our video here.\\n\\n\\nfrom typing import List\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\n@task\\ndef orchestrator(topic: str):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {topic}\"),\\n        ]\\n    )\\n\\n    return report_sections.sections\\n\\n\\n@task\\ndef llm_call(section: Section):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    result = llm.invoke(\\n        [\\n            SystemMessage(content=\"Write a report section.\"),\\n            HumanMessage(\\n                content=f\"Here is the section name: {section.name} and description: {section.description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return result.content\\n\\n\\n@task\\ndef synthesizer(completed_sections: list[str]):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n    final_report = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n    return final_report\\n\\n\\n@entrypoint()\\ndef orchestrator_worker(topic: str):\\n    sections = orchestrator(topic).result()\\n    section_futures = [llm_call(section) for section in sections]\\n    final_report = synthesizer(\\n        [section_fut.result() for section_fut in section_futures]\\n    ).result()\\n    return final_report\\n\\n# Invoke\\nreport = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\\nfrom IPython.display import Markdown\\nMarkdown(report)\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/75a636d0-6179-4a12-9836-e0aa571e87c5/r\\n\\n\\n\\nEvaluator-optimizer¶\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop:\\n\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\\nWhen to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/86ab3e60-2000-4bff-b988-9b89a3269789/r\\nResources:\\nExamples\\nHere is an assistant that uses evaluator-optimizer to improve a report. See our video here.\\nHere is a RAG workflow that grades answers for hallucinations or errors. See our video here.\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\n@task\\ndef llm_call_generator(topic: str, feedback: Feedback):\\n    \"\"\"LLM generates a joke\"\"\"\\n    if feedback:\\n        msg = llm.invoke(\\n            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef llm_call_evaluator(joke: str):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\\n    return feedback\\n\\n\\n@entrypoint()\\ndef optimizer_workflow(topic: str):\\n    feedback = None\\n    while True:\\n        joke = llm_call_generator(topic, feedback).result()\\n        feedback = llm_call_evaluator(joke).result()\\n        if feedback.grade == \"funny\":\\n            break\\n\\n    return joke\\n\\n# Invoke\\nfor step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/f66830be-4339-4a6b-8a93-389ce5ae27b4/r\\n\\n\\n\\nAgent¶\\nAgents are typically implemented as an LLM performing actions (via tool-calling) based on environmental feedback in a loop. As noted in the Anthropic blog on Building Effective Agents:\\n\\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully.\\nWhen to use agents: Agents can be used for open-ended problems where it’s difficult or impossible to predict the required number of steps, and where you can’t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents\\' autonomy makes them ideal for scaling tasks in trusted environments.\\n\\n\\nAPI Reference: tool\\nfrom langchain_core.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional API\\n\\n\\nfrom langgraph.graph import MessagesState\\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"environment\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"Action\"\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"environment\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    {\\n        # Name returned by should_continue : Name of next node to visit\\n        \"Action\": \"environment\",\\n        END: END,\\n    },\\n)\\nagent_builder.add_edge(\"environment\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/051f0391-6761-4f8c-a53b-22231b016690/r\\nResources:\\nLangChain Academy\\nSee our lesson on agents here.\\nExamples\\nHere is a project that uses a tool calling agent to create / store long-term memories.\\n\\n\\nfrom langgraph.graph import add_messages\\nfrom langchain_core.messages import (\\n    SystemMessage,\\n    HumanMessage,\\n    BaseMessage,\\n    ToolCall,\\n)\\n\\n\\n@task\\ndef call_llm(messages: list[BaseMessage]):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n    return llm_with_tools.invoke(\\n        [\\n            SystemMessage(\\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n            )\\n        ]\\n        + messages\\n    )\\n\\n\\n@task\\ndef call_tool(tool_call: ToolCall):\\n    \"\"\"Performs the tool call\"\"\"\\n    tool = tools_by_name[tool_call[\"name\"]]\\n    return tool.invoke(tool_call)\\n\\n\\n@entrypoint()\\ndef agent(messages: list[BaseMessage]):\\n    llm_response = call_llm(messages).result()\\n\\n    while True:\\n        if not llm_response.tool_calls:\\n            break\\n\\n        # Execute tools\\n        tool_result_futures = [\\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\\n        ]\\n        tool_results = [fut.result() for fut in tool_result_futures]\\n        messages = add_messages(messages, [llm_response, *tool_results])\\n        llm_response = call_llm(messages).result()\\n\\n    messages = add_messages(messages, llm_response)\\n    return messages\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\\n    print(chunk)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/42ae8bf9-3935-4504-a081-8ddbcbfc8b2e/r\\n\\n\\n\\nPre-built¶\\nLangGraph also provides a pre-built method for creating an agent as defined above (using the create_react_agent function):\\nhttps://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\\nAPI Reference: create_react_agent\\nfrom langgraph.prebuilt import create_react_agent\\n\\n# Pass in:\\n# (1) the augmented LLM with tools\\n# (2) the tools list (which is used to create the tool node)\\npre_built_agent = create_react_agent(llm, tools=tools)\\n\\n# Show the agent\\ndisplay(Image(pre_built_agent.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = pre_built_agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/graph-api/#map-reduce-and-the-send-api', 'title': 'Use the Graph API', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse the Graph API\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Use the Graph API\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Guides\\n          \\n\\n\\n\\n\\n\\n    Agent development\\n    \\n  \\n\\n\\n\\n\\n\\n            Agent development\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n    Run an agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph APIs\\n    \\n  \\n\\n\\n\\n\\n\\n            LangGraph APIs\\n          \\n\\n\\n\\n\\n\\n    Graph API\\n    \\n  \\n\\n\\n\\n\\n\\n            Graph API\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Use the Graph API\\n    \\n  \\n\\n\\n\\n\\n    Use the Graph API\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Setup\\n    \\n\\n\\n\\n\\n\\n      Define and update state\\n    \\n\\n\\n\\n\\n\\n\\n      Define state\\n    \\n\\n\\n\\n\\n\\n      Update state\\n    \\n\\n\\n\\n\\n\\n      Process state updates with reducers\\n    \\n\\n\\n\\n\\n\\n\\n      MessagesState\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Define input and output schemas\\n    \\n\\n\\n\\n\\n\\n      Pass private state between nodes\\n    \\n\\n\\n\\n\\n\\n      Use Pydantic models for graph state\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Add runtime configuration\\n    \\n\\n\\n\\n\\n\\n      Add retry policies\\n    \\n\\n\\n\\n\\n\\n      Add node caching\\n    \\n\\n\\n\\n\\n\\n      Create a sequence of steps\\n    \\n\\n\\n\\n\\n\\n      Create branches\\n    \\n\\n\\n\\n\\n\\n\\n      Run graph nodes in parallel\\n    \\n\\n\\n\\n\\n\\n      Defer node execution\\n    \\n\\n\\n\\n\\n\\n      Conditional branching\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Map-Reduce and the Send API\\n    \\n\\n\\n\\n\\n\\n      Create and control loops\\n    \\n\\n\\n\\n\\n\\n\\n      Impose a recursion limit\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Async\\n    \\n\\n\\n\\n\\n\\n      Combine control flow and state updates with Command\\n    \\n\\n\\n\\n\\n\\n\\n      Navigate to a node in a parent graph\\n    \\n\\n\\n\\n\\n\\n      Use inside tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Visualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Functional API\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Runtime\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Core capabilities\\n    \\n  \\n\\n\\n\\n\\n\\n            Core capabilities\\n          \\n\\n\\n\\n\\n    Streaming\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Persistence\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Durable execution\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Memory\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Context\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Models\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tools\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Subgraphs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Multi-agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    MCP\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tracing\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Platform-only capabilities\\n    \\n  \\n\\n\\n\\n\\n\\n            Platform-only capabilities\\n          \\n\\n\\n\\n\\n    LangGraph Platform\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Authentication & access control\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Assistants\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Double-texting\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Webhooks\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Cron jobs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Server customization\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Data management\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Deployment\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Setup\\n    \\n\\n\\n\\n\\n\\n      Define and update state\\n    \\n\\n\\n\\n\\n\\n\\n      Define state\\n    \\n\\n\\n\\n\\n\\n      Update state\\n    \\n\\n\\n\\n\\n\\n      Process state updates with reducers\\n    \\n\\n\\n\\n\\n\\n\\n      MessagesState\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Define input and output schemas\\n    \\n\\n\\n\\n\\n\\n      Pass private state between nodes\\n    \\n\\n\\n\\n\\n\\n      Use Pydantic models for graph state\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Add runtime configuration\\n    \\n\\n\\n\\n\\n\\n      Add retry policies\\n    \\n\\n\\n\\n\\n\\n      Add node caching\\n    \\n\\n\\n\\n\\n\\n      Create a sequence of steps\\n    \\n\\n\\n\\n\\n\\n      Create branches\\n    \\n\\n\\n\\n\\n\\n\\n      Run graph nodes in parallel\\n    \\n\\n\\n\\n\\n\\n      Defer node execution\\n    \\n\\n\\n\\n\\n\\n      Conditional branching\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Map-Reduce and the Send API\\n    \\n\\n\\n\\n\\n\\n      Create and control loops\\n    \\n\\n\\n\\n\\n\\n\\n      Impose a recursion limit\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Async\\n    \\n\\n\\n\\n\\n\\n      Combine control flow and state updates with Command\\n    \\n\\n\\n\\n\\n\\n\\n      Navigate to a node in a parent graph\\n    \\n\\n\\n\\n\\n\\n      Use inside tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Visualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow to use the graph API¶\\nThis guide demonstrates the basics of LangGraph\\'s Graph API. It walks through state, as well as composing common graph structures such as sequences, branches, and loops. It also covers LangGraph\\'s control features, including the Send API for map-reduce workflows and the Command API for combining state updates with \"hops\" across nodes.\\nSetup¶\\nInstall langgraph:\\npip install -U langgraph\\n\\n\\nSet up LangSmith for better debugging\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the docs.\\n\\nDefine and update state¶\\nHere we show how to define and update state in LangGraph. We will demonstrate:\\n\\nHow to use state to define a graph\\'s schema\\nHow to use reducers to control how state updates are processed.\\n\\nDefine state¶\\nState in LangGraph can be a TypedDict, Pydantic model, or dataclass. Below we will use TypedDict. See this section for detail on using Pydantic.\\nBy default, graphs will have the same input and output schema, and the state determines that schema. See this section for how to define distinct input and output schemas.\\nLet\\'s consider a simple example using messages. This represents a versatile formulation of state for many LLM applications. See our concepts page for more detail.\\nAPI Reference: AnyMessage\\nfrom langchain_core.messages import AnyMessage\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    messages: list[AnyMessage]\\n    extra_field: int\\n\\nThis state tracks a list of message objects, as well as an extra integer field.\\nUpdate state¶\\nLet\\'s build an example graph with a single node. Our node is just a Python function that reads our graph\\'s state and makes updates to it. The first argument to this function will always be the state:\\nAPI Reference: AIMessage\\nfrom langchain_core.messages import AIMessage\\n\\ndef node(state: State):\\n    messages = state[\"messages\"]\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": messages + [new_message], \"extra_field\": 10}\\n\\nThis node simply appends a message to our message list, and populates an extra field.\\n\\nImportant\\nNodes should return updates to the state directly, instead of mutating the state.\\n\\nLet\\'s next define a simple graph containing this node. We use StateGraph to define a graph that operates on this state. We then use add_node populate our graph.\\nAPI Reference: StateGraph\\nfrom langgraph.graph import StateGraph\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(node)\\nbuilder.set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our graph. See this section for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIn this case, our graph just executes a single node. Let\\'s proceed with a simple invocation:\\nAPI Reference: HumanMessage\\nfrom langchain_core.messages import HumanMessage\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\nresult\\n\\n{\\'messages\\': [HumanMessage(content=\\'Hi\\'), AIMessage(content=\\'Hello!\\')], \\'extra_field\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by updating a single key of the state.\\nWe receive the entire state in the invocation result.\\n\\nFor convenience, we frequently inspect the content of message objects via pretty-print:\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nProcess state updates with reducers¶\\nEach key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.\\nFor TypedDict state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.\\nIn the earlier example, our node updated the \"messages\" key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:\\nfrom typing_extensions import Annotated\\n\\ndef add(left, right):\\n    \"\"\"Can also import `add` from the `operator` built-in.\"\"\"\\n    return left + right\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add]\\n    extra_field: int\\n\\nNow our node can be simplified:\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\nAPI Reference: START\\nfrom langgraph.graph import START\\n\\ngraph = StateGraph(State).add_node(node).add_edge(START, \"node\").compile()\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nMessagesState¶\\nIn practice, there are additional considerations for updating lists of messages:\\n\\nWe may wish to update an existing message in the state.\\nWe may want to accept short-hands for message formats, such as OpenAI format.\\n\\nLangGraph includes a built-in reducer add_messages that handles these considerations:\\nAPI Reference: add_messages\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n    extra_field: int\\n\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\ngraph = StateGraph(State).add_node(node).set_entry_point(\"node\").compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hi\"}\\n\\nresult = graph.invoke({\"messages\": [input_message]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nThis is a versatile representation of state for applications involving chat models. LangGraph includes a pre-built MessagesState for convenience, so that we can have:\\nfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    extra_field: int\\n\\nDefine input and output schemas¶\\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it\\'s also possible to define distinct input and output schemas for a graph.\\nWhen distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.\\nBelow, we\\'ll see how to define distinct input and output schema.\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# Define the schema for the input\\nclass InputState(TypedDict):\\n    question: str\\n\\n# Define the schema for the output\\nclass OutputState(TypedDict):\\n    answer: str\\n\\n# Define the overall schema, combining both input and output\\nclass OverallState(InputState, OutputState):\\n    pass\\n\\n# Define the node that processes the input and generates an answer\\ndef answer_node(state: InputState):\\n    # Example answer and an extra key\\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\\n\\n# Build the graph with input and output schemas specified\\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\\nbuilder.add_node(answer_node)  # Add the answer node\\nbuilder.add_edge(START, \"answer_node\")  # Define the starting edge\\nbuilder.add_edge(\"answer_node\", END)  # Define the ending edge\\ngraph = builder.compile()  # Compile the graph\\n\\n# Invoke the graph with an input and print the result\\nprint(graph.invoke({\"question\": \"hi\"}))\\n\\n{\\'answer\\': \\'bye\\'}\\n\\nNotice that the output of invoke only includes the output schema.\\nPass private state between nodes¶\\nIn some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn\\'t need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.\\nBelow, we\\'ll create an example sequential graph consisting of three nodes (node_1, node_2 and node_3), where private data is passed between the first two steps (node_1 and node_2), while the third step (node_3) only has access to the public overall state.\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(TypedDict):\\n    a: str\\n\\n# Output from node_1 contains private data that is not part of the overall state\\nclass Node1Output(TypedDict):\\n    private_data: str\\n\\n# The private data is only shared between node_1 and node_2\\ndef node_1(state: OverallState) -> Node1Output:\\n    output = {\"private_data\": \"set by node_1\"}\\n    print(f\"Entered node `node_1`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 2 input only requests the private data available after node_1\\nclass Node2Input(TypedDict):\\n    private_data: str\\n\\ndef node_2(state: Node2Input) -> OverallState:\\n    output = {\"a\": \"set by node_2\"}\\n    print(f\"Entered node `node_2`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 3 only has access to the overall state (no access to private data from node_1)\\ndef node_3(state: OverallState) -> OverallState:\\n    output = {\"a\": \"set by node_3\"}\\n    print(f\"Entered node `node_3`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Connect nodes in a sequence\\n# node_2 accepts private data from node_1, whereas\\n# node_3 does not see the private data.\\nbuilder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])\\nbuilder.add_edge(START, \"node_1\")\\ngraph = builder.compile()\\n\\n# Invoke the graph with the initial state\\nresponse = graph.invoke(\\n    {\\n        \"a\": \"set at start\",\\n    }\\n)\\n\\nprint()\\nprint(f\"Output of graph invocation: {response}\")\\n\\nEntered node `node_1`:\\n    Input: {\\'a\\': \\'set at start\\'}.\\n    Returned: {\\'private_data\\': \\'set by node_1\\'}\\nEntered node `node_2`:\\n    Input: {\\'private_data\\': \\'set by node_1\\'}.\\n    Returned: {\\'a\\': \\'set by node_2\\'}\\nEntered node `node_3`:\\n    Input: {\\'a\\': \\'set by node_2\\'}.\\n    Returned: {\\'a\\': \\'set by node_3\\'}\\n\\nOutput of graph invocation: {\\'a\\': \\'set by node_3\\'}\\n\\nUse Pydantic models for graph state¶\\nA StateGraph accepts a state_schema argument on initialization that specifies the \"shape\" of the state that the nodes in the graph can access and update.\\nIn our examples, we typically use a python-native TypedDict or dataclass for state_schema, but state_schema can be any type.\\nHere, we\\'ll see how a Pydantic BaseModel can be used for state_schema to add run-time validation on inputs.\\n\\nKnown Limitations\\n\\nCurrently, the output of the graph will NOT be an instance of a pydantic model.\\nRun-time validation only occurs on inputs into nodes, not on the outputs.\\nThe validation error trace from pydantic does not show which node the error arises in.\\nPydantic\\'s recursive validation can be slow. For performance-sensitive applications, you may want to consider using a dataclass instead.\\n\\n\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\nfrom pydantic import BaseModel\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(BaseModel):\\n    a: str\\n\\ndef node(state: OverallState):\\n    return {\"a\": \"goodbye\"}\\n\\n# Build the state graph\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(node)  # node_1 is the first node\\nbuilder.add_edge(START, \"node\")  # Start the graph with node_1\\nbuilder.add_edge(\"node\", END)  # End the graph after node_1\\ngraph = builder.compile()\\n\\n# Test the graph with a valid input\\ngraph.invoke({\"a\": \"hello\"})\\n\\nInvoke the graph with an invalid input\\ntry:\\n    graph.invoke({\"a\": 123})  # Should be a string\\nexcept Exception as e:\\n    print(\"An exception was raised because `a` is an integer rather than a string.\")\\n    print(e)\\n\\nAn exception was raised because `a` is an integer rather than a string.\\n1 validation error for OverallState\\na\\n  Input should be a valid string [type=string_type, input_value=123, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type\\n\\nSee below for additional features of Pydantic model state:\\n\\nSerialization Behavior\\nWhen using Pydantic models as state schemas, it\\'s important to understand how serialization works, especially when:\\n- Passing Pydantic objects as inputs\\n- Receiving outputs from the graph\\n- Working with nested Pydantic models\\nLet\\'s see these behaviors in action.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass NestedModel(BaseModel):\\n    value: str\\n\\nclass ComplexState(BaseModel):\\n    text: str\\n    count: int\\n    nested: NestedModel\\n\\ndef process_node(state: ComplexState):\\n    # Node receives a validated Pydantic object\\n    print(f\"Input state type: {type(state)}\")\\n    print(f\"Nested type: {type(state.nested)}\")\\n    # Return a dictionary update\\n    return {\"text\": state.text + \" processed\", \"count\": state.count + 1}\\n\\n# Build the graph\\nbuilder = StateGraph(ComplexState)\\nbuilder.add_node(\"process\", process_node)\\nbuilder.add_edge(START, \"process\")\\nbuilder.add_edge(\"process\", END)\\ngraph = builder.compile()\\n\\n# Create a Pydantic instance for input\\ninput_state = ComplexState(text=\"hello\", count=0, nested=NestedModel(value=\"test\"))\\nprint(f\"Input object type: {type(input_state)}\")\\n\\n# Invoke graph with a Pydantic instance\\nresult = graph.invoke(input_state)\\nprint(f\"Output type: {type(result)}\")\\nprint(f\"Output content: {result}\")\\n\\n# Convert back to Pydantic model if needed\\noutput_model = ComplexState(**result)\\nprint(f\"Converted back to Pydantic: {type(output_model)}\")\\n\\n\\n\\nRuntime Type Coercion\\nPydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you\\'re not aware of it.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass CoercionExample(BaseModel):\\n    # Pydantic will coerce string numbers to integers\\n    number: int\\n    # Pydantic will parse string booleans to bool\\n    flag: bool\\n\\ndef inspect_node(state: CoercionExample):\\n    print(f\"number: {state.number} (type: {type(state.number)})\")\\n    print(f\"flag: {state.flag} (type: {type(state.flag)})\")\\n    return {}\\n\\nbuilder = StateGraph(CoercionExample)\\nbuilder.add_node(\"inspect\", inspect_node)\\nbuilder.add_edge(START, \"inspect\")\\nbuilder.add_edge(\"inspect\", END)\\ngraph = builder.compile()\\n\\n# Demonstrate coercion with string inputs that will be converted\\nresult = graph.invoke({\"number\": \"42\", \"flag\": \"true\"})\\n\\n# This would fail with a validation error\\ntry:\\n    graph.invoke({\"number\": \"not-a-number\", \"flag\": \"true\"})\\nexcept Exception as e:\\n    print(f\"\\\\nExpected validation error: {e}\")\\n\\n\\n\\nWorking with Message Models\\nWhen working with LangChain message types in your state schema, there are important considerations for serialization. You should use AnyMessage (rather than BaseMessage) for proper serialization/deserialization when using message objects over the wire.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, AIMessage, AnyMessage\\nfrom typing import List\\n\\nclass ChatState(BaseModel):\\n    messages: List[AnyMessage]\\n    context: str\\n\\ndef add_message(state: ChatState):\\n    return {\"messages\": state.messages + [AIMessage(content=\"Hello there!\")]}\\n\\nbuilder = StateGraph(ChatState)\\nbuilder.add_node(\"add_message\", add_message)\\nbuilder.add_edge(START, \"add_message\")\\nbuilder.add_edge(\"add_message\", END)\\ngraph = builder.compile()\\n\\n# Create input with a message\\ninitial_state = ChatState(\\n    messages=[HumanMessage(content=\"Hi\")], context=\"Customer support chat\"\\n)\\n\\nresult = graph.invoke(initial_state)\\nprint(f\"Output: {result}\")\\n\\n# Convert back to Pydantic model to see message types\\noutput_model = ChatState(**result)\\nfor i, msg in enumerate(output_model.messages):\\n    print(f\"Message {i}: {type(msg).__name__} - {msg.content}\")\\n\\n\\nAdd runtime configuration¶\\nSometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, without polluting the graph state with these parameters.\\nTo add runtime configuration:\\n\\nSpecify a schema for your configuration\\nAdd the configuration to the function signature for nodes or conditional edges\\nPass the configuration into the graph.\\n\\nSee below for a simple example:\\nAPI Reference: END | StateGraph | START\\nfrom langgraph.graph import END, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n# 1. Specify config schema\\nclass ContextSchema(TypedDict):\\n    my_runtime_value: str\\n\\n# 2. Define a graph that accesses the config in a node\\nclass State(TypedDict):\\n    my_state_value: str\\n\\ndef node(state: State, runtime: Runtime[ContextSchema]):\\n    if runtime.context[\"my_runtime_value\"] == \"a\":\\n        return {\"my_state_value\": 1}\\n    elif runtime.context[\"my_runtime_value\"] == \"b\":\\n        return {\"my_state_value\": 2}\\n    else:\\n        raise ValueError(\"Unknown values.\")\\n\\nbuilder = StateGraph(State, context_schema=ContextSchema)\\nbuilder.add_node(node)\\nbuilder.add_edge(START, \"node\")\\nbuilder.add_edge(\"node\", END)\\n\\ngraph = builder.compile()\\n\\n# 3. Pass in configuration at runtime:\\nprint(graph.invoke({}, context={\"my_runtime_value\": \"a\"}))\\nprint(graph.invoke({}, context={\"my_runtime_value\": \"b\"}))\\n\\n{\\'my_state_value\\': 1}\\n{\\'my_state_value\\': 2}\\n\\n\\nExtended example: specifying LLM at runtime\\nBelow we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.\\nfrom dataclasses import dataclass\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, END, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n@dataclass\\nclass ContextSchema:\\n    model_provider: str = \"anthropic\"\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\\n    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\\n    model = MODELS[runtime.context.model_provider]\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, context_schema=ContextSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\n# With no configuration, uses default (Anthropic)\\nresponse_1 = graph.invoke({\"messages\": [input_message]})[\"messages\"][-1]\\n# Or, can set OpenAI\\nresponse_2 = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\"})[\"messages\"][-1]\\n\\nprint(response_1.response_metadata[\"model_name\"])\\nprint(response_2.response_metadata[\"model_name\"])\\n\\nclaude-3-5-haiku-20241022\\ngpt-4.1-mini-2025-04-14\\n\\n\\n\\nExtended example: specifying model and system message at runtime\\nBelow we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.\\nfrom dataclasses import dataclass\\nfrom typing import Optional\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.messages import SystemMessage\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n@dataclass\\nclass ContextSchema:\\n    model_provider: str = \"anthropic\"\\n    system_message: str | None = None\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\\n    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\\n    model = MODELS[runtime.context.model_provider]\\n    messages = state[\"messages\"]\\n    if (system_message := runtime.context.system_message):\\n        messages = [SystemMessage(system_message)] + messages\\n    response = model.invoke(messages)\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, context_schema=ContextSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\nresponse = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\", \"system_message\": \"Respond in Italian.\"})\\nfor message in response[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nhi\\n================================== Ai Message ==================================\\n\\nCiao! Come posso aiutarti oggi?\\n\\n\\nAdd retry policies¶\\nThere are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes.\\nTo configure a retry policy, pass the retry_policy parameter to the add_node. The retry_policy parameter takes in a RetryPolicy named tuple object. Below we instantiate a RetryPolicy object with the default parameters and associate it with a node:\\nfrom langgraph.pregel import RetryPolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    retry_policy=RetryPolicy(),\\n)\\n\\nBy default, the retry_on parameter uses the default_retry_on function, which retries on any exception except for the following:\\n\\nValueError\\nTypeError\\nArithmeticError\\nImportError\\nLookupError\\nNameError\\nSyntaxError\\nRuntimeError\\nReferenceError\\nStopIteration\\nStopAsyncIteration\\nOSError\\n\\nIn addition, for exceptions from popular http request libraries such as requests and httpx it only retries on 5xx status codes.\\n\\nExtended example: customizing retry policies\\nConsider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes:\\nimport sqlite3\\nfrom typing_extensions import TypedDict\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.pregel import RetryPolicy\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_core.messages import AIMessage\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///:memory:\")\\nmodel = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\\n\\ndef query_database(state: MessagesState):\\n    query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\\n    return {\"messages\": [AIMessage(content=query_result)]}\\n\\ndef call_model(state: MessagesState):\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\n# Define a new graph\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(\\n    \"query_database\",\\n    query_database,\\n    retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),\\n)\\nbuilder.add_node(\"model\", call_model, retry_policy=RetryPolicy(max_attempts=5))\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", \"query_database\")\\nbuilder.add_edge(\"query_database\", END)\\ngraph = builder.compile()\\n\\n\\nAdd node caching¶\\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph.\\nTo configure a cache policy, pass the cache_policy parameter to the add_node function. In the following example, a CachePolicy object is instantiated with a time to live of 120 seconds and the default key_func generator. Then it is associated with a node:\\nfrom langgraph.types import CachePolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    cache_policy=CachePolicy(ttl=120),\\n)\\n\\nThen, to enable node-level caching for a graph, set the cache argument when compiling the graph. The example below uses InMemoryCache to set up a graph with in-memory cache, but SqliteCache is also available.\\nfrom langgraph.cache.memory import InMemoryCache\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nCreate a sequence of steps¶\\n\\nPrerequisites\\nThis guide assumes familiarity with the above section on state.\\n\\nHere we demonstrate how to construct a simple sequence of steps. We will show:\\n\\nHow to build a sequential graph\\nBuilt-in short-hand for constructing similar graphs.\\n\\nTo add a sequence of nodes, we use the .add_node and .add_edge methods of our graph:\\nAPI Reference: START | StateGraph\\nfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\nWe can also use the built-in shorthand .add_sequence:\\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\n\\nWhy split application steps into a sequence with LangGraph?\\nLangGraph makes it easy to add an underlying persistence layer to your application.\\nThis allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:\\n\\nHow state updates are checkpointed\\nHow interruptions are resumed in human-in-the-loop workflows\\nHow we can \"rewind\" and branch-off executions using LangGraph\\'s time travel features\\n\\nThey also determine how execution steps are streamed, and how your application is visualized\\nand debugged using LangGraph Studio.\\n\\nLet\\'s demonstrate an end-to-end example. We will create a sequence of three steps:\\n\\nPopulate a value in a key of the state\\nUpdate the same value\\nPopulate a different value\\n\\nLet\\'s first define our state. This governs the schema of the graph, and can also specify how to apply updates. See this section for more detail.\\nIn our case, we will just keep track of two values:\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    value_1: str\\n    value_2: int\\n\\nOur nodes are just Python functions that read our graph\\'s state and make updates to it. The first argument to this function will always be the state:\\ndef step_1(state: State):\\n    return {\"value_1\": \"a\"}\\n\\ndef step_2(state: State):\\n    current_value_1 = state[\"value_1\"]\\n    return {\"value_1\": f\"{current_value_1} b\"}\\n\\ndef step_3(state: State):\\n    return {\"value_2\": 10}\\n\\n\\nNote\\nNote that when issuing updates to the state, each node can just specify the value of the key it wishes to update.\\nBy default, this will overwrite the value of the corresponding key. You can also use reducers to control how updates are processed— for example, you can append successive updates to a key instead. See this section for more detail.\\n\\nFinally, we define the graph. We use StateGraph to define a graph that operates on this state.\\nWe will then use add_node and add_edge to populate our graph and define its control flow.\\nAPI Reference: START | StateGraph\\nfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\n\\nSpecifying custom names\\nYou can specify custom names for nodes using .add_node:\\nbuilder.add_node(\"my_node\", step_1)\\n\\n\\nNote that:\\n\\n.add_edge takes the names of nodes, which for functions defaults to node.__name__.\\nWe must specify the entry point of the graph. For this we add an edge with the START node.\\nThe graph halts when there are no more nodes to execute.\\n\\nWe next compile our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a checkpointer, it would also be passed in here.\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our sequence. See this guide for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nLet\\'s proceed with a simple invocation:\\ngraph.invoke({\"value_1\": \"c\"})\\n\\n{\\'value_1\\': \\'a b\\', \\'value_2\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.\\nThe value we passed in was overwritten by the first node.\\nThe second node updated the value.\\nThe third node populated a different value.\\n\\n\\nBuilt-in shorthand\\nlanggraph>=0.2.46 includes a built-in short-hand add_sequence for adding node sequences. You can compile the same graph as follows:\\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"value_1\": \"c\"})    \\n\\n\\nCreate branches¶\\nParallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.\\nRun graph nodes in parallel¶\\nIn this example, we fan out from Node A to B and C and then fan in to D. With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on state reducers for more detail on updating state with reducers.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nWith the reducer, you can see that the values added in each node are accumulated.\\ngraph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\n\\nAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\']\\n\\n\\nNote\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. Because they are in the same step, node \"d\" executes after both \"b\" and \"c\" are finished.\\nImportantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.\\n\\n\\nException handling?\\nLangGraph executes nodes within supersteps, meaning that while parallel branches are executed in parallel, the entire superstep is transactional. If any of these branches raises an exception, none of the updates are applied to the state (the entire superstep errors).\\nImportantly, when using a checkpointer, results from successful nodes within a superstep are saved, and don\\'t repeat when resumed.\\nIf you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this:\\n\\nYou can write regular python code within your node to catch and handle exceptions.\\nYou can set a retry_policy to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn\\'t worry about performing redundant work.\\n\\nTogether, these let you perform parallel execution and fully control exception handling.\\n\\nDefer node execution¶\\nDeferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows.\\nThe above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let\\'s add a node \"b_2\" in the \"b\" branch:\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef b_2(state: State):\\n    print(f\\'Adding \"B_2\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B_2\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(b_2)\\nbuilder.add_node(c)\\nbuilder.add_node(d, defer=True)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"b_2\")\\nbuilder.add_edge(\"b_2\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\ngraph.invoke({\"aggregate\": []})\\n\\nAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"B_2\" to [\\'A\\', \\'B\\', \\'C\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\', \\'B_2\\']\\n\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. We set defer=True on node d so it will not execute until all pending tasks are finished. In this case, this means that \"d\" waits to execute until the entire \"b\" branch is finished.\\nConditional branching¶\\nIf your fan-out should vary at runtime based on the state, you can use add_conditional_edges to select one or more paths using the graph state. See example below, where node a generates a state update that determines the following node.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Literal, Sequence\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    # Add a key to the state. We will set this key to determine\\n    # how we branch.\\n    which: str\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"], \"which\": \"c\"}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"b\", END)\\nbuilder.add_edge(\"c\", END)\\n\\ndef conditional_edge(state: State) -> Literal[\"b\", \"c\"]:\\n    # Fill in arbitrary logic here that uses the state\\n    # to determine the next node\\n    return state[\"which\"]\\n\\nbuilder.add_conditional_edges(\"a\", conditional_edge)\\n\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nresult = graph.invoke({\"aggregate\": []})\\nprint(result)\\n\\nAdding \"A\" to []\\nAdding \"C\" to [\\'A\\']\\n{\\'aggregate\\': [\\'A\\', \\'C\\'], \\'which\\': \\'c\\'}\\n\\n\\nTip\\nYour conditional edges can route to multiple destination nodes. For example:\\ndef route_bc_or_cd(state: State) -> Sequence[str]:\\n    if state[\"which\"] == \"cd\":\\n        return [\"c\", \"d\"]\\n    return [\"b\", \"c\"]\\n\\n\\nMap-Reduce and the Send API¶\\nLangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:\\nAPI Reference: StateGraph | START | END | Send\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Send\\nfrom typing_extensions import TypedDict, Annotated\\nimport operator\\n\\nclass OverallState(TypedDict):\\n    topic: str\\n    subjects: list[str]\\n    jokes: Annotated[list[str], operator.add]\\n    best_selected_joke: str\\n\\ndef generate_topics(state: OverallState):\\n    return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]}\\n\\ndef generate_joke(state: OverallState):\\n    joke_map = {\\n        \"lions\": \"Why don\\'t lions like fast food? Because they can\\'t catch it!\",\\n        \"elephants\": \"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\",\\n        \"penguins\": \"Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\"\\n    }\\n    return {\"jokes\": [joke_map[state[\"subject\"]]]}\\n\\ndef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\\n\\ndef best_joke(state: OverallState):\\n    return {\"best_selected_joke\": \"penguins\"}\\n\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(\"generate_topics\", generate_topics)\\nbuilder.add_node(\"generate_joke\", generate_joke)\\nbuilder.add_node(\"best_joke\", best_joke)\\nbuilder.add_edge(START, \"generate_topics\")\\nbuilder.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\\nbuilder.add_edge(\"generate_joke\", \"best_joke\")\\nbuilder.add_edge(\"best_joke\", END)\\nbuilder.add_edge(\"generate_topics\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\n# Call the graph: here we call it to generate a list of jokes\\nfor step in graph.stream({\"topic\": \"animals\"}):\\n    print(step)\\n\\n{\\'generate_topics\\': {\\'subjects\\': [\\'lions\\', \\'elephants\\', \\'penguins\\']}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t lions like fast food? Because they can\\'t catch it!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\\'Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\\']}}\\n{\\'best_joke\\': {\\'best_selected_joke\\': \\'penguins\\'}}\\n\\nCreate and control loops¶\\nWhen creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition.\\nYou can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here.\\nLet\\'s consider a simple graph with a loop to better understand how these mechanisms work.\\n\\nTip\\nTo return the last value of your state instead of receiving a recursion limit error, see the next section.\\n\\nWhen creating a loop, you can include a conditional edge that specifies a termination condition:\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if termination_condition(state):\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nTo control the recursion limit, specify \"recursion_limit\" in the config. This will raise a GraphRecursionError, which you can catch and handle:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke(inputs, {\"recursion_limit\": 3})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nLet\\'s define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis architecture is similar to a ReAct agent in which node \"a\" is a tool-calling model, and node \"b\" represents the tools.\\nIn our route conditional edge, we specify that we should end after the \"aggregate\" list in the state passes a threshold length.\\nInvoking the graph, we see that we alternate between nodes \"a\" and \"b\" before terminating once we reach the termination condition.\\ngraph.invoke({\"aggregate\": []})\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\', \\'B\\']\\n\\nImpose a recursion limit¶\\nIn some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph\\'s recursion limit. This will raise a GraphRecursionError after a given number of supersteps. We can then catch and handle this exception:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\nExtended example: return state on hitting recursion limit\\nInstead of raising GraphRecursionError, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.\\nLangGraph implements a special RemainingSteps annotation. Under the hood, it creates a ManagedValue channel -- a state channel that will exist for the duration of our graph run and no longer.\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.managed.is_last_step import RemainingSteps\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    remaining_steps: RemainingSteps\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if state[\"remaining_steps\"] <= 2:\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\n# Test it out\\nresult = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nprint(result)\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\n{\\'aggregate\\': [\\'A\\', \\'B\\', \\'A\\']}\\n\\n\\n\\nExtended example: loops with branches\\nTo better understand how the recursion limit works, let\\'s consider a more complex example. Below we implement a loop, but one step fans out into two nodes:\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Node C sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Node D sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge([\"c\", \"d\"], \"a\")\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis graph looks complex, but can be conceptualized as loop of supersteps:\\n\\nNode A\\nNode B\\nNodes C and D\\nNode A\\n...\\n\\nWe have a loop of four supersteps, where nodes C and D are executed concurrently.\\nInvoking the graph as before, we see that we complete two full \"laps\" before hitting the termination condition:\\nresult = graph.invoke({\"aggregate\": []})\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nNode B sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\']\\nNode D sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\', \\'C\\', \\'D\\']\\n\\nHowever, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\nAsync¶\\nUsing the async programming paradigm can produce significant performance improvements when running IO-bound code concurrently (e.g., making concurrent API requests to a chat model provider).\\nTo convert a sync implementation of the graph to an async implementation, you will need to:\\n\\nUpdate nodes use async def instead of def.\\nUpdate the code inside to use await appropriately.\\nInvoke the graph with .ainvoke or .astream as desired.\\n\\nBecause many LangChain objects implement the Runnable Protocol which has async variants of all the sync methods it\\'s typically fairly quick to upgrade a sync graph to an async graph.\\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\\nOpenAIAnthropicAzureGoogle GeminiAWS Bedrock\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"openai:gpt-4.1\")\\n\\n👉 Read the OpenAI integration docs\\n\\n\\npip install -U \"langchain[anthropic]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\\n\\n👉 Read the Anthropic integration docs\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nllm = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\n👉 Read the Azure integration docs\\n\\n\\npip install -U \"langchain[google-genai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\\n\\n👉 Read the Google GenAI integration docs\\n\\n\\npip install -U \"langchain[aws]\"\\n\\nfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nllm = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\n👉 Read the AWS Bedrock integration docs\\n\\n\\n\\nAPI Reference: init_chat_model | StateGraph\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, StateGraph\\n\\nasync def node(state: MessagesState): # (1)!\\n    new_message = await llm.ainvoke(state[\"messages\"]) # (2)!\\n    return {\"messages\": [new_message]}\\n\\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\\nresult = await graph.ainvoke({\"messages\": [input_message]}) # (3)!\\n\\n\\nDeclare nodes to be async functions.\\nUse async invocations when available within the node.\\nUse async invocations on the graph object itself.\\n\\n\\nAsync streaming\\nSee the streaming guide for examples of streaming with async.\\n\\nCombine control flow and state updates with Command¶\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWe show an end-to-end example below. Let\\'s create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.\\nAPI Reference: StateGraph | START | Command\\nimport random\\nfrom typing_extensions import TypedDict, Literal\\nfrom langgraph.graph import StateGraph, START\\nfrom langgraph.types import Command\\n\\n# Define graph state\\nclass State(TypedDict):\\n    foo: str\\n\\n# Define the nodes\\n\\ndef node_a(state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\\n    print(\"Called A\")\\n    value = random.choice([\"b\", \"c\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"b\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        # this is the state update\\n        update={\"foo\": value},\\n        # this is a replacement for an edge\\n        goto=goto,\\n    )\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    return {\"foo\": state[\"foo\"] + \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": state[\"foo\"] + \"c\"}\\n\\nWe can now create the StateGraph with the above nodes. Notice that the graph doesn\\'t have conditional edges for routing! This is because control flow is defined with Command inside node_a.\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"node_a\")\\nbuilder.add_node(node_a)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n# NOTE: there are no edges between nodes A, B and C!\\n\\ngraph = builder.compile()\\n\\n\\nImportant\\nYou might have noticed that we used Command as a return type annotation, e.g. Command[Literal[\"node_b\", \"node_c\"]]. This is necessary for the graph rendering and tells LangGraph that node_a can navigate to node_b and node_c.\\n\\nfrom IPython.display import display, Image\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIf we run the graph multiple times, we\\'d see it take different paths (A -> B or A -> C) based on the random choice in node A.\\ngraph.invoke({\"foo\": \"\"})\\n\\nCalled A\\nCalled C\\n\\nNavigate to a node in a parent graph¶\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n\\nLet\\'s demonstrate this using the above example. We\\'ll do so by changing node_a in the above example into a single-node graph that we\\'ll add as a subgraph to our parent graph.\\n\\nState updates with Command.PARENT\\nWhen you send updates from a subgraph node to a parent graph node for a key that\\'s shared by both parent and subgraph state schemas, you must define a reducer for the key you\\'re updating in the parent graph state. See the example below.\\n\\nimport operator\\nfrom typing_extensions import Annotated\\n\\nclass State(TypedDict):\\n    # NOTE: we define a reducer here\\n    foo: Annotated[str, operator.add]\\n\\ndef node_a(state: State):\\n    print(\"Called A\")\\n    value = random.choice([\"a\", \"b\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"a\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        update={\"foo\": value},\\n        goto=goto,\\n        # this tells LangGraph to navigate to node_b or node_c in the parent graph\\n        # NOTE: this will navigate to the closest parent graph relative to the subgraph\\n        graph=Command.PARENT,\\n    )\\n\\nsubgraph = StateGraph(State).add_node(node_a).add_edge(START, \"node_a\").compile()\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    # NOTE: since we\\'ve defined a reducer, we don\\'t need to manually append\\n    # new characters to existing \\'foo\\' value. instead, reducer will append these\\n    # automatically (via operator.add)\\n    return {\"foo\": \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": \"c\"}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"subgraph\")\\nbuilder.add_node(\"subgraph\", subgraph)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"foo\": \"\"})\\n\\nCalled A\\nCalled C\\n\\nUse inside tools¶\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]}) from the tool:\\n@tool\\ndef lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\\n    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\\n    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\\n    return Command(\\n        update={\\n            # update the state keys\\n            \"user_info\": user_info,\\n            # update the message history\\n            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\\n        }\\n    )\\n\\n\\nImportant\\nYou MUST include messages (or any state key used for the message history) in Command.update when returning Command from a tool and the list of messages in messages MUST contain a ToolMessage. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).\\n\\nIf you are using tools that update state via Command, we recommend using prebuilt ToolNode which automatically handles tools returning Command objects and propagates them to the graph state. If you\\'re writing a custom node that calls tools, you would need to manually propagate Command objects returned by the tools as the update from the node.\\nVisualize your graph¶\\nHere we demonstrate how to visualize the graphs you create.\\nYou can visualize any arbitrary Graph, including StateGraph. Let\\'s have some fun by drawing fractals :).\\nAPI Reference: StateGraph | START | END | add_messages\\nimport random\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\nclass MyNode:\\n    def __init__(self, name: str):\\n        self.name = name\\n    def __call__(self, state: State):\\n        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\\n\\ndef route(state) -> Literal[\"entry_node\", \"__end__\"]:\\n    if len(state[\"messages\"]) > 10:\\n        return \"__end__\"\\n    return \"entry_node\"\\n\\ndef add_fractal_nodes(builder, current_node, level, max_level):\\n    if level > max_level:\\n        return\\n    # Number of nodes to create at this level\\n    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\\n    for i in range(num_nodes):\\n        nm = [\"A\", \"B\", \"C\"][i]\\n        node_name = f\"node_{current_node}_{nm}\"\\n        builder.add_node(node_name, MyNode(node_name))\\n        builder.add_edge(current_node, node_name)\\n        # Recursively add more nodes\\n        r = random.random()\\n        if r > 0.2 and level + 1 < max_level:\\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\\n        elif r > 0.05:\\n            builder.add_conditional_edges(node_name, route, node_name)\\n        else:\\n            # End\\n            builder.add_edge(node_name, \"__end__\")\\n\\ndef build_fractal_graph(max_level: int):\\n    builder = StateGraph(State)\\n    entry_point = \"entry_node\"\\n    builder.add_node(entry_point, MyNode(entry_point))\\n    builder.add_edge(START, entry_point)\\n    add_fractal_nodes(builder, entry_point, 1, max_level)\\n    # Optional: set a finish point if required\\n    builder.add_edge(entry_point, END)  # or any specific node\\n    return builder.compile()\\n\\napp = build_fractal_graph(3)\\n\\nMermaid¶\\nWe can also convert a graph class into Mermaid syntax.\\nprint(app.get_graph().draw_mermaid())\\n\\n%%{init: {\\'flowchart\\': {\\'curve\\': \\'linear\\'}}}%%\\ngraph TD;\\n    __start__([<p>__start__</p>]):::first\\n    entry_node(entry_node)\\n    node_entry_node_A(node_entry_node_A)\\n    node_entry_node_B(node_entry_node_B)\\n    node_node_entry_node_B_A(node_node_entry_node_B_A)\\n    node_node_entry_node_B_B(node_node_entry_node_B_B)\\n    node_node_entry_node_B_C(node_node_entry_node_B_C)\\n    __end__([<p>__end__</p>]):::last\\n    __start__ --> entry_node;\\n    entry_node --> __end__;\\n    entry_node --> node_entry_node_A;\\n    entry_node --> node_entry_node_B;\\n    node_entry_node_B --> node_node_entry_node_B_A;\\n    node_entry_node_B --> node_node_entry_node_B_B;\\n    node_entry_node_B --> node_node_entry_node_B_C;\\n    node_entry_node_A -.-> entry_node;\\n    node_entry_node_A -.-> __end__;\\n    node_node_entry_node_B_A -.-> entry_node;\\n    node_node_entry_node_B_A -.-> __end__;\\n    node_node_entry_node_B_B -.-> entry_node;\\n    node_node_entry_node_B_B -.-> __end__;\\n    node_node_entry_node_B_C -.-> entry_node;\\n    node_node_entry_node_B_C -.-> __end__;\\n    classDef default fill:#f2f0ff,line-height:1.2\\n    classDef first fill-opacity:0\\n    classDef last fill:#bfb6fc\\n\\nPNG¶\\nIf preferred, we could render the Graph into a  .png. Here we could use three options:\\n\\nUsing Mermaid.ink API (does not require additional packages)\\nUsing Mermaid + Pyppeteer (requires pip install pyppeteer)\\nUsing graphviz (which requires pip install graphviz)\\n\\nUsing Mermaid.Ink\\nBy default, draw_mermaid_png() uses Mermaid.Ink\\'s API to generate the diagram.\\nAPI Reference: CurveStyle | MermaidDrawMethod | NodeStyles\\nfrom IPython.display import Image, display\\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\\n\\ndisplay(Image(app.get_graph().draw_mermaid_png()))\\n\\n\\nUsing Mermaid + Pyppeteer\\nimport nest_asyncio\\n\\nnest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\\n\\ndisplay(\\n    Image(\\n        app.get_graph().draw_mermaid_png(\\n            curve_style=CurveStyle.LINEAR,\\n            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\\n            wrap_label_n_words=9,\\n            output_file_path=None,\\n            draw_method=MermaidDrawMethod.PYPPETEER,\\n            background_color=\"white\",\\n            padding=10,\\n        )\\n    )\\n)\\n\\nUsing Graphviz\\ntry:\\n    display(Image(app.get_graph().draw_png()))\\nexcept ImportError:\\n    print(\\n        \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\"\\n    )\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Overview\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Overview\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_urls=[\n",
    "    \"https://python.langchain.com/docs/introduction/\",\n",
    "    \"https://python.langchain.com/docs/tutorials/\",\n",
    "    \"https://python.langchain.com/docs/concepts/why_langchain/\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b082a248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/concepts/why-langgraph/', 'title': 'Overview', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Overview\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n            Build a custom workflow\\n          \\n\\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n    1. Build a basic chatbot\\n    \\n  \\n\\n\\n\\n\\n\\n    2. Add tools\\n    \\n  \\n\\n\\n\\n\\n\\n    3. Add memory\\n    \\n  \\n\\n\\n\\n\\n\\n    4. Add human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n    5. Customize state\\n    \\n  \\n\\n\\n\\n\\n\\n    6. Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview¶\\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\\n\\nReliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\\nLow-level and extensible. Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\\nFirst-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.\\n\\nLearn LangGraph basics¶\\nTo get acquainted with LangGraph's key concepts and features, complete the following LangGraph basics tutorials series:\\n\\nBuild a basic chatbot\\nAdd tools\\nAdd memory\\nAdd human-in-the-loop controls\\nCustomize state\\nTime travel\\n\\nIn completing this series of tutorials, you will build a support chatbot in LangGraph that can:\\n\\n✅ Answer common questions by searching the web\\n✅ Maintain conversation state across calls  \\n✅ Route complex queries to a human for review  \\n✅ Use custom state to control its behavior  \\n✅ Rewind and explore alternative conversation paths  \\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Start with a prebuilt agent\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                1. Build a basic chatbot\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/tutorials/workflows/', 'title': 'Workflows & agents', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows & agents\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Workflows & agents\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n    Build a custom workflow\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Run a local server\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    General concepts\\n    \\n  \\n\\n\\n\\n\\n\\n            General concepts\\n          \\n\\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n    Workflows & agents\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Agent architectures\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Set up\\n    \\n\\n\\n\\n\\n\\n      Building Blocks: The Augmented LLM\\n    \\n\\n\\n\\n\\n\\n      Prompt chaining\\n    \\n\\n\\n\\n\\n\\n      Parallelization\\n    \\n\\n\\n\\n\\n\\n      Routing\\n    \\n\\n\\n\\n\\n\\n      Orchestrator-Worker\\n    \\n\\n\\n\\n\\n\\n      Evaluator-optimizer\\n    \\n\\n\\n\\n\\n\\n      Agent\\n    \\n\\n\\n\\n\\n\\n\\n      Pre-built\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      What LangGraph provides\\n    \\n\\n\\n\\n\\n\\n\\n      Persistence: Human-in-the-Loop\\n    \\n\\n\\n\\n\\n\\n      Persistence: Memory\\n    \\n\\n\\n\\n\\n\\n      Streaming\\n    \\n\\n\\n\\n\\n\\n      Deployment\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWorkflows and Agents¶\\nThis guide reviews common patterns for agentic systems. In describing these systems, it can be useful to make a distinction between \"workflows\" and \"agents\". One way to think about this difference is nicely explained in Anthropic\\'s Building Effective Agents blog post:\\n\\nWorkflows are systems where LLMs and tools are orchestrated through predefined code paths.\\nAgents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.\\n\\nHere is a simple way to visualize these differences:\\n\\nWhen building agents and workflows, LangGraph offers a number of benefits including persistence, streaming, and support for debugging as well as deployment.\\nSet up¶\\nYou can use any chat model that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.\\n\\nInstall dependencies\\npip install langchain_core langchain-anthropic langgraph \\n\\n\\nInitialize an LLM\\nAPI Reference: ChatAnthropic\\nimport os\\nimport getpass\\n\\nfrom langchain_anthropic import ChatAnthropic\\n\\ndef _set_env(var: str):\\n    if not os.environ.get(var):\\n        os.environ[var] = getpass.getpass(f\"{var}: \")\\n\\n\\n_set_env(\"ANTHROPIC_API_KEY\")\\n\\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\\n\\nBuilding Blocks: The Augmented LLM¶\\nLLM have augmentations that support building workflows and agents. These include structured outputs and tool calling, as shown in this image from the Anthropic blog on Building Effective Agents:\\n\\n# Schema for structured output\\nfrom pydantic import BaseModel, Field\\n\\nclass SearchQuery(BaseModel):\\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\\n    justification: str = Field(\\n        None, description=\"Why this query is relevant to the user\\'s request.\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nstructured_llm = llm.with_structured_output(SearchQuery)\\n\\n# Invoke the augmented LLM\\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\\n\\n# Define a tool\\ndef multiply(a: int, b: int) -> int:\\n    return a * b\\n\\n# Augment the LLM with tools\\nllm_with_tools = llm.bind_tools([multiply])\\n\\n# Invoke the LLM with input that triggers the tool call\\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\\n\\n# Get the tool call\\nmsg.tool_calls\\n\\nPrompt chaining¶\\nIn prompt chaining, each LLM call processes the output of the previous one. \\nAs noted in the Anthropic blog on Building Effective Agents: \\n\\nPrompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate” in the diagram below) on any intermediate steps to ensure that the process is still on track.\\nWhen to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom IPython.display import Image, display\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    improved_joke: str\\n    final_joke: str\\n\\n\\n# Nodes\\ndef generate_joke(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a short joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef check_punchline(state: State):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\\n        return \"Pass\"\\n    return \"Fail\"\\n\\n\\ndef improve_joke(state: State):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state[\\'joke\\']}\")\\n    return {\"improved_joke\": msg.content}\\n\\n\\ndef polish_joke(state: State):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state[\\'improved_joke\\']}\")\\n    return {\"final_joke\": msg.content}\\n\\n\\n# Build workflow\\nworkflow = StateGraph(State)\\n\\n# Add nodes\\nworkflow.add_node(\"generate_joke\", generate_joke)\\nworkflow.add_node(\"improve_joke\", improve_joke)\\nworkflow.add_node(\"polish_joke\", polish_joke)\\n\\n# Add edges to connect nodes\\nworkflow.add_edge(START, \"generate_joke\")\\nworkflow.add_conditional_edges(\\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\\n)\\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\\nworkflow.add_edge(\"polish_joke\", END)\\n\\n# Compile\\nchain = workflow.compile()\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/a0281fca-3a71-46de-beee-791468607b75/r\\nResources:\\nLangChain Academy\\nSee our lesson on Prompt Chaining here.\\n\\n\\nfrom langgraph.func import entrypoint, task\\n\\n\\n# Tasks\\n@task\\ndef generate_joke(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\\n    return msg.content\\n\\n\\ndef check_punchline(joke: str):\\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\\n    # Simple check - does the joke contain \"?\" or \"!\"\\n    if \"?\" in joke or \"!\" in joke:\\n        return \"Fail\"\\n\\n    return \"Pass\"\\n\\n\\n@task\\ndef improve_joke(joke: str):\\n    \"\"\"Second LLM call to improve the joke\"\"\"\\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\\n    return msg.content\\n\\n\\n@task\\ndef polish_joke(joke: str):\\n    \"\"\"Third LLM call for final polish\"\"\"\\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\\n    return msg.content\\n\\n\\n@entrypoint()\\ndef prompt_chaining_workflow(topic: str):\\n    original_joke = generate_joke(topic).result()\\n    if check_punchline(original_joke) == \"Pass\":\\n        return original_joke\\n\\n    improved_joke = improve_joke(original_joke).result()\\n    return polish_joke(improved_joke).result()\\n\\n# Invoke\\nfor step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/332fa4fc-b6ca-416e-baa3-161625e69163/r\\n\\n\\n\\nParallelization¶\\nWith parallelization, LLMs work simultaneously on a task:\\n\\nLLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs.\\nWhen to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str\\n    joke: str\\n    story: str\\n    poem: str\\n    combined_output: str\\n\\n\\n# Nodes\\ndef call_llm_1(state: State):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n\\n    msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef call_llm_2(state: State):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n\\n    msg = llm.invoke(f\"Write a story about {state[\\'topic\\']}\")\\n    return {\"story\": msg.content}\\n\\n\\ndef call_llm_3(state: State):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n\\n    msg = llm.invoke(f\"Write a poem about {state[\\'topic\\']}\")\\n    return {\"poem\": msg.content}\\n\\n\\ndef aggregator(state: State):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {state[\\'topic\\']}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{state[\\'story\\']}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{state[\\'joke\\']}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{state[\\'poem\\']}\"\\n    return {\"combined_output\": combined}\\n\\n\\n# Build workflow\\nparallel_builder = StateGraph(State)\\n\\n# Add nodes\\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\\nparallel_builder.add_node(\"aggregator\", aggregator)\\n\\n# Add edges to connect nodes\\nparallel_builder.add_edge(START, \"call_llm_1\")\\nparallel_builder.add_edge(START, \"call_llm_2\")\\nparallel_builder.add_edge(START, \"call_llm_3\")\\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\\nparallel_builder.add_edge(\"aggregator\", END)\\nparallel_workflow = parallel_builder.compile()\\n\\n# Show workflow\\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\\nprint(state[\"combined_output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/3be2e53c-ca94-40dd-934f-82ff87fac277/r\\nResources:\\nDocumentation\\nSee our documentation on parallelization here.\\nLangChain Academy\\nSee our lesson on parallelization here.\\n\\n\\n@task\\ndef call_llm_1(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_2(topic: str):\\n    \"\"\"Second LLM call to generate story\"\"\"\\n    msg = llm.invoke(f\"Write a story about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef call_llm_3(topic):\\n    \"\"\"Third LLM call to generate poem\"\"\"\\n    msg = llm.invoke(f\"Write a poem about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef aggregator(topic, joke, story, poem):\\n    \"\"\"Combine the joke and story into a single output\"\"\"\\n\\n    combined = f\"Here\\'s a story, joke, and poem about {topic}!\\\\n\\\\n\"\\n    combined += f\"STORY:\\\\n{story}\\\\n\\\\n\"\\n    combined += f\"JOKE:\\\\n{joke}\\\\n\\\\n\"\\n    combined += f\"POEM:\\\\n{poem}\"\\n    return combined\\n\\n\\n# Build workflow\\n@entrypoint()\\ndef parallel_workflow(topic: str):\\n    joke_fut = call_llm_1(topic)\\n    story_fut = call_llm_2(topic)\\n    poem_fut = call_llm_3(topic)\\n    return aggregator(\\n        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\\n    ).result()\\n\\n# Invoke\\nfor step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/623d033f-e814-41e9-80b1-75e6abb67801/r\\n\\n\\n\\nRouting¶\\nRouting classifies an input and directs it to a followup task. As noted in the Anthropic blog on Building Effective Agents: \\n\\nRouting classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.\\nWhen to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing_extensions import Literal\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n# State\\nclass State(TypedDict):\\n    input: str\\n    decision: str\\n    output: str\\n\\n\\n# Nodes\\ndef llm_call_1(state: State):\\n    \"\"\"Write a story\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_2(state: State):\\n    \"\"\"Write a joke\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_3(state: State):\\n    \"\"\"Write a poem\"\"\"\\n\\n    result = llm.invoke(state[\"input\"])\\n    return {\"output\": result.content}\\n\\n\\ndef llm_call_router(state: State):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=state[\"input\"]),\\n        ]\\n    )\\n\\n    return {\"decision\": decision.step}\\n\\n\\n# Conditional edge function to route to the appropriate node\\ndef route_decision(state: State):\\n    # Return the node name you want to visit next\\n    if state[\"decision\"] == \"story\":\\n        return \"llm_call_1\"\\n    elif state[\"decision\"] == \"joke\":\\n        return \"llm_call_2\"\\n    elif state[\"decision\"] == \"poem\":\\n        return \"llm_call_3\"\\n\\n\\n# Build workflow\\nrouter_builder = StateGraph(State)\\n\\n# Add nodes\\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\\n\\n# Add edges to connect nodes\\nrouter_builder.add_edge(START, \"llm_call_router\")\\nrouter_builder.add_conditional_edges(\\n    \"llm_call_router\",\\n    route_decision,\\n    {  # Name returned by route_decision : Name of next node to visit\\n        \"llm_call_1\": \"llm_call_1\",\\n        \"llm_call_2\": \"llm_call_2\",\\n        \"llm_call_3\": \"llm_call_3\",\\n    },\\n)\\nrouter_builder.add_edge(\"llm_call_1\", END)\\nrouter_builder.add_edge(\"llm_call_2\", END)\\nrouter_builder.add_edge(\"llm_call_3\", END)\\n\\n# Compile workflow\\nrouter_workflow = router_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\\nprint(state[\"output\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/c4580b74-fe91-47e4-96fe-7fac598d509c/r\\nResources:\\nLangChain Academy\\nSee our lesson on routing here.\\nExamples\\nHere is RAG workflow that routes questions. See our video here.\\n\\n\\nfrom typing_extensions import Literal\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, SystemMessage\\n\\n\\n# Schema for structured output to use as routing logic\\nclass Route(BaseModel):\\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\\n        None, description=\"The next step in the routing process\"\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nrouter = llm.with_structured_output(Route)\\n\\n\\n@task\\ndef llm_call_1(input_: str):\\n    \"\"\"Write a story\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_2(input_: str):\\n    \"\"\"Write a joke\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\n@task\\ndef llm_call_3(input_: str):\\n    \"\"\"Write a poem\"\"\"\\n    result = llm.invoke(input_)\\n    return result.content\\n\\n\\ndef llm_call_router(input_: str):\\n    \"\"\"Route the input to the appropriate node\"\"\"\\n    # Run the augmented LLM with structured output to serve as routing logic\\n    decision = router.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Route the input to story, joke, or poem based on the user\\'s request.\"\\n            ),\\n            HumanMessage(content=input_),\\n        ]\\n    )\\n    return decision.step\\n\\n\\n# Create workflow\\n@entrypoint()\\ndef router_workflow(input_: str):\\n    next_step = llm_call_router(input_)\\n    if next_step == \"story\":\\n        llm_call = llm_call_1\\n    elif next_step == \"joke\":\\n        llm_call = llm_call_2\\n    elif next_step == \"poem\":\\n        llm_call = llm_call_3\\n\\n    return llm_call(input_).result()\\n\\n# Invoke\\nfor step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/5e2eb979-82dd-402c-b1a0-a8cceaf2a28a/r\\n\\n\\n\\nOrchestrator-Worker¶\\nWith orchestrator-worker, an orchestrator breaks down a task and delegates each sub-task to workers. As noted in the Anthropic blog on Building Effective Agents: \\n\\nIn the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.\\nWhen to use this workflow: This workflow is well-suited for complex tasks where you can’t predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it’s topographically similar, the key difference from parallelization is its flexibility—subtasks aren\\'t pre-defined, but determined by the orchestrator based on the specific input.\\n\\n\\nGraph APIFunctional API\\n\\n\\nfrom typing import Annotated, List\\nimport operator\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\nCreating Workers in LangGraph\\nBecause orchestrator-worker workflows are common, LangGraph has the Send API to support this. It lets you dynamically create worker nodes and send each one a specific input. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. As you can see below, we iterate over a list of sections and Send each to a worker node. See further documentation here and here.\\nfrom langgraph.types import Send\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    topic: str  # Report topic\\n    sections: list[Section]  # List of report sections\\n    completed_sections: Annotated[\\n        list, operator.add\\n    ]  # All workers write to this key in parallel\\n    final_report: str  # Final report\\n\\n\\n# Worker state\\nclass WorkerState(TypedDict):\\n    section: Section\\n    completed_sections: Annotated[list, operator.add]\\n\\n\\n# Nodes\\ndef orchestrator(state: State):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {state[\\'topic\\']}\"),\\n        ]\\n    )\\n\\n    return {\"sections\": report_sections.sections}\\n\\n\\ndef llm_call(state: WorkerState):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    section = llm.invoke(\\n        [\\n            SystemMessage(\\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\\n            ),\\n            HumanMessage(\\n                content=f\"Here is the section name: {state[\\'section\\'].name} and description: {state[\\'section\\'].description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return {\"completed_sections\": [section.content]}\\n\\n\\ndef synthesizer(state: State):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n\\n    # List of completed sections\\n    completed_sections = state[\"completed_sections\"]\\n\\n    # Format completed section to str to use as context for final sections\\n    completed_report_sections = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n\\n    return {\"final_report\": completed_report_sections}\\n\\n\\n# Conditional edge function to create llm_call workers that each write a section of the report\\ndef assign_workers(state: State):\\n    \"\"\"Assign a worker to each section in the plan\"\"\"\\n\\n    # Kick off section writing in parallel via Send() API\\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\\n\\n\\n# Build workflow\\norchestrator_worker_builder = StateGraph(State)\\n\\n# Add the nodes\\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\\n\\n# Add edges to connect nodes\\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\\norchestrator_worker_builder.add_conditional_edges(\\n    \"orchestrator\", assign_workers, [\"llm_call\"]\\n)\\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\\n\\n# Compile the workflow\\norchestrator_worker = orchestrator_worker_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\\n\\nfrom IPython.display import Markdown\\nMarkdown(state[\"final_report\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/78cbcfc3-38bf-471d-b62a-b299b144237d/r\\nResources:\\nLangChain Academy\\nSee our lesson on orchestrator-worker here.\\nExamples\\nHere is a project that uses orchestrator-worker for report planning and writing. See our video here.\\n\\n\\nfrom typing import List\\n\\n\\n# Schema for structured output to use in planning\\nclass Section(BaseModel):\\n    name: str = Field(\\n        description=\"Name for this section of the report.\",\\n    )\\n    description: str = Field(\\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\\n    )\\n\\n\\nclass Sections(BaseModel):\\n    sections: List[Section] = Field(\\n        description=\"Sections of the report.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nplanner = llm.with_structured_output(Sections)\\n\\n\\n@task\\ndef orchestrator(topic: str):\\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\\n    # Generate queries\\n    report_sections = planner.invoke(\\n        [\\n            SystemMessage(content=\"Generate a plan for the report.\"),\\n            HumanMessage(content=f\"Here is the report topic: {topic}\"),\\n        ]\\n    )\\n\\n    return report_sections.sections\\n\\n\\n@task\\ndef llm_call(section: Section):\\n    \"\"\"Worker writes a section of the report\"\"\"\\n\\n    # Generate section\\n    result = llm.invoke(\\n        [\\n            SystemMessage(content=\"Write a report section.\"),\\n            HumanMessage(\\n                content=f\"Here is the section name: {section.name} and description: {section.description}\"\\n            ),\\n        ]\\n    )\\n\\n    # Write the updated section to completed sections\\n    return result.content\\n\\n\\n@task\\ndef synthesizer(completed_sections: list[str]):\\n    \"\"\"Synthesize full report from sections\"\"\"\\n    final_report = \"\\\\n\\\\n---\\\\n\\\\n\".join(completed_sections)\\n    return final_report\\n\\n\\n@entrypoint()\\ndef orchestrator_worker(topic: str):\\n    sections = orchestrator(topic).result()\\n    section_futures = [llm_call(section) for section in sections]\\n    final_report = synthesizer(\\n        [section_fut.result() for section_fut in section_futures]\\n    ).result()\\n    return final_report\\n\\n# Invoke\\nreport = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\\nfrom IPython.display import Markdown\\nMarkdown(report)\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/75a636d0-6179-4a12-9836-e0aa571e87c5/r\\n\\n\\n\\nEvaluator-optimizer¶\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop:\\n\\nIn the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop.\\nWhen to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.\\n\\n\\nGraph APIFunctional API\\n\\n\\n# Graph state\\nclass State(TypedDict):\\n    joke: str\\n    topic: str\\n    feedback: str\\n    funny_or_not: str\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\ndef llm_call_generator(state: State):\\n    \"\"\"LLM generates a joke\"\"\"\\n\\n    if state.get(\"feedback\"):\\n        msg = llm.invoke(\\n            f\"Write a joke about {state[\\'topic\\']} but take into account the feedback: {state[\\'feedback\\']}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {state[\\'topic\\']}\")\\n    return {\"joke\": msg.content}\\n\\n\\ndef llm_call_evaluator(state: State):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n\\n    grade = evaluator.invoke(f\"Grade the joke {state[\\'joke\\']}\")\\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\\n\\n\\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\\ndef route_joke(state: State):\\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\\n\\n    if state[\"funny_or_not\"] == \"funny\":\\n        return \"Accepted\"\\n    elif state[\"funny_or_not\"] == \"not funny\":\\n        return \"Rejected + Feedback\"\\n\\n\\n# Build workflow\\noptimizer_builder = StateGraph(State)\\n\\n# Add the nodes\\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\\n\\n# Add edges to connect nodes\\noptimizer_builder.add_edge(START, \"llm_call_generator\")\\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\\noptimizer_builder.add_conditional_edges(\\n    \"llm_call_evaluator\",\\n    route_joke,\\n    {  # Name returned by route_joke : Name of next node to visit\\n        \"Accepted\": END,\\n        \"Rejected + Feedback\": \"llm_call_generator\",\\n    },\\n)\\n\\n# Compile the workflow\\noptimizer_workflow = optimizer_builder.compile()\\n\\n# Show the workflow\\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\\nprint(state[\"joke\"])\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/86ab3e60-2000-4bff-b988-9b89a3269789/r\\nResources:\\nExamples\\nHere is an assistant that uses evaluator-optimizer to improve a report. See our video here.\\nHere is a RAG workflow that grades answers for hallucinations or errors. See our video here.\\n\\n\\n# Schema for structured output to use in evaluation\\nclass Feedback(BaseModel):\\n    grade: Literal[\"funny\", \"not funny\"] = Field(\\n        description=\"Decide if the joke is funny or not.\",\\n    )\\n    feedback: str = Field(\\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\\n    )\\n\\n\\n# Augment the LLM with schema for structured output\\nevaluator = llm.with_structured_output(Feedback)\\n\\n\\n# Nodes\\n@task\\ndef llm_call_generator(topic: str, feedback: Feedback):\\n    \"\"\"LLM generates a joke\"\"\"\\n    if feedback:\\n        msg = llm.invoke(\\n            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\\n        )\\n    else:\\n        msg = llm.invoke(f\"Write a joke about {topic}\")\\n    return msg.content\\n\\n\\n@task\\ndef llm_call_evaluator(joke: str):\\n    \"\"\"LLM evaluates the joke\"\"\"\\n    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\\n    return feedback\\n\\n\\n@entrypoint()\\ndef optimizer_workflow(topic: str):\\n    feedback = None\\n    while True:\\n        joke = llm_call_generator(topic, feedback).result()\\n        feedback = llm_call_evaluator(joke).result()\\n        if feedback.grade == \"funny\":\\n            break\\n\\n    return joke\\n\\n# Invoke\\nfor step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\\n    print(step)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/f66830be-4339-4a6b-8a93-389ce5ae27b4/r\\n\\n\\n\\nAgent¶\\nAgents are typically implemented as an LLM performing actions (via tool-calling) based on environmental feedback in a loop. As noted in the Anthropic blog on Building Effective Agents:\\n\\nAgents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully.\\nWhen to use agents: Agents can be used for open-ended problems where it’s difficult or impossible to predict the required number of steps, and where you can’t hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents\\' autonomy makes them ideal for scaling tasks in trusted environments.\\n\\n\\nAPI Reference: tool\\nfrom langchain_core.tools import tool\\n\\n\\n# Define tools\\n@tool\\ndef multiply(a: int, b: int) -> int:\\n    \"\"\"Multiply a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a * b\\n\\n\\n@tool\\ndef add(a: int, b: int) -> int:\\n    \"\"\"Adds a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a + b\\n\\n\\n@tool\\ndef divide(a: int, b: int) -> float:\\n    \"\"\"Divide a and b.\\n\\n    Args:\\n        a: first int\\n        b: second int\\n    \"\"\"\\n    return a / b\\n\\n\\n# Augment the LLM with tools\\ntools = [add, multiply, divide]\\ntools_by_name = {tool.name: tool for tool in tools}\\nllm_with_tools = llm.bind_tools(tools)\\n\\nGraph APIFunctional API\\n\\n\\nfrom langgraph.graph import MessagesState\\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\\n\\n\\n# Nodes\\ndef llm_call(state: MessagesState):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n\\n    return {\\n        \"messages\": [\\n            llm_with_tools.invoke(\\n                [\\n                    SystemMessage(\\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n                    )\\n                ]\\n                + state[\"messages\"]\\n            )\\n        ]\\n    }\\n\\n\\ndef tool_node(state: dict):\\n    \"\"\"Performs the tool call\"\"\"\\n\\n    result = []\\n    for tool_call in state[\"messages\"][-1].tool_calls:\\n        tool = tools_by_name[tool_call[\"name\"]]\\n        observation = tool.invoke(tool_call[\"args\"])\\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\\n    return {\"messages\": result}\\n\\n\\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\\ndef should_continue(state: MessagesState) -> Literal[\"environment\", END]:\\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\\n\\n    messages = state[\"messages\"]\\n    last_message = messages[-1]\\n    # If the LLM makes a tool call, then perform an action\\n    if last_message.tool_calls:\\n        return \"Action\"\\n    # Otherwise, we stop (reply to the user)\\n    return END\\n\\n\\n# Build workflow\\nagent_builder = StateGraph(MessagesState)\\n\\n# Add nodes\\nagent_builder.add_node(\"llm_call\", llm_call)\\nagent_builder.add_node(\"environment\", tool_node)\\n\\n# Add edges to connect nodes\\nagent_builder.add_edge(START, \"llm_call\")\\nagent_builder.add_conditional_edges(\\n    \"llm_call\",\\n    should_continue,\\n    {\\n        # Name returned by should_continue : Name of next node to visit\\n        \"Action\": \"environment\",\\n        END: END,\\n    },\\n)\\nagent_builder.add_edge(\"environment\", \"llm_call\")\\n\\n# Compile the agent\\nagent = agent_builder.compile()\\n\\n# Show the agent\\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/051f0391-6761-4f8c-a53b-22231b016690/r\\nResources:\\nLangChain Academy\\nSee our lesson on agents here.\\nExamples\\nHere is a project that uses a tool calling agent to create / store long-term memories.\\n\\n\\nfrom langgraph.graph import add_messages\\nfrom langchain_core.messages import (\\n    SystemMessage,\\n    HumanMessage,\\n    BaseMessage,\\n    ToolCall,\\n)\\n\\n\\n@task\\ndef call_llm(messages: list[BaseMessage]):\\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\\n    return llm_with_tools.invoke(\\n        [\\n            SystemMessage(\\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\\n            )\\n        ]\\n        + messages\\n    )\\n\\n\\n@task\\ndef call_tool(tool_call: ToolCall):\\n    \"\"\"Performs the tool call\"\"\"\\n    tool = tools_by_name[tool_call[\"name\"]]\\n    return tool.invoke(tool_call)\\n\\n\\n@entrypoint()\\ndef agent(messages: list[BaseMessage]):\\n    llm_response = call_llm(messages).result()\\n\\n    while True:\\n        if not llm_response.tool_calls:\\n            break\\n\\n        # Execute tools\\n        tool_result_futures = [\\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\\n        ]\\n        tool_results = [fut.result() for fut in tool_result_futures]\\n        messages = add_messages(messages, [llm_response, *tool_results])\\n        llm_response = call_llm(messages).result()\\n\\n    messages = add_messages(messages, llm_response)\\n    return messages\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\\n    print(chunk)\\n    print(\"\\\\n\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/42ae8bf9-3935-4504-a081-8ddbcbfc8b2e/r\\n\\n\\n\\nPre-built¶\\nLangGraph also provides a pre-built method for creating an agent as defined above (using the create_react_agent function):\\nhttps://langchain-ai.github.io/langgraph/how-tos/create-react-agent/\\nAPI Reference: create_react_agent\\nfrom langgraph.prebuilt import create_react_agent\\n\\n# Pass in:\\n# (1) the augmented LLM with tools\\n# (2) the tools list (which is used to create the tool node)\\npre_built_agent = create_react_agent(llm, tools=tools)\\n\\n# Show the agent\\ndisplay(Image(pre_built_agent.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\\nmessages = pre_built_agent.invoke({\"messages\": messages})\\nfor m in messages[\"messages\"]:\\n    m.pretty_print()\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r\\nWhat LangGraph provides¶\\nBy constructing each of the above in LangGraph, we get a few things:\\nPersistence: Human-in-the-Loop¶\\nLangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.\\nPersistence: Memory¶\\nLangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:\\nStreaming¶\\nLangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.\\nDeployment¶\\nLangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Run a local server\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Agent architectures\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://langchain-ai.github.io/langgraph/how-tos/graph-api/#map-reduce-and-the-send-api', 'title': 'Use the Graph API', 'description': 'Build reliable, stateful AI systems, without giving up control', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nUse the Graph API\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Use the Graph API\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Guides\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Guides\\n          \\n\\n\\n\\n\\n\\n    Agent development\\n    \\n  \\n\\n\\n\\n\\n\\n            Agent development\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n    Run an agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph APIs\\n    \\n  \\n\\n\\n\\n\\n\\n            LangGraph APIs\\n          \\n\\n\\n\\n\\n\\n    Graph API\\n    \\n  \\n\\n\\n\\n\\n\\n            Graph API\\n          \\n\\n\\n\\n\\n    Overview\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Use the Graph API\\n    \\n  \\n\\n\\n\\n\\n    Use the Graph API\\n    \\n  \\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Setup\\n    \\n\\n\\n\\n\\n\\n      Define and update state\\n    \\n\\n\\n\\n\\n\\n\\n      Define state\\n    \\n\\n\\n\\n\\n\\n      Update state\\n    \\n\\n\\n\\n\\n\\n      Process state updates with reducers\\n    \\n\\n\\n\\n\\n\\n\\n      MessagesState\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Define input and output schemas\\n    \\n\\n\\n\\n\\n\\n      Pass private state between nodes\\n    \\n\\n\\n\\n\\n\\n      Use Pydantic models for graph state\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Add runtime configuration\\n    \\n\\n\\n\\n\\n\\n      Add retry policies\\n    \\n\\n\\n\\n\\n\\n      Add node caching\\n    \\n\\n\\n\\n\\n\\n      Create a sequence of steps\\n    \\n\\n\\n\\n\\n\\n      Create branches\\n    \\n\\n\\n\\n\\n\\n\\n      Run graph nodes in parallel\\n    \\n\\n\\n\\n\\n\\n      Defer node execution\\n    \\n\\n\\n\\n\\n\\n      Conditional branching\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Map-Reduce and the Send API\\n    \\n\\n\\n\\n\\n\\n      Create and control loops\\n    \\n\\n\\n\\n\\n\\n\\n      Impose a recursion limit\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Async\\n    \\n\\n\\n\\n\\n\\n      Combine control flow and state updates with Command\\n    \\n\\n\\n\\n\\n\\n\\n      Navigate to a node in a parent graph\\n    \\n\\n\\n\\n\\n\\n      Use inside tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Visualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Functional API\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Runtime\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Core capabilities\\n    \\n  \\n\\n\\n\\n\\n\\n            Core capabilities\\n          \\n\\n\\n\\n\\n    Streaming\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Persistence\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Durable execution\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Memory\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Context\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Models\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tools\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Human-in-the-loop\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Time travel\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Subgraphs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Multi-agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    MCP\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Tracing\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Platform-only capabilities\\n    \\n  \\n\\n\\n\\n\\n\\n            Platform-only capabilities\\n          \\n\\n\\n\\n\\n    LangGraph Platform\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Authentication & access control\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Assistants\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Double-texting\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Webhooks\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Cron jobs\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Server customization\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Data management\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Deployment\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Reference\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Examples\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Additional resources\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Table of contents\\n    \\n\\n\\n\\n\\n      Setup\\n    \\n\\n\\n\\n\\n\\n      Define and update state\\n    \\n\\n\\n\\n\\n\\n\\n      Define state\\n    \\n\\n\\n\\n\\n\\n      Update state\\n    \\n\\n\\n\\n\\n\\n      Process state updates with reducers\\n    \\n\\n\\n\\n\\n\\n\\n      MessagesState\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Define input and output schemas\\n    \\n\\n\\n\\n\\n\\n      Pass private state between nodes\\n    \\n\\n\\n\\n\\n\\n      Use Pydantic models for graph state\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Add runtime configuration\\n    \\n\\n\\n\\n\\n\\n      Add retry policies\\n    \\n\\n\\n\\n\\n\\n      Add node caching\\n    \\n\\n\\n\\n\\n\\n      Create a sequence of steps\\n    \\n\\n\\n\\n\\n\\n      Create branches\\n    \\n\\n\\n\\n\\n\\n\\n      Run graph nodes in parallel\\n    \\n\\n\\n\\n\\n\\n      Defer node execution\\n    \\n\\n\\n\\n\\n\\n      Conditional branching\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Map-Reduce and the Send API\\n    \\n\\n\\n\\n\\n\\n      Create and control loops\\n    \\n\\n\\n\\n\\n\\n\\n      Impose a recursion limit\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Async\\n    \\n\\n\\n\\n\\n\\n      Combine control flow and state updates with Command\\n    \\n\\n\\n\\n\\n\\n\\n      Navigate to a node in a parent graph\\n    \\n\\n\\n\\n\\n\\n      Use inside tools\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n      Visualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow to use the graph API¶\\nThis guide demonstrates the basics of LangGraph\\'s Graph API. It walks through state, as well as composing common graph structures such as sequences, branches, and loops. It also covers LangGraph\\'s control features, including the Send API for map-reduce workflows and the Command API for combining state updates with \"hops\" across nodes.\\nSetup¶\\nInstall langgraph:\\npip install -U langgraph\\n\\n\\nSet up LangSmith for better debugging\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the docs.\\n\\nDefine and update state¶\\nHere we show how to define and update state in LangGraph. We will demonstrate:\\n\\nHow to use state to define a graph\\'s schema\\nHow to use reducers to control how state updates are processed.\\n\\nDefine state¶\\nState in LangGraph can be a TypedDict, Pydantic model, or dataclass. Below we will use TypedDict. See this section for detail on using Pydantic.\\nBy default, graphs will have the same input and output schema, and the state determines that schema. See this section for how to define distinct input and output schemas.\\nLet\\'s consider a simple example using messages. This represents a versatile formulation of state for many LLM applications. See our concepts page for more detail.\\nAPI Reference: AnyMessage\\nfrom langchain_core.messages import AnyMessage\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    messages: list[AnyMessage]\\n    extra_field: int\\n\\nThis state tracks a list of message objects, as well as an extra integer field.\\nUpdate state¶\\nLet\\'s build an example graph with a single node. Our node is just a Python function that reads our graph\\'s state and makes updates to it. The first argument to this function will always be the state:\\nAPI Reference: AIMessage\\nfrom langchain_core.messages import AIMessage\\n\\ndef node(state: State):\\n    messages = state[\"messages\"]\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": messages + [new_message], \"extra_field\": 10}\\n\\nThis node simply appends a message to our message list, and populates an extra field.\\n\\nImportant\\nNodes should return updates to the state directly, instead of mutating the state.\\n\\nLet\\'s next define a simple graph containing this node. We use StateGraph to define a graph that operates on this state. We then use add_node populate our graph.\\nAPI Reference: StateGraph\\nfrom langgraph.graph import StateGraph\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(node)\\nbuilder.set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our graph. See this section for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIn this case, our graph just executes a single node. Let\\'s proceed with a simple invocation:\\nAPI Reference: HumanMessage\\nfrom langchain_core.messages import HumanMessage\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\nresult\\n\\n{\\'messages\\': [HumanMessage(content=\\'Hi\\'), AIMessage(content=\\'Hello!\\')], \\'extra_field\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by updating a single key of the state.\\nWe receive the entire state in the invocation result.\\n\\nFor convenience, we frequently inspect the content of message objects via pretty-print:\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nProcess state updates with reducers¶\\nEach key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.\\nFor TypedDict state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.\\nIn the earlier example, our node updated the \"messages\" key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:\\nfrom typing_extensions import Annotated\\n\\ndef add(left, right):\\n    \"\"\"Can also import `add` from the `operator` built-in.\"\"\"\\n    return left + right\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add]\\n    extra_field: int\\n\\nNow our node can be simplified:\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\nAPI Reference: START\\nfrom langgraph.graph import START\\n\\ngraph = StateGraph(State).add_node(node).add_edge(START, \"node\").compile()\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nMessagesState¶\\nIn practice, there are additional considerations for updating lists of messages:\\n\\nWe may wish to update an existing message in the state.\\nWe may want to accept short-hands for message formats, such as OpenAI format.\\n\\nLangGraph includes a built-in reducer add_messages that handles these considerations:\\nAPI Reference: add_messages\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list[AnyMessage], add_messages]\\n    extra_field: int\\n\\ndef node(state: State):\\n    new_message = AIMessage(\"Hello!\")\\n    return {\"messages\": [new_message], \"extra_field\": 10}\\n\\ngraph = StateGraph(State).add_node(node).set_entry_point(\"node\").compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hi\"}\\n\\nresult = graph.invoke({\"messages\": [input_message]})\\n\\nfor message in result[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nHi\\n================================== Ai Message ==================================\\n\\nHello!\\n\\nThis is a versatile representation of state for applications involving chat models. LangGraph includes a pre-built MessagesState for convenience, so that we can have:\\nfrom langgraph.graph import MessagesState\\n\\nclass State(MessagesState):\\n    extra_field: int\\n\\nDefine input and output schemas¶\\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it\\'s also possible to define distinct input and output schemas for a graph.\\nWhen distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.\\nBelow, we\\'ll see how to define distinct input and output schema.\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# Define the schema for the input\\nclass InputState(TypedDict):\\n    question: str\\n\\n# Define the schema for the output\\nclass OutputState(TypedDict):\\n    answer: str\\n\\n# Define the overall schema, combining both input and output\\nclass OverallState(InputState, OutputState):\\n    pass\\n\\n# Define the node that processes the input and generates an answer\\ndef answer_node(state: InputState):\\n    # Example answer and an extra key\\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\\n\\n# Build the graph with input and output schemas specified\\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\\nbuilder.add_node(answer_node)  # Add the answer node\\nbuilder.add_edge(START, \"answer_node\")  # Define the starting edge\\nbuilder.add_edge(\"answer_node\", END)  # Define the ending edge\\ngraph = builder.compile()  # Compile the graph\\n\\n# Invoke the graph with an input and print the result\\nprint(graph.invoke({\"question\": \"hi\"}))\\n\\n{\\'answer\\': \\'bye\\'}\\n\\nNotice that the output of invoke only includes the output schema.\\nPass private state between nodes¶\\nIn some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn\\'t need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes.\\nBelow, we\\'ll create an example sequential graph consisting of three nodes (node_1, node_2 and node_3), where private data is passed between the first two steps (node_1 and node_2), while the third step (node_3) only has access to the public overall state.\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(TypedDict):\\n    a: str\\n\\n# Output from node_1 contains private data that is not part of the overall state\\nclass Node1Output(TypedDict):\\n    private_data: str\\n\\n# The private data is only shared between node_1 and node_2\\ndef node_1(state: OverallState) -> Node1Output:\\n    output = {\"private_data\": \"set by node_1\"}\\n    print(f\"Entered node `node_1`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 2 input only requests the private data available after node_1\\nclass Node2Input(TypedDict):\\n    private_data: str\\n\\ndef node_2(state: Node2Input) -> OverallState:\\n    output = {\"a\": \"set by node_2\"}\\n    print(f\"Entered node `node_2`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Node 3 only has access to the overall state (no access to private data from node_1)\\ndef node_3(state: OverallState) -> OverallState:\\n    output = {\"a\": \"set by node_3\"}\\n    print(f\"Entered node `node_3`:\\\\n\\\\tInput: {state}.\\\\n\\\\tReturned: {output}\")\\n    return output\\n\\n# Connect nodes in a sequence\\n# node_2 accepts private data from node_1, whereas\\n# node_3 does not see the private data.\\nbuilder = StateGraph(OverallState).add_sequence([node_1, node_2, node_3])\\nbuilder.add_edge(START, \"node_1\")\\ngraph = builder.compile()\\n\\n# Invoke the graph with the initial state\\nresponse = graph.invoke(\\n    {\\n        \"a\": \"set at start\",\\n    }\\n)\\n\\nprint()\\nprint(f\"Output of graph invocation: {response}\")\\n\\nEntered node `node_1`:\\n    Input: {\\'a\\': \\'set at start\\'}.\\n    Returned: {\\'private_data\\': \\'set by node_1\\'}\\nEntered node `node_2`:\\n    Input: {\\'private_data\\': \\'set by node_1\\'}.\\n    Returned: {\\'a\\': \\'set by node_2\\'}\\nEntered node `node_3`:\\n    Input: {\\'a\\': \\'set by node_2\\'}.\\n    Returned: {\\'a\\': \\'set by node_3\\'}\\n\\nOutput of graph invocation: {\\'a\\': \\'set by node_3\\'}\\n\\nUse Pydantic models for graph state¶\\nA StateGraph accepts a state_schema argument on initialization that specifies the \"shape\" of the state that the nodes in the graph can access and update.\\nIn our examples, we typically use a python-native TypedDict or dataclass for state_schema, but state_schema can be any type.\\nHere, we\\'ll see how a Pydantic BaseModel can be used for state_schema to add run-time validation on inputs.\\n\\nKnown Limitations\\n\\nCurrently, the output of the graph will NOT be an instance of a pydantic model.\\nRun-time validation only occurs on inputs into nodes, not on the outputs.\\nThe validation error trace from pydantic does not show which node the error arises in.\\nPydantic\\'s recursive validation can be slow. For performance-sensitive applications, you may want to consider using a dataclass instead.\\n\\n\\nAPI Reference: StateGraph | START | END\\nfrom langgraph.graph import StateGraph, START, END\\nfrom typing_extensions import TypedDict\\nfrom pydantic import BaseModel\\n\\n# The overall state of the graph (this is the public state shared across nodes)\\nclass OverallState(BaseModel):\\n    a: str\\n\\ndef node(state: OverallState):\\n    return {\"a\": \"goodbye\"}\\n\\n# Build the state graph\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(node)  # node_1 is the first node\\nbuilder.add_edge(START, \"node\")  # Start the graph with node_1\\nbuilder.add_edge(\"node\", END)  # End the graph after node_1\\ngraph = builder.compile()\\n\\n# Test the graph with a valid input\\ngraph.invoke({\"a\": \"hello\"})\\n\\nInvoke the graph with an invalid input\\ntry:\\n    graph.invoke({\"a\": 123})  # Should be a string\\nexcept Exception as e:\\n    print(\"An exception was raised because `a` is an integer rather than a string.\")\\n    print(e)\\n\\nAn exception was raised because `a` is an integer rather than a string.\\n1 validation error for OverallState\\na\\n  Input should be a valid string [type=string_type, input_value=123, input_type=int]\\n    For further information visit https://errors.pydantic.dev/2.9/v/string_type\\n\\nSee below for additional features of Pydantic model state:\\n\\nSerialization Behavior\\nWhen using Pydantic models as state schemas, it\\'s important to understand how serialization works, especially when:\\n- Passing Pydantic objects as inputs\\n- Receiving outputs from the graph\\n- Working with nested Pydantic models\\nLet\\'s see these behaviors in action.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass NestedModel(BaseModel):\\n    value: str\\n\\nclass ComplexState(BaseModel):\\n    text: str\\n    count: int\\n    nested: NestedModel\\n\\ndef process_node(state: ComplexState):\\n    # Node receives a validated Pydantic object\\n    print(f\"Input state type: {type(state)}\")\\n    print(f\"Nested type: {type(state.nested)}\")\\n    # Return a dictionary update\\n    return {\"text\": state.text + \" processed\", \"count\": state.count + 1}\\n\\n# Build the graph\\nbuilder = StateGraph(ComplexState)\\nbuilder.add_node(\"process\", process_node)\\nbuilder.add_edge(START, \"process\")\\nbuilder.add_edge(\"process\", END)\\ngraph = builder.compile()\\n\\n# Create a Pydantic instance for input\\ninput_state = ComplexState(text=\"hello\", count=0, nested=NestedModel(value=\"test\"))\\nprint(f\"Input object type: {type(input_state)}\")\\n\\n# Invoke graph with a Pydantic instance\\nresult = graph.invoke(input_state)\\nprint(f\"Output type: {type(result)}\")\\nprint(f\"Output content: {result}\")\\n\\n# Convert back to Pydantic model if needed\\noutput_model = ComplexState(**result)\\nprint(f\"Converted back to Pydantic: {type(output_model)}\")\\n\\n\\n\\nRuntime Type Coercion\\nPydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you\\'re not aware of it.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\n\\nclass CoercionExample(BaseModel):\\n    # Pydantic will coerce string numbers to integers\\n    number: int\\n    # Pydantic will parse string booleans to bool\\n    flag: bool\\n\\ndef inspect_node(state: CoercionExample):\\n    print(f\"number: {state.number} (type: {type(state.number)})\")\\n    print(f\"flag: {state.flag} (type: {type(state.flag)})\")\\n    return {}\\n\\nbuilder = StateGraph(CoercionExample)\\nbuilder.add_node(\"inspect\", inspect_node)\\nbuilder.add_edge(START, \"inspect\")\\nbuilder.add_edge(\"inspect\", END)\\ngraph = builder.compile()\\n\\n# Demonstrate coercion with string inputs that will be converted\\nresult = graph.invoke({\"number\": \"42\", \"flag\": \"true\"})\\n\\n# This would fail with a validation error\\ntry:\\n    graph.invoke({\"number\": \"not-a-number\", \"flag\": \"true\"})\\nexcept Exception as e:\\n    print(f\"\\\\nExpected validation error: {e}\")\\n\\n\\n\\nWorking with Message Models\\nWhen working with LangChain message types in your state schema, there are important considerations for serialization. You should use AnyMessage (rather than BaseMessage) for proper serialization/deserialization when using message objects over the wire.\\nfrom langgraph.graph import StateGraph, START, END\\nfrom pydantic import BaseModel\\nfrom langchain_core.messages import HumanMessage, AIMessage, AnyMessage\\nfrom typing import List\\n\\nclass ChatState(BaseModel):\\n    messages: List[AnyMessage]\\n    context: str\\n\\ndef add_message(state: ChatState):\\n    return {\"messages\": state.messages + [AIMessage(content=\"Hello there!\")]}\\n\\nbuilder = StateGraph(ChatState)\\nbuilder.add_node(\"add_message\", add_message)\\nbuilder.add_edge(START, \"add_message\")\\nbuilder.add_edge(\"add_message\", END)\\ngraph = builder.compile()\\n\\n# Create input with a message\\ninitial_state = ChatState(\\n    messages=[HumanMessage(content=\"Hi\")], context=\"Customer support chat\"\\n)\\n\\nresult = graph.invoke(initial_state)\\nprint(f\"Output: {result}\")\\n\\n# Convert back to Pydantic model to see message types\\noutput_model = ChatState(**result)\\nfor i, msg in enumerate(output_model.messages):\\n    print(f\"Message {i}: {type(msg).__name__} - {msg.content}\")\\n\\n\\nAdd runtime configuration¶\\nSometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, without polluting the graph state with these parameters.\\nTo add runtime configuration:\\n\\nSpecify a schema for your configuration\\nAdd the configuration to the function signature for nodes or conditional edges\\nPass the configuration into the graph.\\n\\nSee below for a simple example:\\nAPI Reference: END | StateGraph | START\\nfrom langgraph.graph import END, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n# 1. Specify config schema\\nclass ContextSchema(TypedDict):\\n    my_runtime_value: str\\n\\n# 2. Define a graph that accesses the config in a node\\nclass State(TypedDict):\\n    my_state_value: str\\n\\ndef node(state: State, runtime: Runtime[ContextSchema]):\\n    if runtime.context[\"my_runtime_value\"] == \"a\":\\n        return {\"my_state_value\": 1}\\n    elif runtime.context[\"my_runtime_value\"] == \"b\":\\n        return {\"my_state_value\": 2}\\n    else:\\n        raise ValueError(\"Unknown values.\")\\n\\nbuilder = StateGraph(State, context_schema=ContextSchema)\\nbuilder.add_node(node)\\nbuilder.add_edge(START, \"node\")\\nbuilder.add_edge(\"node\", END)\\n\\ngraph = builder.compile()\\n\\n# 3. Pass in configuration at runtime:\\nprint(graph.invoke({}, context={\"my_runtime_value\": \"a\"}))\\nprint(graph.invoke({}, context={\"my_runtime_value\": \"b\"}))\\n\\n{\\'my_state_value\\': 1}\\n{\\'my_state_value\\': 2}\\n\\n\\nExtended example: specifying LLM at runtime\\nBelow we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.\\nfrom dataclasses import dataclass\\n\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, END, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n@dataclass\\nclass ContextSchema:\\n    model_provider: str = \"anthropic\"\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\\n    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\\n    model = MODELS[runtime.context.model_provider]\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, context_schema=ContextSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\n# With no configuration, uses default (Anthropic)\\nresponse_1 = graph.invoke({\"messages\": [input_message]})[\"messages\"][-1]\\n# Or, can set OpenAI\\nresponse_2 = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\"})[\"messages\"][-1]\\n\\nprint(response_1.response_metadata[\"model_name\"])\\nprint(response_2.response_metadata[\"model_name\"])\\n\\nclaude-3-5-haiku-20241022\\ngpt-4.1-mini-2025-04-14\\n\\n\\n\\nExtended example: specifying model and system message at runtime\\nBelow we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.\\nfrom dataclasses import dataclass\\nfrom typing import Optional\\nfrom langchain.chat_models import init_chat_model\\nfrom langchain_core.messages import SystemMessage\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.runtime import Runtime\\nfrom typing_extensions import TypedDict\\n\\n@dataclass\\nclass ContextSchema:\\n    model_provider: str = \"anthropic\"\\n    system_message: str | None = None\\n\\nMODELS = {\\n    \"anthropic\": init_chat_model(\"anthropic:claude-3-5-haiku-latest\"),\\n    \"openai\": init_chat_model(\"openai:gpt-4.1-mini\"),\\n}\\n\\ndef call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\\n    model = MODELS[runtime.context.model_provider]\\n    messages = state[\"messages\"]\\n    if (system_message := runtime.context.system_message):\\n        messages = [SystemMessage(system_message)] + messages\\n    response = model.invoke(messages)\\n    return {\"messages\": [response]}\\n\\nbuilder = StateGraph(MessagesState, context_schema=ContextSchema)\\nbuilder.add_node(\"model\", call_model)\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", END)\\n\\ngraph = builder.compile()\\n\\n# Usage\\ninput_message = {\"role\": \"user\", \"content\": \"hi\"}\\nresponse = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\", \"system_message\": \"Respond in Italian.\"})\\nfor message in response[\"messages\"]:\\n    message.pretty_print()\\n\\n================================ Human Message ================================\\n\\nhi\\n================================== Ai Message ==================================\\n\\nCiao! Come posso aiutarti oggi?\\n\\n\\nAdd retry policies¶\\nThere are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes.\\nTo configure a retry policy, pass the retry_policy parameter to the add_node. The retry_policy parameter takes in a RetryPolicy named tuple object. Below we instantiate a RetryPolicy object with the default parameters and associate it with a node:\\nfrom langgraph.pregel import RetryPolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    retry_policy=RetryPolicy(),\\n)\\n\\nBy default, the retry_on parameter uses the default_retry_on function, which retries on any exception except for the following:\\n\\nValueError\\nTypeError\\nArithmeticError\\nImportError\\nLookupError\\nNameError\\nSyntaxError\\nRuntimeError\\nReferenceError\\nStopIteration\\nStopAsyncIteration\\nOSError\\n\\nIn addition, for exceptions from popular http request libraries such as requests and httpx it only retries on 5xx status codes.\\n\\nExtended example: customizing retry policies\\nConsider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes:\\nimport sqlite3\\nfrom typing_extensions import TypedDict\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import END, MessagesState, StateGraph, START\\nfrom langgraph.pregel import RetryPolicy\\nfrom langchain_community.utilities import SQLDatabase\\nfrom langchain_core.messages import AIMessage\\n\\ndb = SQLDatabase.from_uri(\"sqlite:///:memory:\")\\nmodel = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\\n\\ndef query_database(state: MessagesState):\\n    query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\\n    return {\"messages\": [AIMessage(content=query_result)]}\\n\\ndef call_model(state: MessagesState):\\n    response = model.invoke(state[\"messages\"])\\n    return {\"messages\": [response]}\\n\\n# Define a new graph\\nbuilder = StateGraph(MessagesState)\\nbuilder.add_node(\\n    \"query_database\",\\n    query_database,\\n    retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),\\n)\\nbuilder.add_node(\"model\", call_model, retry_policy=RetryPolicy(max_attempts=5))\\nbuilder.add_edge(START, \"model\")\\nbuilder.add_edge(\"model\", \"query_database\")\\nbuilder.add_edge(\"query_database\", END)\\ngraph = builder.compile()\\n\\n\\nAdd node caching¶\\nNode caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph.\\nTo configure a cache policy, pass the cache_policy parameter to the add_node function. In the following example, a CachePolicy object is instantiated with a time to live of 120 seconds and the default key_func generator. Then it is associated with a node:\\nfrom langgraph.types import CachePolicy\\n\\nbuilder.add_node(\\n    \"node_name\",\\n    node_function,\\n    cache_policy=CachePolicy(ttl=120),\\n)\\n\\nThen, to enable node-level caching for a graph, set the cache argument when compiling the graph. The example below uses InMemoryCache to set up a graph with in-memory cache, but SqliteCache is also available.\\nfrom langgraph.cache.memory import InMemoryCache\\n\\ngraph = builder.compile(cache=InMemoryCache())\\n\\nCreate a sequence of steps¶\\n\\nPrerequisites\\nThis guide assumes familiarity with the above section on state.\\n\\nHere we demonstrate how to construct a simple sequence of steps. We will show:\\n\\nHow to build a sequential graph\\nBuilt-in short-hand for constructing similar graphs.\\n\\nTo add a sequence of nodes, we use the .add_node and .add_edge methods of our graph:\\nAPI Reference: START | StateGraph\\nfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\nWe can also use the built-in shorthand .add_sequence:\\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\n\\nWhy split application steps into a sequence with LangGraph?\\nLangGraph makes it easy to add an underlying persistence layer to your application.\\nThis allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern:\\n\\nHow state updates are checkpointed\\nHow interruptions are resumed in human-in-the-loop workflows\\nHow we can \"rewind\" and branch-off executions using LangGraph\\'s time travel features\\n\\nThey also determine how execution steps are streamed, and how your application is visualized\\nand debugged using LangGraph Studio.\\n\\nLet\\'s demonstrate an end-to-end example. We will create a sequence of three steps:\\n\\nPopulate a value in a key of the state\\nUpdate the same value\\nPopulate a different value\\n\\nLet\\'s first define our state. This governs the schema of the graph, and can also specify how to apply updates. See this section for more detail.\\nIn our case, we will just keep track of two values:\\nfrom typing_extensions import TypedDict\\n\\nclass State(TypedDict):\\n    value_1: str\\n    value_2: int\\n\\nOur nodes are just Python functions that read our graph\\'s state and make updates to it. The first argument to this function will always be the state:\\ndef step_1(state: State):\\n    return {\"value_1\": \"a\"}\\n\\ndef step_2(state: State):\\n    current_value_1 = state[\"value_1\"]\\n    return {\"value_1\": f\"{current_value_1} b\"}\\n\\ndef step_3(state: State):\\n    return {\"value_2\": 10}\\n\\n\\nNote\\nNote that when issuing updates to the state, each node can just specify the value of the key it wishes to update.\\nBy default, this will overwrite the value of the corresponding key. You can also use reducers to control how updates are processed— for example, you can append successive updates to a key instead. See this section for more detail.\\n\\nFinally, we define the graph. We use StateGraph to define a graph that operates on this state.\\nWe will then use add_node and add_edge to populate our graph and define its control flow.\\nAPI Reference: START | StateGraph\\nfrom langgraph.graph import START, StateGraph\\n\\nbuilder = StateGraph(State)\\n\\n# Add nodes\\nbuilder.add_node(step_1)\\nbuilder.add_node(step_2)\\nbuilder.add_node(step_3)\\n\\n# Add edges\\nbuilder.add_edge(START, \"step_1\")\\nbuilder.add_edge(\"step_1\", \"step_2\")\\nbuilder.add_edge(\"step_2\", \"step_3\")\\n\\n\\nSpecifying custom names\\nYou can specify custom names for nodes using .add_node:\\nbuilder.add_node(\"my_node\", step_1)\\n\\n\\nNote that:\\n\\n.add_edge takes the names of nodes, which for functions defaults to node.__name__.\\nWe must specify the entry point of the graph. For this we add an edge with the START node.\\nThe graph halts when there are no more nodes to execute.\\n\\nWe next compile our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a checkpointer, it would also be passed in here.\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our sequence. See this guide for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nLet\\'s proceed with a simple invocation:\\ngraph.invoke({\"value_1\": \"c\"})\\n\\n{\\'value_1\\': \\'a b\\', \\'value_2\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.\\nThe value we passed in was overwritten by the first node.\\nThe second node updated the value.\\nThe third node populated a different value.\\n\\n\\nBuilt-in shorthand\\nlanggraph>=0.2.46 includes a built-in short-hand add_sequence for adding node sequences. You can compile the same graph as follows:\\nbuilder = StateGraph(State).add_sequence([step_1, step_2, step_3])\\nbuilder.add_edge(START, \"step_1\")\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"value_1\": \"c\"})    \\n\\n\\nCreate branches¶\\nParallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and conditional_edges. Below are some examples showing how to add create branching dataflows that work for you.\\nRun graph nodes in parallel¶\\nIn this example, we fan out from Node A to B and C and then fan in to D. With our state, we specify the reducer add operation. This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on state reducers for more detail on updating state with reducers.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nWith the reducer, you can see that the values added in each node are accumulated.\\ngraph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\\n\\nAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\']\\n\\n\\nNote\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. Because they are in the same step, node \"d\" executes after both \"b\" and \"c\" are finished.\\nImportantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.\\n\\n\\nException handling?\\nLangGraph executes nodes within supersteps, meaning that while parallel branches are executed in parallel, the entire superstep is transactional. If any of these branches raises an exception, none of the updates are applied to the state (the entire superstep errors).\\nImportantly, when using a checkpointer, results from successful nodes within a superstep are saved, and don\\'t repeat when resumed.\\nIf you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this:\\n\\nYou can write regular python code within your node to catch and handle exceptions.\\nYou can set a retry_policy to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn\\'t worry about performing redundant work.\\n\\nTogether, these let you perform parallel execution and fully control exception handling.\\n\\nDefer node execution¶\\nDeferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows.\\nThe above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let\\'s add a node \"b_2\" in the \"b\" branch:\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Any\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef b_2(state: State):\\n    print(f\\'Adding \"B_2\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B_2\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Adding \"D\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(b_2)\\nbuilder.add_node(c)\\nbuilder.add_node(d, defer=True)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"a\", \"b\")\\nbuilder.add_edge(\"a\", \"c\")\\nbuilder.add_edge(\"b\", \"b_2\")\\nbuilder.add_edge(\"b_2\", \"d\")\\nbuilder.add_edge(\"c\", \"d\")\\nbuilder.add_edge(\"d\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\ngraph.invoke({\"aggregate\": []})\\n\\nAdding \"A\" to []\\nAdding \"B\" to [\\'A\\']\\nAdding \"C\" to [\\'A\\']\\nAdding \"B_2\" to [\\'A\\', \\'B\\', \\'C\\']\\nAdding \"D\" to [\\'A\\', \\'B\\', \\'C\\', \\'B_2\\']\\n\\nIn the above example, nodes \"b\" and \"c\" are executed concurrently in the same superstep. We set defer=True on node d so it will not execute until all pending tasks are finished. In this case, this means that \"d\" waits to execute until the entire \"b\" branch is finished.\\nConditional branching¶\\nIf your fan-out should vary at runtime based on the state, you can use add_conditional_edges to select one or more paths using the graph state. See example below, where node a generates a state update that determines the following node.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Literal, Sequence\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    # Add a key to the state. We will set this key to determine\\n    # how we branch.\\n    which: str\\n\\ndef a(state: State):\\n    print(f\\'Adding \"A\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"], \"which\": \"c\"}\\n\\ndef b(state: State):\\n    print(f\\'Adding \"B\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Adding \"C\" to {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_edge(\"b\", END)\\nbuilder.add_edge(\"c\", END)\\n\\ndef conditional_edge(state: State) -> Literal[\"b\", \"c\"]:\\n    # Fill in arbitrary logic here that uses the state\\n    # to determine the next node\\n    return state[\"which\"]\\n\\nbuilder.add_conditional_edges(\"a\", conditional_edge)\\n\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nresult = graph.invoke({\"aggregate\": []})\\nprint(result)\\n\\nAdding \"A\" to []\\nAdding \"C\" to [\\'A\\']\\n{\\'aggregate\\': [\\'A\\', \\'C\\'], \\'which\\': \\'c\\'}\\n\\n\\nTip\\nYour conditional edges can route to multiple destination nodes. For example:\\ndef route_bc_or_cd(state: State) -> Sequence[str]:\\n    if state[\"which\"] == \"cd\":\\n        return [\"c\", \"d\"]\\n    return [\"b\", \"c\"]\\n\\n\\nMap-Reduce and the Send API¶\\nLangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it:\\nAPI Reference: StateGraph | START | END | Send\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.types import Send\\nfrom typing_extensions import TypedDict, Annotated\\nimport operator\\n\\nclass OverallState(TypedDict):\\n    topic: str\\n    subjects: list[str]\\n    jokes: Annotated[list[str], operator.add]\\n    best_selected_joke: str\\n\\ndef generate_topics(state: OverallState):\\n    return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]}\\n\\ndef generate_joke(state: OverallState):\\n    joke_map = {\\n        \"lions\": \"Why don\\'t lions like fast food? Because they can\\'t catch it!\",\\n        \"elephants\": \"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\",\\n        \"penguins\": \"Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\"\\n    }\\n    return {\"jokes\": [joke_map[state[\"subject\"]]]}\\n\\ndef continue_to_jokes(state: OverallState):\\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\\n\\ndef best_joke(state: OverallState):\\n    return {\"best_selected_joke\": \"penguins\"}\\n\\nbuilder = StateGraph(OverallState)\\nbuilder.add_node(\"generate_topics\", generate_topics)\\nbuilder.add_node(\"generate_joke\", generate_joke)\\nbuilder.add_node(\"best_joke\", best_joke)\\nbuilder.add_edge(START, \"generate_topics\")\\nbuilder.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\\nbuilder.add_edge(\"generate_joke\", \"best_joke\")\\nbuilder.add_edge(\"best_joke\", END)\\nbuilder.add_edge(\"generate_topics\", END)\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\n# Call the graph: here we call it to generate a list of jokes\\nfor step in graph.stream({\"topic\": \"animals\"}):\\n    print(step)\\n\\n{\\'generate_topics\\': {\\'subjects\\': [\\'lions\\', \\'elephants\\', \\'penguins\\']}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t lions like fast food? Because they can\\'t catch it!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\"Why don\\'t elephants use computers? They\\'re afraid of the mouse!\"]}}\\n{\\'generate_joke\\': {\\'jokes\\': [\\'Why don\\'t penguins like talking to strangers at parties? Because they find it hard to break the ice.\\']}}\\n{\\'best_joke\\': {\\'best_selected_joke\\': \\'penguins\\'}}\\n\\nCreate and control loops¶\\nWhen creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a conditional edge that routes to the END node once we reach some termination condition.\\nYou can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of supersteps that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits here.\\nLet\\'s consider a simple graph with a loop to better understand how these mechanisms work.\\n\\nTip\\nTo return the last value of your state instead of receiving a recursion limit error, see the next section.\\n\\nWhen creating a loop, you can include a conditional edge that specifies a termination condition:\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if termination_condition(state):\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nTo control the recursion limit, specify \"recursion_limit\" in the config. This will raise a GraphRecursionError, which you can catch and handle:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke(inputs, {\"recursion_limit\": 3})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nLet\\'s define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition.\\nAPI Reference: StateGraph | START | END\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    # The operator.add reducer fn makes this append-only\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis architecture is similar to a ReAct agent in which node \"a\" is a tool-calling model, and node \"b\" represents the tools.\\nIn our route conditional edge, we specify that we should end after the \"aggregate\" list in the state passes a threshold length.\\nInvoking the graph, we see that we alternate between nodes \"a\" and \"b\" before terminating once we reach the termination condition.\\ngraph.invoke({\"aggregate\": []})\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\']\\nNode B sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\']\\nNode A sees [\\'A\\', \\'B\\', \\'A\\', \\'B\\', \\'A\\', \\'B\\']\\n\\nImpose a recursion limit¶\\nIn some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph\\'s recursion limit. This will raise a GraphRecursionError after a given number of supersteps. We can then catch and handle this exception:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\nExtended example: return state on hitting recursion limit\\nInstead of raising GraphRecursionError, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run.\\nLangGraph implements a special RemainingSteps annotation. Under the hood, it creates a ManagedValue channel -- a state channel that will exist for the duration of our graph run and no longer.\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.managed.is_last_step import RemainingSteps\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n    remaining_steps: RemainingSteps\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if state[\"remaining_steps\"] <= 2:\\n        return END\\n    else:\\n        return \"b\"\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"a\")\\ngraph = builder.compile()\\n\\n# Test it out\\nresult = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nprint(result)\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode A sees [\\'A\\', \\'B\\']\\n{\\'aggregate\\': [\\'A\\', \\'B\\', \\'A\\']}\\n\\n\\n\\nExtended example: loops with branches\\nTo better understand how the recursion limit works, let\\'s consider a more complex example. Below we implement a loop, but one step fans out into two nodes:\\nimport operator\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\n\\nclass State(TypedDict):\\n    aggregate: Annotated[list, operator.add]\\n\\ndef a(state: State):\\n    print(f\\'Node A sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"A\"]}\\n\\ndef b(state: State):\\n    print(f\\'Node B sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"B\"]}\\n\\ndef c(state: State):\\n    print(f\\'Node C sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"C\"]}\\n\\ndef d(state: State):\\n    print(f\\'Node D sees {state[\"aggregate\"]}\\')\\n    return {\"aggregate\": [\"D\"]}\\n\\n# Define nodes\\nbuilder = StateGraph(State)\\nbuilder.add_node(a)\\nbuilder.add_node(b)\\nbuilder.add_node(c)\\nbuilder.add_node(d)\\n\\n# Define edges\\ndef route(state: State) -> Literal[\"b\", END]:\\n    if len(state[\"aggregate\"]) < 7:\\n        return \"b\"\\n    else:\\n        return END\\n\\nbuilder.add_edge(START, \"a\")\\nbuilder.add_conditional_edges(\"a\", route)\\nbuilder.add_edge(\"b\", \"c\")\\nbuilder.add_edge(\"b\", \"d\")\\nbuilder.add_edge([\"c\", \"d\"], \"a\")\\ngraph = builder.compile()\\n\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nThis graph looks complex, but can be conceptualized as loop of supersteps:\\n\\nNode A\\nNode B\\nNodes C and D\\nNode A\\n...\\n\\nWe have a loop of four supersteps, where nodes C and D are executed concurrently.\\nInvoking the graph as before, we see that we complete two full \"laps\" before hitting the termination condition:\\nresult = graph.invoke({\"aggregate\": []})\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nNode B sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\']\\nNode D sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode C sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\', \\'A\\', \\'B\\', \\'C\\', \\'D\\']\\n\\nHowever, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps:\\nfrom langgraph.errors import GraphRecursionError\\n\\ntry:\\n    result = graph.invoke({\"aggregate\": []}, {\"recursion_limit\": 4})\\nexcept GraphRecursionError:\\n    print(\"Recursion Error\")\\n\\nNode A sees []\\nNode B sees [\\'A\\']\\nNode C sees [\\'A\\', \\'B\\']\\nNode D sees [\\'A\\', \\'B\\']\\nNode A sees [\\'A\\', \\'B\\', \\'C\\', \\'D\\']\\nRecursion Error\\n\\n\\nAsync¶\\nUsing the async programming paradigm can produce significant performance improvements when running IO-bound code concurrently (e.g., making concurrent API requests to a chat model provider).\\nTo convert a sync implementation of the graph to an async implementation, you will need to:\\n\\nUpdate nodes use async def instead of def.\\nUpdate the code inside to use await appropriately.\\nInvoke the graph with .ainvoke or .astream as desired.\\n\\nBecause many LangChain objects implement the Runnable Protocol which has async variants of all the sync methods it\\'s typically fairly quick to upgrade a sync graph to an async graph.\\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\\nOpenAIAnthropicAzureGoogle GeminiAWS Bedrock\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"openai:gpt-4.1\")\\n\\n👉 Read the OpenAI integration docs\\n\\n\\npip install -U \"langchain[anthropic]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\\n\\n👉 Read the Anthropic integration docs\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\\n\\nllm = init_chat_model(\\n    \"azure_openai:gpt-4.1\",\\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\\n)\\n\\n👉 Read the Azure integration docs\\n\\n\\npip install -U \"langchain[google-genai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\\n\\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\\n\\n👉 Read the Google GenAI integration docs\\n\\n\\npip install -U \"langchain[aws]\"\\n\\nfrom langchain.chat_models import init_chat_model\\n\\n# Follow the steps here to configure your credentials:\\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\\n\\nllm = init_chat_model(\\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\\n    model_provider=\"bedrock_converse\",\\n)\\n\\n👉 Read the AWS Bedrock integration docs\\n\\n\\n\\nAPI Reference: init_chat_model | StateGraph\\nfrom langchain.chat_models import init_chat_model\\nfrom langgraph.graph import MessagesState, StateGraph\\n\\nasync def node(state: MessagesState): # (1)!\\n    new_message = await llm.ainvoke(state[\"messages\"]) # (2)!\\n    return {\"messages\": [new_message]}\\n\\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\\nresult = await graph.ainvoke({\"messages\": [input_message]}) # (3)!\\n\\n\\nDeclare nodes to be async functions.\\nUse async invocations when available within the node.\\nUse async invocations on the graph object itself.\\n\\n\\nAsync streaming\\nSee the streaming guide for examples of streaming with async.\\n\\nCombine control flow and state updates with Command¶\\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a Command object from node functions:\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        # state update\\n        update={\"foo\": \"bar\"},\\n        # control flow\\n        goto=\"my_other_node\"\\n    )\\n\\nWe show an end-to-end example below. Let\\'s create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A.\\nAPI Reference: StateGraph | START | Command\\nimport random\\nfrom typing_extensions import TypedDict, Literal\\nfrom langgraph.graph import StateGraph, START\\nfrom langgraph.types import Command\\n\\n# Define graph state\\nclass State(TypedDict):\\n    foo: str\\n\\n# Define the nodes\\n\\ndef node_a(state: State) -> Command[Literal[\"node_b\", \"node_c\"]]:\\n    print(\"Called A\")\\n    value = random.choice([\"b\", \"c\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"b\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        # this is the state update\\n        update={\"foo\": value},\\n        # this is a replacement for an edge\\n        goto=goto,\\n    )\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    return {\"foo\": state[\"foo\"] + \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": state[\"foo\"] + \"c\"}\\n\\nWe can now create the StateGraph with the above nodes. Notice that the graph doesn\\'t have conditional edges for routing! This is because control flow is defined with Command inside node_a.\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"node_a\")\\nbuilder.add_node(node_a)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n# NOTE: there are no edges between nodes A, B and C!\\n\\ngraph = builder.compile()\\n\\n\\nImportant\\nYou might have noticed that we used Command as a return type annotation, e.g. Command[Literal[\"node_b\", \"node_c\"]]. This is necessary for the graph rendering and tells LangGraph that node_a can navigate to node_b and node_c.\\n\\nfrom IPython.display import display, Image\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIf we run the graph multiple times, we\\'d see it take different paths (A -> B or A -> C) based on the random choice in node A.\\ngraph.invoke({\"foo\": \"\"})\\n\\nCalled A\\nCalled C\\n\\nNavigate to a node in a parent graph¶\\nIf you are using subgraphs, you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify graph=Command.PARENT in Command:\\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\\n    return Command(\\n        update={\"foo\": \"bar\"},\\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\\n        graph=Command.PARENT\\n    )\\n\\nLet\\'s demonstrate this using the above example. We\\'ll do so by changing node_a in the above example into a single-node graph that we\\'ll add as a subgraph to our parent graph.\\n\\nState updates with Command.PARENT\\nWhen you send updates from a subgraph node to a parent graph node for a key that\\'s shared by both parent and subgraph state schemas, you must define a reducer for the key you\\'re updating in the parent graph state. See the example below.\\n\\nimport operator\\nfrom typing_extensions import Annotated\\n\\nclass State(TypedDict):\\n    # NOTE: we define a reducer here\\n    foo: Annotated[str, operator.add]\\n\\ndef node_a(state: State):\\n    print(\"Called A\")\\n    value = random.choice([\"a\", \"b\"])\\n    # this is a replacement for a conditional edge function\\n    if value == \"a\":\\n        goto = \"node_b\"\\n    else:\\n        goto = \"node_c\"\\n\\n    # note how Command allows you to BOTH update the graph state AND route to the next node\\n    return Command(\\n        update={\"foo\": value},\\n        goto=goto,\\n        # this tells LangGraph to navigate to node_b or node_c in the parent graph\\n        # NOTE: this will navigate to the closest parent graph relative to the subgraph\\n        graph=Command.PARENT,\\n    )\\n\\nsubgraph = StateGraph(State).add_node(node_a).add_edge(START, \"node_a\").compile()\\n\\ndef node_b(state: State):\\n    print(\"Called B\")\\n    # NOTE: since we\\'ve defined a reducer, we don\\'t need to manually append\\n    # new characters to existing \\'foo\\' value. instead, reducer will append these\\n    # automatically (via operator.add)\\n    return {\"foo\": \"b\"}\\n\\ndef node_c(state: State):\\n    print(\"Called C\")\\n    return {\"foo\": \"c\"}\\n\\nbuilder = StateGraph(State)\\nbuilder.add_edge(START, \"subgraph\")\\nbuilder.add_node(\"subgraph\", subgraph)\\nbuilder.add_node(node_b)\\nbuilder.add_node(node_c)\\n\\ngraph = builder.compile()\\n\\ngraph.invoke({\"foo\": \"\"})\\n\\nCalled A\\nCalled C\\n\\nUse inside tools¶\\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return Command(update={\"my_custom_key\": \"foo\", \"messages\": [...]}) from the tool:\\n@tool\\ndef lookup_user_info(tool_call_id: Annotated[str, InjectedToolCallId], config: RunnableConfig):\\n    \"\"\"Use this to look up user information to better assist them with their questions.\"\"\"\\n    user_info = get_user_info(config.get(\"configurable\", {}).get(\"user_id\"))\\n    return Command(\\n        update={\\n            # update the state keys\\n            \"user_info\": user_info,\\n            # update the message history\\n            \"messages\": [ToolMessage(\"Successfully looked up user information\", tool_call_id=tool_call_id)]\\n        }\\n    )\\n\\n\\nImportant\\nYou MUST include messages (or any state key used for the message history) in Command.update when returning Command from a tool and the list of messages in messages MUST contain a ToolMessage. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).\\n\\nIf you are using tools that update state via Command, we recommend using prebuilt ToolNode which automatically handles tools returning Command objects and propagates them to the graph state. If you\\'re writing a custom node that calls tools, you would need to manually propagate Command objects returned by the tools as the update from the node.\\nVisualize your graph¶\\nHere we demonstrate how to visualize the graphs you create.\\nYou can visualize any arbitrary Graph, including StateGraph. Let\\'s have some fun by drawing fractals :).\\nAPI Reference: StateGraph | START | END | add_messages\\nimport random\\nfrom typing import Annotated, Literal\\nfrom typing_extensions import TypedDict\\nfrom langgraph.graph import StateGraph, START, END\\nfrom langgraph.graph.message import add_messages\\n\\nclass State(TypedDict):\\n    messages: Annotated[list, add_messages]\\n\\nclass MyNode:\\n    def __init__(self, name: str):\\n        self.name = name\\n    def __call__(self, state: State):\\n        return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]}\\n\\ndef route(state) -> Literal[\"entry_node\", \"__end__\"]:\\n    if len(state[\"messages\"]) > 10:\\n        return \"__end__\"\\n    return \"entry_node\"\\n\\ndef add_fractal_nodes(builder, current_node, level, max_level):\\n    if level > max_level:\\n        return\\n    # Number of nodes to create at this level\\n    num_nodes = random.randint(1, 3)  # Adjust randomness as needed\\n    for i in range(num_nodes):\\n        nm = [\"A\", \"B\", \"C\"][i]\\n        node_name = f\"node_{current_node}_{nm}\"\\n        builder.add_node(node_name, MyNode(node_name))\\n        builder.add_edge(current_node, node_name)\\n        # Recursively add more nodes\\n        r = random.random()\\n        if r > 0.2 and level + 1 < max_level:\\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\\n        elif r > 0.05:\\n            builder.add_conditional_edges(node_name, route, node_name)\\n        else:\\n            # End\\n            builder.add_edge(node_name, \"__end__\")\\n\\ndef build_fractal_graph(max_level: int):\\n    builder = StateGraph(State)\\n    entry_point = \"entry_node\"\\n    builder.add_node(entry_point, MyNode(entry_point))\\n    builder.add_edge(START, entry_point)\\n    add_fractal_nodes(builder, entry_point, 1, max_level)\\n    # Optional: set a finish point if required\\n    builder.add_edge(entry_point, END)  # or any specific node\\n    return builder.compile()\\n\\napp = build_fractal_graph(3)\\n\\nMermaid¶\\nWe can also convert a graph class into Mermaid syntax.\\nprint(app.get_graph().draw_mermaid())\\n\\n%%{init: {\\'flowchart\\': {\\'curve\\': \\'linear\\'}}}%%\\ngraph TD;\\n    __start__([<p>__start__</p>]):::first\\n    entry_node(entry_node)\\n    node_entry_node_A(node_entry_node_A)\\n    node_entry_node_B(node_entry_node_B)\\n    node_node_entry_node_B_A(node_node_entry_node_B_A)\\n    node_node_entry_node_B_B(node_node_entry_node_B_B)\\n    node_node_entry_node_B_C(node_node_entry_node_B_C)\\n    __end__([<p>__end__</p>]):::last\\n    __start__ --> entry_node;\\n    entry_node --> __end__;\\n    entry_node --> node_entry_node_A;\\n    entry_node --> node_entry_node_B;\\n    node_entry_node_B --> node_node_entry_node_B_A;\\n    node_entry_node_B --> node_node_entry_node_B_B;\\n    node_entry_node_B --> node_node_entry_node_B_C;\\n    node_entry_node_A -.-> entry_node;\\n    node_entry_node_A -.-> __end__;\\n    node_node_entry_node_B_A -.-> entry_node;\\n    node_node_entry_node_B_A -.-> __end__;\\n    node_node_entry_node_B_B -.-> entry_node;\\n    node_node_entry_node_B_B -.-> __end__;\\n    node_node_entry_node_B_C -.-> entry_node;\\n    node_node_entry_node_B_C -.-> __end__;\\n    classDef default fill:#f2f0ff,line-height:1.2\\n    classDef first fill-opacity:0\\n    classDef last fill:#bfb6fc\\n\\nPNG¶\\nIf preferred, we could render the Graph into a  .png. Here we could use three options:\\n\\nUsing Mermaid.ink API (does not require additional packages)\\nUsing Mermaid + Pyppeteer (requires pip install pyppeteer)\\nUsing graphviz (which requires pip install graphviz)\\n\\nUsing Mermaid.Ink\\nBy default, draw_mermaid_png() uses Mermaid.Ink\\'s API to generate the diagram.\\nAPI Reference: CurveStyle | MermaidDrawMethod | NodeStyles\\nfrom IPython.display import Image, display\\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\\n\\ndisplay(Image(app.get_graph().draw_mermaid_png()))\\n\\n\\nUsing Mermaid + Pyppeteer\\nimport nest_asyncio\\n\\nnest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\\n\\ndisplay(\\n    Image(\\n        app.get_graph().draw_mermaid_png(\\n            curve_style=CurveStyle.LINEAR,\\n            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\\n            wrap_label_n_words=9,\\n            output_file_path=None,\\n            draw_method=MermaidDrawMethod.PYPPETEER,\\n            background_color=\"white\",\\n            padding=10,\\n        )\\n    )\\n)\\n\\nUsing Graphviz\\ntry:\\n    display(Image(app.get_graph().draw_png()))\\nexcept ImportError:\\n    print(\\n        \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\"\\n    )\\n\\n\\n\\n\\n\\n\\n\\n\\n  Back to top\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                Previous\\n              \\n\\n                Overview\\n              \\n\\n\\n\\n\\n\\n                Next\\n              \\n\\n                Overview\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list = [item for sublist in docs for item in sublist]\n",
    "doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30e4edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RCTS(chunk_size = 1000, chunk_overlap= 100)\n",
    "docs_split = splitter.split_documents(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5599eb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore_langchain = FAISS.from_documents(docs_split, MistralAIEmbeddings())\n",
    "langchain_retriever = vectorstore_langchain.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "622c10ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever_vector_langchain_blog', description='Search and run information about LangChain', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000251FF47FF60>, retriever=VectorStoreRetriever(tags=['FAISS', 'MistralAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000251C5BB9490>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000251C6F91080>, retriever=VectorStoreRetriever(tags=['FAISS', 'MistralAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000251C5BB9490>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retriever to retriever tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "langchain_retriever_tool = create_retriever_tool(langchain_retriever, name = \"retriever_vector_langchain_blog\", description = \"Search and run information about LangChain\")\n",
    "langchain_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4d22e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [langchain_retriever_tool, retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4c9e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the agentic workflow\n",
    "\n",
    "from typing_extensions import Annotated, TypedDict, Sequence\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages \n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d43cba2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854989bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invoke the agent model to generate a response based on the current state. Given the question,\n",
    "    it will decide to retrieve using the retriever tool, or simply end.\n",
    "    \n",
    "    Args:\n",
    "        state(messages): The current state\n",
    "        \n",
    "    Returns:\n",
    "        dict: updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL_AGENT---\")\n",
    "    print(\"state messages: \",state[\"messages\"])\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatGroq(model = \"qwen/qwen3-32b\")\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # print(\"model with tools response: \",response)\n",
    "    return {\"messages\":[response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4abd0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing_extensions import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain import hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4406a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state) -> Literal[\"rewrite\", \"generate\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "    \n",
    "    Args:\n",
    "        state[\"messages\"]: The current state\n",
    "        \n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "    print(\"---CHECK_REVELANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check\"\"\"\n",
    "        binary_score: str = Field(description = \"Relevance score 'yes' or 'no'\")\n",
    "    \n",
    "    # LLM\n",
    "    model = ChatGroq(model = \"qwen/qwen3-32b\")\n",
    "\n",
    "    # LLM with tools and validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template= \n",
    "        \"\"\"\n",
    "        You are a grader assessing relevance of a retrieved document to a user question.\\n\n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: \\n\\n {question} \\n\\n\n",
    "        If the document keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score \"yes\" or \"no\" to indicate whether the document is relevant to the question.\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    question = messages[0].content\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\":question, \"context\":docs})\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DOCS_RELEVANT---\")\n",
    "        return \"generate\"\n",
    "    else: \n",
    "        print(\"---DOCS_IRRELEVANT---\")\n",
    "        return \"rewrite\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58c3662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate Answer\n",
    "\n",
    "    Args:\n",
    "        state(messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated message   \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    prompt = PromptTemplate(template=\"\"\"\n",
    "    Use the following context to answer the question.\\n\\n\n",
    "\n",
    "        Context: \\n\n",
    "        {context} \\n\\n\n",
    "\n",
    "        Question:\\n\n",
    "        {question}\\n\\n\n",
    "                            \"\"\")\n",
    "\n",
    "    llm = ChatGroq(model= \"qwen/qwen3-32b\")\n",
    "    rag_chain = prompt|llm|StrOutputParser()\n",
    "\n",
    "    response = rag_chain.invoke({\"context\":docs, \"question\":question})\n",
    "    return {\"messages\":[response]}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde513c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to perform a better question\n",
    "\n",
    "    Args:\n",
    "        state(messages): the current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM_QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(content = f\"\"\" \n",
    "            Look at the input and try to reason about the underlying semantic intent or meaning.\\n\n",
    "            here is the initial question:\\n\n",
    "            {question} \\n\n",
    "            formulate the improved question:\"\"\")\n",
    "    ]\n",
    "\n",
    "    llm = ChatGroq(model= \"qwen/qwen3-32b\")\n",
    "    response = llm.invoke(msg)\n",
    "    # print(\"rewrite response: \",response)\n",
    "    return {\"message\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c34490ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode(tools)\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\"generate\", generate)  \n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\"agent\",tools_condition,{\"tools\": \"retrieve\",END: END})\n",
    "workflow.add_conditional_edges(\"retrieve\",grade_documents)\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd2fd766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL_AGENT---\n",
      "state messages:  [HumanMessage(content='What is Langgraph?', additional_kwargs={}, response_metadata={}, id='cec0b0f4-0d9f-4d1f-aba5-52e3ea19f2c6')]\n",
      "model with tools response:  content='' additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is Langgraph?\" First, I need to figure out which tool to use here. There are two retrievers available: one for the LangChain blog and another for the DB blog. Since Langgraph is likely related to LangChain, given the name similarity, I should use the retriever_vector_langchain_blog function. The query parameter should be \"Langgraph\" to look up relevant information. I\\'ll structure the tool call with that query.\\n', 'tool_calls': [{'id': 'gwpa8a7yr', 'function': {'arguments': '{\"query\":\"Langgraph\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 218, 'total_tokens': 346, 'completion_time': 0.258335279, 'prompt_time': 0.008364519, 'queue_time': 0.051548035, 'total_time': 0.266699798}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_f17c2eb555', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--4d595e6a-61fd-4698-aa9f-b18d984bad0b-0' tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'Langgraph'}, 'id': 'gwpa8a7yr', 'type': 'tool_call'}] usage_metadata={'input_tokens': 218, 'output_tokens': 128, 'total_tokens': 346}\n",
      "---CHECK_REVELANCE---\n",
      "---DOCS_RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Langgraph?', additional_kwargs={}, response_metadata={}, id='cec0b0f4-0d9f-4d1f-aba5-52e3ea19f2c6'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is Langgraph?\" First, I need to figure out which tool to use here. There are two retrievers available: one for the LangChain blog and another for the DB blog. Since Langgraph is likely related to LangChain, given the name similarity, I should use the retriever_vector_langchain_blog function. The query parameter should be \"Langgraph\" to look up relevant information. I\\'ll structure the tool call with that query.\\n', 'tool_calls': [{'id': 'gwpa8a7yr', 'function': {'arguments': '{\"query\":\"Langgraph\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 128, 'prompt_tokens': 218, 'total_tokens': 346, 'completion_time': 0.258335279, 'prompt_time': 0.008364519, 'queue_time': 0.051548035, 'total_time': 0.266699798}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_f17c2eb555', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--4d595e6a-61fd-4698-aa9f-b18d984bad0b-0', tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'Langgraph'}, 'id': 'gwpa8a7yr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 218, 'output_tokens': 128, 'total_tokens': 346}),\n",
       "  ToolMessage(content='Table of contents\\n    \\n\\n\\n\\n\\n      Learn LangGraph basics\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOverview¶\\nLangGraph is built for developers who want to build powerful, adaptable AI agents. Developers choose LangGraph for:\\n\\nReliability and controllability. Steer agent actions with moderation checks and human-in-the-loop approvals. LangGraph persists context for long-running workflows, keeping your agents on course.\\nLow-level and extensible. Build custom agents with fully descriptive, low-level primitives free from rigid abstractions that limit customization. Design scalable multi-agent systems, with each agent serving a specific role tailored to your use case.\\nFirst-class streaming support. With token-by-token streaming and streaming of intermediate steps, LangGraph gives users clear visibility into agent reasoning and actions as they unfold in real time.\\n\\nVisualize your graph\\n    \\n\\n\\n\\n\\n\\n\\n      Mermaid\\n    \\n\\n\\n\\n\\n\\n      PNG\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow to use the graph API¶\\nThis guide demonstrates the basics of LangGraph\\'s Graph API. It walks through state, as well as composing common graph structures such as sequences, branches, and loops. It also covers LangGraph\\'s control features, including the Send API for map-reduce workflows and the Command API for combining state updates with \"hops\" across nodes.\\nSetup¶\\nInstall langgraph:\\npip install -U langgraph\\n\\n\\nSet up LangSmith for better debugging\\nSign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph — read more about how to get started in the docs.\\n\\nDefine and update state¶\\nHere we show how to define and update state in LangGraph. We will demonstrate:\\n\\nImportant\\nNodes should return updates to the state directly, instead of mutating the state.\\n\\nLet\\'s next define a simple graph containing this node. We use StateGraph to define a graph that operates on this state. We then use add_node populate our graph.\\nAPI Reference: StateGraph\\nfrom langgraph.graph import StateGraph\\n\\nbuilder = StateGraph(State)\\nbuilder.add_node(node)\\nbuilder.set_entry_point(\"node\")\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our graph. See this section for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nIn this case, our graph just executes a single node. Let\\'s proceed with a simple invocation:\\nAPI Reference: HumanMessage\\nfrom langchain_core.messages import HumanMessage\\n\\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\\nresult\\n\\n{\\'messages\\': [HumanMessage(content=\\'Hi\\'), AIMessage(content=\\'Hello!\\')], \\'extra_field\\': 10}\\n\\nWe next compile our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a checkpointer, it would also be passed in here.\\ngraph = builder.compile()\\n\\nLangGraph provides built-in utilities for visualizing your graph. Let\\'s inspect our sequence. See this guide for detail on visualization.\\nfrom IPython.display import Image, display\\n\\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\\n\\n\\nLet\\'s proceed with a simple invocation:\\ngraph.invoke({\"value_1\": \"c\"})\\n\\n{\\'value_1\\': \\'a b\\', \\'value_2\\': 10}\\n\\nNote that:\\n\\nWe kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key.\\nThe value we passed in was overwritten by the first node.\\nThe second node updated the value.\\nThe third node populated a different value.', name='retriever_vector_langchain_blog', id='528df18e-fd23-4a31-ac91-e055a0c3046b', tool_call_id='gwpa8a7yr'),\n",
       "  HumanMessage(content='<think>\\nOkay, I need to figure out what LangGraph is based on the provided context. Let me start by going through the context carefully.\\n\\nThe context begins with an overview stating that LangGraph is for developers who want to build powerful, adaptable AI agents. The key points mentioned are reliability, controllability, low-level and extensible features, and first-class streaming support. \\n\\nUnder the \"Learn LangGraph basics\" section, it lists reasons developers choose LangGraph: \\n1. Reliability and controllability with moderation checks and human-in-the-loop approvals. It also mentions persisting context for long workflows.\\n2. Low-level and extensible, allowing custom agents with descriptive primitives and scalable multi-agent systems.\\n3. Streaming support with token-by-token and intermediate steps visibility.\\n\\nThe \"How to use the graph API\" section explains setting up the graph, defining state, and using nodes. There\\'s code showing how to install LangGraph, set up LangSmith, define state, add nodes to a graph, and invoke it. Examples include a simple graph execution and visualization using Mermaid PNG.\\n\\nThe user is asking \"What is Langgraph?\" so I need to synthesize the key points from the context into a concise definition. I should highlight its purpose (building AI agents), key features like reliability, low-level control, streaming, and mention the Graph API usage. Also, note that it\\'s part of the LangChain ecosystem given the references to langchain_core and LangSmith.\\n\\nI need to make sure not to include code examples unless necessary, but since the question is about what it is, focus on the overview points. Also, check if there\\'s any mention of its relation to other tools like LangChain. The context does mention LangSmith for debugging and testing, which is part of the LangChain ecosystem. So LangGraph is likely part of that ecosystem.\\n\\nPotential answer structure:\\n- LangGraph is a framework for building AI agents, designed for developers needing control and adaptability.\\n- Key features: reliability with checks/approvals, low-level primitives for customization, streaming support.\\n- Integration with LangChain tools like LangSmith for debugging and monitoring.\\n- Uses a Graph API to define workflows with nodes, state management, and visualization capabilities.\\n\\nAvoid technical jargon where possible, but ensure the essential aspects are covered. Also, note the ability to create sequences, branches, loops, and multi-agent systems as part of its flexibility.\\n</think>\\n\\nLangGraph is a framework designed for developers to build robust, adaptable AI agents with fine-grained control and scalability. It emphasizes **reliability** through features like moderation checks and human-in-the-loop approvals, ensuring agents stay aligned with desired workflows. Key characteristics include:\\n\\n1. **Low-Level Flexibility**: Offers customizable primitives for building agents or multi-agent systems, avoiding rigid abstractions to enable tailored solutions.\\n2. **Streaming Support**: Provides real-time visibility into agent reasoning via token-by-token streaming and intermediate step tracking.\\n3. **Graph-Based Workflows**: Uses a `StateGraph` API to define sequences, branches, loops, and complex interactions between nodes, with state management that persists context across long-running processes.\\n4. **Integration with LangChain**: Works seamlessly with tools like **LangSmith** for debugging, testing, and monitoring, enhancing development efficiency.\\n\\nBy combining these features, LangGraph empowers developers to create scalable, transparent AI systems while maintaining control over agent behavior and workflows.', additional_kwargs={}, response_metadata={}, id='e6550b1b-6078-4839-9ad1-7135786fbb89')]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is Langgraph?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c044efc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL_AGENT---\n",
      "state messages:  [HumanMessage(content='What is Langchain?', additional_kwargs={}, response_metadata={}, id='c5785a80-7136-410a-86f2-f76f1d548275')]\n",
      "model with tools response:  content='' additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is Langchain?\" I need to figure out which tool to use here. Let me check the available functions. There are two retrievers: one for the Langchain blog and another for a database blog. Since the question is about Langchain itself, the Langchain blog retriever seems more appropriate. The user probably wants information directly from Langchain\\'s sources. I\\'ll use the retriever_vector_langchain_blog function with the query \"What is Langchain?\" to get the relevant information.\\n', 'tool_calls': [{'id': 'kkezshkma', 'function': {'arguments': '{\"query\":\"What is Langchain?\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 140, 'prompt_tokens': 218, 'total_tokens': 358, 'completion_time': 0.268713081, 'prompt_time': 0.00866116, 'queue_time': 0.050261665, 'total_time': 0.277374241}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_f17c2eb555', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--40d8947a-1466-421a-97b4-050ab1d52d7f-0' tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'What is Langchain?'}, 'id': 'kkezshkma', 'type': 'tool_call'}] usage_metadata={'input_tokens': 218, 'output_tokens': 140, 'total_tokens': 358}\n",
      "---CHECK_REVELANCE---\n",
      "---DOCS_RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Langchain?', additional_kwargs={}, response_metadata={}, id='c5785a80-7136-410a-86f2-f76f1d548275'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is Langchain?\" I need to figure out which tool to use here. Let me check the available functions. There are two retrievers: one for the Langchain blog and another for a database blog. Since the question is about Langchain itself, the Langchain blog retriever seems more appropriate. The user probably wants information directly from Langchain\\'s sources. I\\'ll use the retriever_vector_langchain_blog function with the query \"What is Langchain?\" to get the relevant information.\\n', 'tool_calls': [{'id': 'kkezshkma', 'function': {'arguments': '{\"query\":\"What is Langchain?\"}', 'name': 'retriever_vector_langchain_blog'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 140, 'prompt_tokens': 218, 'total_tokens': 358, 'completion_time': 0.268713081, 'prompt_time': 0.00866116, 'queue_time': 0.050261665, 'total_time': 0.277374241}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_f17c2eb555', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--40d8947a-1466-421a-97b4-050ab1d52d7f-0', tool_calls=[{'name': 'retriever_vector_langchain_blog', 'args': {'query': 'What is Langchain?'}, 'id': 'kkezshkma', 'type': 'tool_call'}], usage_metadata={'input_tokens': 218, 'output_tokens': 140, 'total_tokens': 358}),\n",
       "  ToolMessage(content='Copyright © 2025 LangChain, Inc | Consent Preferences\\n\\n  \\n  \\n    Made with\\n    \\n      Material for MkDocs\\n\\nOverview\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Skip to content\\n        \\n\\n\\n\\n\\n\\n\\n\\n            \\n            \\nOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!\\n\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            LangGraph\\n          \\n\\n\\n\\n            \\n              Overview\\n            \\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            Initializing search\\n          \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Get started\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Guides\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Reference\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Examples\\n\\n        \\n\\n\\n\\n          \\n  \\n  \\n    \\n  \\n  Additional resources\\n\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    LangGraph\\n  \\n\\n\\n\\n\\n\\n\\n    GitHub\\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n    Get started\\n    \\n  \\n\\n\\n\\n\\n\\n\\n\\n\\n            Get started\\n          \\n\\n\\n\\n\\n\\n    Quickstarts\\n    \\n  \\n\\n\\n\\n\\n\\n            Quickstarts\\n          \\n\\n\\n\\n\\n    Start with a prebuilt agent\\n    \\n  \\n\\n\\n\\n\\n\\n\\n    Build a custom workflow\\n\\n# Show workflow\\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\\n\\n# Invoke\\nstate = chain.invoke({\"topic\": \"cats\"})\\nprint(\"Initial joke:\")\\nprint(state[\"joke\"])\\nprint(\"\\\\n--- --- ---\\\\n\")\\nif \"improved_joke\" in state:\\n    print(\"Improved joke:\")\\n    print(state[\"improved_joke\"])\\n    print(\"\\\\n--- --- ---\\\\n\")\\n\\n    print(\"Final joke:\")\\n    print(state[\"final_joke\"])\\nelse:\\n    print(\"Joke failed quality gate - no punchline detected!\")\\n\\nLangSmith Trace\\nhttps://smith.langchain.com/public/a0281fca-3a71-46de-beee-791468607b75/r\\nResources:\\nLangChain Academy\\nSee our lesson on Prompt Chaining here.\\n\\n\\nfrom langgraph.func import entrypoint, task\\n\\n\\n# Tasks\\n@task\\ndef generate_joke(topic: str):\\n    \"\"\"First LLM call to generate initial joke\"\"\"\\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\\n    return msg.content\\n\\nBecause many LangChain objects implement the Runnable Protocol which has async variants of all the sync methods it\\'s typically fairly quick to upgrade a sync graph to an async graph.\\nSee example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:\\nOpenAIAnthropicAzureGoogle GeminiAWS Bedrock\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"openai:gpt-4.1\")\\n\\n👉 Read the OpenAI integration docs\\n\\n\\npip install -U \"langchain[anthropic]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model\\n\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\\n\\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\\n\\n👉 Read the Anthropic integration docs\\n\\n\\npip install -U \"langchain[openai]\"\\n\\nimport os\\nfrom langchain.chat_models import init_chat_model', name='retriever_vector_langchain_blog', id='9a10526f-3302-439a-9015-00a767818c0c', tool_call_id='kkezshkma'),\n",
       "  HumanMessage(content='<think>\\nOkay, let\\'s try to figure out what LangChain is based on the provided context. The user is asking for an answer using the given context.\\n\\nFirst, I\\'ll look through the context provided. There\\'s a copyright notice from 2025 LangChain, Inc, which mentions \"Consent Preferences\" and \"Made with Material for MkDocs.\" Then there\\'s some content about a course called \"Building Ambient Agents with LangGraph\" available on LangChain Academy.\\n\\nFurther down, there\\'s a section titled \"LangGraph\" with links to GitHub, Get started, Guides, Reference, Examples, and Additional resources. Under \"Get started,\" there\\'s a Quickstarts section with options to \"Start with a prebuilt agent\" and \"Build a custom workflow.\" There\\'s code snippets showing the use of LangGraph, like displaying a workflow graph and invoking a chain with a topic. The code includes Python functions using decorators like `@task` for generating jokes, which suggests LangChain is involved in creating workflows with tasks.\\n\\nThe context also includes examples of integrating with different models like OpenAI, Anthropic, and others, indicating that LangChain helps in connecting with various AI models. There\\'s mention of LangSmith Trace and a public URL, which might be part of their tools for tracking or monitoring.\\n\\nPutting this together, LangChain seems to be a framework or platform that allows users to build workflows (like agents or custom processes) using large language models (LLMs) and other AI models. It provides tools for creating, managing, and executing these workflows, possibly with integrations for different AI providers. The presence of LangGraph suggests that it\\'s a part of LangChain for structuring workflows as graphs. The courses and academy imply there\\'s educational content to help users get started.\\n\\nThe key points are: workflow building, integration with various LLMs, tools for managing AI applications, and educational resources. The user might be looking for a concise definition without external links, so I should summarize these aspects.\\n</think>\\n\\n**LangChain** is a framework designed to help developers build and manage workflows that integrate large language models (LLMs) and other AI tools. Key aspects include:\\n\\n1. **Workflow Construction**: Enables the creation of custom workflows (e.g., \"LangGraph\") for tasks like agent development, allowing users to chain prompts, manage state, and define multi-step processes.\\n2. **Model Integration**: Supports seamless connections with major LLM providers like OpenAI, Anthropic, and Google Gemini via standardized protocols (e.g., `Runnable Protocol` for async/sync operations).\\n3. **Tools & Resources**: Offers tools like **LangSmith** for tracking execution, debugging, and monitoring, along with educational content (e.g., LangChain Academy courses on agent-building).\\n4. **Extensibility**: Provides APIs for extending functionality, including task decomposition, memory management, and integration with external data sources.\\n\\nLangChain simplifies the development of AI-driven applications by abstracting complexity in model orchestration and workflow management.', additional_kwargs={}, response_metadata={}, id='edd89484-e073-4a96-8c28-8705ad885704')]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is Langchain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f317b02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL_AGENT---\n",
      "state messages:  [HumanMessage(content='What is Machine learning?', additional_kwargs={}, response_metadata={}, id='0da0be7c-7178-4c03-9aa5-2248c211dbd4')]\n",
      "model with tools response:  content='Machine learning is a subset of artificial intelligence (AI) that enables systems to learn patterns and make decisions from data without being explicitly programmed. It involves algorithms that improve their performance on a task through experience (i.e., exposure to more data over time). Key approaches include:\\n\\n1. **Supervised Learning**: Using labeled data to train models (e.g., classification, regression).\\n2. **Unsupervised Learning**: Finding hidden patterns in unlabeled data (e.g., clustering, dimensionality reduction).\\n3. **Reinforcement Learning**: Learning optimal actions through trial and error with rewards/punishments.\\n\\nApplications range from image recognition and natural language processing to recommendation systems and autonomous vehicles. Modern techniques like deep learning (neural networks) have significantly advanced the field.' additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is Machine learning?\" Let me think about how to approach this.\\n\\nFirst, I need to determine if I should use any of the provided tools. The tools available are retriever_vector_langchain_blog and retriever_vector_db_blog. Both seem to be retrievers that can look up information based on a query. Since the question is a general definition, maybe the answer is straightforward and doesn\\'t require external data. However, it\\'s possible that the user expects detailed or up-to-date information that might be in the blogs these retrievers access.\\n\\nI should check if my existing knowledge is sufficient. Machine learning is a well-established field, so the basic definition is something I can provide without tools. But perhaps the user wants insights from recent blogs or specific databases. To be thorough, I should consider using the retrievers to see if there\\'s additional information. However, since the question is fundamental, using the tools might not be necessary. I\\'ll proceed to answer it directly but also consider if the tools would add value here. If the answer is standard, then no need for tools. Otherwise, maybe use them. Let me confirm the standard answer first.\\n'} response_metadata={'token_usage': {'completion_tokens': 397, 'prompt_tokens': 218, 'total_tokens': 615, 'completion_time': 0.851401343, 'prompt_time': 0.008263403, 'queue_time': 0.058302496999999995, 'total_time': 0.859664746}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_f17c2eb555', 'finish_reason': 'stop', 'logprobs': None} id='run--d010a2c9-b2a0-42bf-b0dd-d419d964878e-0' usage_metadata={'input_tokens': 218, 'output_tokens': 397, 'total_tokens': 615}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is Machine learning?', additional_kwargs={}, response_metadata={}, id='0da0be7c-7178-4c03-9aa5-2248c211dbd4'),\n",
       "  AIMessage(content='Machine learning is a subset of artificial intelligence (AI) that enables systems to learn patterns and make decisions from data without being explicitly programmed. It involves algorithms that improve their performance on a task through experience (i.e., exposure to more data over time). Key approaches include:\\n\\n1. **Supervised Learning**: Using labeled data to train models (e.g., classification, regression).\\n2. **Unsupervised Learning**: Finding hidden patterns in unlabeled data (e.g., clustering, dimensionality reduction).\\n3. **Reinforcement Learning**: Learning optimal actions through trial and error with rewards/punishments.\\n\\nApplications range from image recognition and natural language processing to recommendation systems and autonomous vehicles. Modern techniques like deep learning (neural networks) have significantly advanced the field.', additional_kwargs={'reasoning_content': 'Okay, the user is asking, \"What is Machine learning?\" Let me think about how to approach this.\\n\\nFirst, I need to determine if I should use any of the provided tools. The tools available are retriever_vector_langchain_blog and retriever_vector_db_blog. Both seem to be retrievers that can look up information based on a query. Since the question is a general definition, maybe the answer is straightforward and doesn\\'t require external data. However, it\\'s possible that the user expects detailed or up-to-date information that might be in the blogs these retrievers access.\\n\\nI should check if my existing knowledge is sufficient. Machine learning is a well-established field, so the basic definition is something I can provide without tools. But perhaps the user wants insights from recent blogs or specific databases. To be thorough, I should consider using the retrievers to see if there\\'s additional information. However, since the question is fundamental, using the tools might not be necessary. I\\'ll proceed to answer it directly but also consider if the tools would add value here. If the answer is standard, then no need for tools. Otherwise, maybe use them. Let me confirm the standard answer first.\\n'}, response_metadata={'token_usage': {'completion_tokens': 397, 'prompt_tokens': 218, 'total_tokens': 615, 'completion_time': 0.851401343, 'prompt_time': 0.008263403, 'queue_time': 0.058302496999999995, 'total_time': 0.859664746}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_f17c2eb555', 'finish_reason': 'stop', 'logprobs': None}, id='run--d010a2c9-b2a0-42bf-b0dd-d419d964878e-0', usage_metadata={'input_tokens': 218, 'output_tokens': 397, 'total_tokens': 615})]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"What is Machine learning?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f480d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
