{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0bcf10",
   "metadata": {},
   "source": [
    "# Simple GenAI RAG App with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eb6357",
   "metadata": {},
   "source": [
    "### Use case:\n",
    "- Let's say I have a specific website and that website has some text content\n",
    "- We are going to extract that information (Web Scraping)\n",
    "- Convert that data into chunks and embed it\n",
    "- Using llm along with prompt engineering to specifically get output from that page\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23143d",
   "metadata": {},
   "source": [
    "### üîê Load Environment Variables\n",
    "\n",
    "Here we load API keys and set environment variables using the `.env` file.  \n",
    "- Make sure `.env` is in `.gitignore`\n",
    "- LANGCHAIN_TRACING_V2 enables tracing in LangSmith\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c45648a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"MISTRAL_API_KEY\"]=os.getenv(\"MISTRAL_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"SIMPLE_GENAI_APP\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438ed42",
   "metadata": {},
   "source": [
    "### üß† Load the LLM\n",
    "\n",
    "We're using Groq's `gemma2-9b-it` model via `langchain_groq`.  \n",
    "This will be our core language model for answering questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fa4c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e787eb09",
   "metadata": {},
   "source": [
    "### üåê Load Documents from the Web\n",
    "\n",
    "Using `WebBaseLoader` to pull documentation directly from LangSmith's website.  \n",
    "This returns a list of document objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57cd5e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/?_gl=1*1sq7mh8*_gcl_au*MjU3OTg0MzA2LjE3NTE3MDM2NDY.*_ga*ODYzNDc1NDYwLjE3NTE3MDM2NDY.*_ga_47WX3HKKY2*czE3NTE5MDM3MTAkbzIkZzAkdDE3NTE5MDM3MTAkajYwJGwwJGgw', 'title': 'Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'LangSmith is a platform for building production-grade LLM applications.', 'language': 'en'}, page_content=\"\\n\\n\\n\\n\\nGet started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur Building Ambient Agents with LangGraph course is now available on LangChain Academy!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic ‚Äî score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\\nLangSmith + LangChain OSSLangSmith is framework-agnostic ‚Äî\\xa0it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\\nFor more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability\\u200b\\nObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\\nThis is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith‚Äôs observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\\n\\nGet started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals\\u200b\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\\n\\nGet started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.\\n\\nPrompt Engineering\\u200b\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.\\n\\nGet started by creating your first prompt.\\nIterate on models and prompts using the Playground.\\nManage prompts programmatically in your application.\\nWas this page helpful?You can leave detailed feedback on GitHub.NextQuick StartObservabilityEvalsPrompt EngineeringCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n\")]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/?_gl=1*1sq7mh8*_gcl_au*MjU3OTg0MzA2LjE3NTE3MDM2NDY.*_ga*ODYzNDc1NDYwLjE3NTE3MDM2NDY.*_ga_47WX3HKKY2*czE3NTE5MDM3MTAkbzIkZzAkdDE3NTE5MDM3MTAkajYwJGwwJGgw\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532eb59",
   "metadata": {},
   "source": [
    "### üìö Split Documents into Chunks\n",
    "\n",
    "We split the documents into 500-character chunks with 100-character overlap.  \n",
    "This helps in handling large documents efficiently for vector embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8d79b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter as RCTS \n",
    "\n",
    "splitter = RCTS(chunk_size = 500, chunk_overlap = 100)\n",
    "final_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a53376",
   "metadata": {},
   "source": [
    "### üß† Generate Embeddings and Store in VectorDB\n",
    "\n",
    "- Use Mistral's `mistral-embed` model to embed document chunks.\n",
    "- Store vectors in FAISS, an efficient in-memory vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ddba6d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HCL Tech\\Udemy\\Gen AI\\venv\\Lib\\site-packages\\langchain_mistralai\\embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "embedding = MistralAIEmbeddings(model = \"mistral-embed\")\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(final_docs, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8524f7b",
   "metadata": {},
   "source": [
    "### üîç Run a Basic Similarity Search\n",
    "\n",
    "Before creating a full RAG pipeline, test if your query finds relevant context chunks from the FAISS DB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b2f13dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Get started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals\\u200b\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# querying from a vectrostore DB\n",
    "query = \"The quality and development speed of AI applications depends on\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4ccb2",
   "metadata": {},
   "source": [
    "### üß© Create Document Chain with Contextual Prompt\n",
    "\n",
    "`create_stuff_documents_chain` merges retrieved docs with a prompt so the LLM gets useful context.  \n",
    "This is the ‚ÄúRAG‚Äù part (Retrieval-Augmented Generation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc245ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Chain, Dcoument Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# create_stuff_document_chain makes our llm able to have context when answering questions\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56afcde",
   "metadata": {},
   "source": [
    "### üß™ Manually Test the Document Chain\n",
    "\n",
    "Provide context manually to see how the chain performs before connecting to retriever.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08566128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided text, the LangSmith SDK and UI make it easy to build and run high-quality evaluations for AI applications. \\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \"input\":\"The quality and development speed of AI applications depends on what?\",\n",
    "        \"context\":[Document(page_content=\"\"\"The quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.Get started by creating your first evaluation.Quickly assess the performance of your application using our off-the-shelf evaluators as a starting point.Analyze results of evaluations in the LangSmith UI and compare results over time.Easily collect human feedback on your data to improve your application.\"\"\")]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ed1377",
   "metadata": {},
   "source": [
    "### üîó Connect Retriever to Document Chain\n",
    "\n",
    "This turns our setup into a full RAG chain.  \n",
    "Retriever fetches relevant chunks ‚Üí chain injects them into the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed8a1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "# document chain is responsible to give us context information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae2bff7",
   "metadata": {},
   "source": [
    "### üí° Get Final Response from RAG Pipeline\n",
    "\n",
    "The query is passed through the retrieval chain which:\n",
    "1. Fetches context via FAISS\n",
    "2. Injects it into prompt\n",
    "3. Sends to LLM (Groq)\n",
    "\n",
    "You get the final answer here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fc8421a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangSmith offers LLM-native observability, which helps you gain insights into your application's performance throughout its development lifecycle.  \\n\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get response from the LLM\n",
    "response = retrieval_chain.invoke({\"input\":\"The quality and development speed of AI applications depends on what?\"})\n",
    "response[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
